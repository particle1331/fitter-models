{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1e169dd1",
   "metadata": {
    "papermill": {
     "duration": 0.01165,
     "end_time": "2025-01-13T14:54:06.705002",
     "exception": false,
     "start_time": "2025-01-13T14:54:06.693352",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Text generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "def72d65",
   "metadata": {
    "papermill": {
     "duration": 0.005283,
     "end_time": "2025-01-13T14:54:06.718420",
     "exception": false,
     "start_time": "2025-01-13T14:54:06.713137",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Recall that the state vector is initialized as zero. So we use a **warmup context** or a **prompt** to allow the RNN cell to update its state iteratively by processing one character at a time from the warmup text. Then, the algorithm simulates the prediction process of our RNN language model, but instead of using a predefined input sequence, it uses the *previous output* as the next input."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "345110a1",
   "metadata": {
    "papermill": {
     "duration": 0.00366,
     "end_time": "2025-01-13T14:54:06.726223",
     "exception": false,
     "start_time": "2025-01-13T14:54:06.722563",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<br>\n",
    "\n",
    "```{figure} ../../../img/nn/04-rnn-textgen.png\n",
    "---\n",
    "width: 500px\n",
    "name: 04-rnn-textgen\n",
    "align: center\n",
    "---\n",
    "An input sequence is used to get a final state vector (this is the warmup stage, i.e. the state goes from zero to some nonzero vector). The final character and state during warmup is used to predict the next character. This process is repeated until the number of predicted tokens is reached.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cd26308a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-13T14:54:06.735144Z",
     "iopub.status.busy": "2025-01-13T14:54:06.734755Z",
     "iopub.status.idle": "2025-01-13T14:54:07.670519Z",
     "shell.execute_reply": "2025-01-13T14:54:07.670148Z"
    },
    "papermill": {
     "duration": 0.941604,
     "end_time": "2025-01-13T14:54:07.671832",
     "exception": false,
     "start_time": "2025-01-13T14:54:06.730228",
     "status": "completed"
    },
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "from chapter import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "663c068c",
   "metadata": {
    "papermill": {
     "duration": 0.001199,
     "end_time": "2025-01-13T14:54:07.674565",
     "exception": false,
     "start_time": "2025-01-13T14:54:07.673366",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Loading the trained RNN language model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9832e1fd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-13T14:54:07.677821Z",
     "iopub.status.busy": "2025-01-13T14:54:07.677635Z",
     "iopub.status.idle": "2025-01-13T14:54:07.761146Z",
     "shell.execute_reply": "2025-01-13T14:54:07.760841Z"
    },
    "papermill": {
     "duration": 0.086356,
     "end_time": "2025-01-13T14:54:07.762178",
     "exception": false,
     "start_time": "2025-01-13T14:54:07.675822",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "DEVICE = \"cpu\"  # faster for RNN inference\n",
    "WEIGHTS_PATH = \"./artifacts/rnn_lm.pkl\"\n",
    "data, tokenizer = TimeMachine().build()\n",
    "VOCAB_SIZE = tokenizer.vocab_size\n",
    "\n",
    "model = LanguageModel(RNN)(VOCAB_SIZE, 64, VOCAB_SIZE)\n",
    "model.load_state_dict(torch.load(WEIGHTS_PATH, map_location=DEVICE));"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "432733cf",
   "metadata": {
    "papermill": {
     "duration": 0.00109,
     "end_time": "2025-01-13T14:54:07.764562",
     "exception": false,
     "start_time": "2025-01-13T14:54:07.763472",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Text generation utils and algorithm:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "64d130f6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-13T14:54:07.770722Z",
     "iopub.status.busy": "2025-01-13T14:54:07.770592Z",
     "iopub.status.idle": "2025-01-13T14:54:07.810801Z",
     "shell.execute_reply": "2025-01-13T14:54:07.810014Z"
    },
    "papermill": {
     "duration": 0.046301,
     "end_time": "2025-01-13T14:54:07.811921",
     "exception": false,
     "start_time": "2025-01-13T14:54:07.765620",
     "status": "completed"
    },
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>pre { line-height: 125%; }\n",
       "td.linenos .normal { color: inherit; background-color: transparent; padding-left: 5px; padding-right: 5px; }\n",
       "span.linenos { color: inherit; background-color: transparent; padding-left: 5px; padding-right: 5px; }\n",
       "td.linenos .special { color: #000000; background-color: #ffffc0; padding-left: 5px; padding-right: 5px; }\n",
       "span.linenos.special { color: #000000; background-color: #ffffc0; padding-left: 5px; padding-right: 5px; }\n",
       ".output_html .hll { background-color: #ffffcc }\n",
       ".output_html { background: #f8f8f8; }\n",
       ".output_html .c { color: #3D7B7B; font-style: italic } /* Comment */\n",
       ".output_html .err { border: 1px solid #FF0000 } /* Error */\n",
       ".output_html .k { color: #008000; font-weight: bold } /* Keyword */\n",
       ".output_html .o { color: #666666 } /* Operator */\n",
       ".output_html .ch { color: #3D7B7B; font-style: italic } /* Comment.Hashbang */\n",
       ".output_html .cm { color: #3D7B7B; font-style: italic } /* Comment.Multiline */\n",
       ".output_html .cp { color: #9C6500 } /* Comment.Preproc */\n",
       ".output_html .cpf { color: #3D7B7B; font-style: italic } /* Comment.PreprocFile */\n",
       ".output_html .c1 { color: #3D7B7B; font-style: italic } /* Comment.Single */\n",
       ".output_html .cs { color: #3D7B7B; font-style: italic } /* Comment.Special */\n",
       ".output_html .gd { color: #A00000 } /* Generic.Deleted */\n",
       ".output_html .ge { font-style: italic } /* Generic.Emph */\n",
       ".output_html .ges { font-weight: bold; font-style: italic } /* Generic.EmphStrong */\n",
       ".output_html .gr { color: #E40000 } /* Generic.Error */\n",
       ".output_html .gh { color: #000080; font-weight: bold } /* Generic.Heading */\n",
       ".output_html .gi { color: #008400 } /* Generic.Inserted */\n",
       ".output_html .go { color: #717171 } /* Generic.Output */\n",
       ".output_html .gp { color: #000080; font-weight: bold } /* Generic.Prompt */\n",
       ".output_html .gs { font-weight: bold } /* Generic.Strong */\n",
       ".output_html .gu { color: #800080; font-weight: bold } /* Generic.Subheading */\n",
       ".output_html .gt { color: #0044DD } /* Generic.Traceback */\n",
       ".output_html .kc { color: #008000; font-weight: bold } /* Keyword.Constant */\n",
       ".output_html .kd { color: #008000; font-weight: bold } /* Keyword.Declaration */\n",
       ".output_html .kn { color: #008000; font-weight: bold } /* Keyword.Namespace */\n",
       ".output_html .kp { color: #008000 } /* Keyword.Pseudo */\n",
       ".output_html .kr { color: #008000; font-weight: bold } /* Keyword.Reserved */\n",
       ".output_html .kt { color: #B00040 } /* Keyword.Type */\n",
       ".output_html .m { color: #666666 } /* Literal.Number */\n",
       ".output_html .s { color: #BA2121 } /* Literal.String */\n",
       ".output_html .na { color: #687822 } /* Name.Attribute */\n",
       ".output_html .nb { color: #008000 } /* Name.Builtin */\n",
       ".output_html .nc { color: #0000FF; font-weight: bold } /* Name.Class */\n",
       ".output_html .no { color: #880000 } /* Name.Constant */\n",
       ".output_html .nd { color: #AA22FF } /* Name.Decorator */\n",
       ".output_html .ni { color: #717171; font-weight: bold } /* Name.Entity */\n",
       ".output_html .ne { color: #CB3F38; font-weight: bold } /* Name.Exception */\n",
       ".output_html .nf { color: #0000FF } /* Name.Function */\n",
       ".output_html .nl { color: #767600 } /* Name.Label */\n",
       ".output_html .nn { color: #0000FF; font-weight: bold } /* Name.Namespace */\n",
       ".output_html .nt { color: #008000; font-weight: bold } /* Name.Tag */\n",
       ".output_html .nv { color: #19177C } /* Name.Variable */\n",
       ".output_html .ow { color: #AA22FF; font-weight: bold } /* Operator.Word */\n",
       ".output_html .w { color: #bbbbbb } /* Text.Whitespace */\n",
       ".output_html .mb { color: #666666 } /* Literal.Number.Bin */\n",
       ".output_html .mf { color: #666666 } /* Literal.Number.Float */\n",
       ".output_html .mh { color: #666666 } /* Literal.Number.Hex */\n",
       ".output_html .mi { color: #666666 } /* Literal.Number.Integer */\n",
       ".output_html .mo { color: #666666 } /* Literal.Number.Oct */\n",
       ".output_html .sa { color: #BA2121 } /* Literal.String.Affix */\n",
       ".output_html .sb { color: #BA2121 } /* Literal.String.Backtick */\n",
       ".output_html .sc { color: #BA2121 } /* Literal.String.Char */\n",
       ".output_html .dl { color: #BA2121 } /* Literal.String.Delimiter */\n",
       ".output_html .sd { color: #BA2121; font-style: italic } /* Literal.String.Doc */\n",
       ".output_html .s2 { color: #BA2121 } /* Literal.String.Double */\n",
       ".output_html .se { color: #AA5D1F; font-weight: bold } /* Literal.String.Escape */\n",
       ".output_html .sh { color: #BA2121 } /* Literal.String.Heredoc */\n",
       ".output_html .si { color: #A45A77; font-weight: bold } /* Literal.String.Interpol */\n",
       ".output_html .sx { color: #008000 } /* Literal.String.Other */\n",
       ".output_html .sr { color: #A45A77 } /* Literal.String.Regex */\n",
       ".output_html .s1 { color: #BA2121 } /* Literal.String.Single */\n",
       ".output_html .ss { color: #19177C } /* Literal.String.Symbol */\n",
       ".output_html .bp { color: #008000 } /* Name.Builtin.Pseudo */\n",
       ".output_html .fm { color: #0000FF } /* Name.Function.Magic */\n",
       ".output_html .vc { color: #19177C } /* Name.Variable.Class */\n",
       ".output_html .vg { color: #19177C } /* Name.Variable.Global */\n",
       ".output_html .vi { color: #19177C } /* Name.Variable.Instance */\n",
       ".output_html .vm { color: #19177C } /* Name.Variable.Magic */\n",
       ".output_html .il { color: #666666 } /* Literal.Number.Integer.Long */</style><div class=\"highlight\"><pre><span></span><span class=\"kn\">import</span> <span class=\"nn\">torch</span>\n",
       "<span class=\"kn\">import</span> <span class=\"nn\">torch.nn.functional</span> <span class=\"k\">as</span> <span class=\"nn\">F</span>\n",
       "\n",
       "<span class=\"k\">class</span> <span class=\"nc\">TextGenerator</span><span class=\"p\">:</span>\n",
       "    <span class=\"k\">def</span> <span class=\"fm\">__init__</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">,</span> <span class=\"n\">model</span><span class=\"p\">,</span> <span class=\"n\">tokenizer</span><span class=\"p\">,</span> <span class=\"n\">device</span><span class=\"o\">=</span><span class=\"s2\">&quot;cpu&quot;</span><span class=\"p\">):</span>\n",
       "        <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">model</span> <span class=\"o\">=</span> <span class=\"n\">model</span><span class=\"o\">.</span><span class=\"n\">to</span><span class=\"p\">(</span><span class=\"n\">device</span><span class=\"p\">)</span>\n",
       "        <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">device</span> <span class=\"o\">=</span> <span class=\"n\">device</span>\n",
       "        <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">tokenizer</span> <span class=\"o\">=</span> <span class=\"n\">tokenizer</span>\n",
       "\n",
       "    <span class=\"k\">def</span> <span class=\"nf\">_inp</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">,</span> <span class=\"n\">indices</span><span class=\"p\">:</span> <span class=\"nb\">list</span><span class=\"p\">[</span><span class=\"nb\">int</span><span class=\"p\">]):</span>\n",
       "<span class=\"w\">        </span><span class=\"sd\">&quot;&quot;&quot;Preprocess indices (T,) to (T, 1, V) shape with B=1.&quot;&quot;&quot;</span>\n",
       "        <span class=\"n\">VOCAB_SIZE</span> <span class=\"o\">=</span> <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">tokenizer</span><span class=\"o\">.</span><span class=\"n\">vocab_size</span>\n",
       "        <span class=\"n\">x</span> <span class=\"o\">=</span> <span class=\"n\">F</span><span class=\"o\">.</span><span class=\"n\">one_hot</span><span class=\"p\">(</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">tensor</span><span class=\"p\">(</span><span class=\"n\">indices</span><span class=\"p\">),</span> <span class=\"n\">VOCAB_SIZE</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"n\">float</span><span class=\"p\">()</span>\n",
       "        <span class=\"k\">return</span> <span class=\"n\">x</span><span class=\"o\">.</span><span class=\"n\">view</span><span class=\"p\">(</span><span class=\"o\">-</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"n\">VOCAB_SIZE</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"n\">to</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">device</span><span class=\"p\">)</span>\n",
       "\n",
       "    <span class=\"nd\">@staticmethod</span>\n",
       "    <span class=\"k\">def</span> <span class=\"nf\">sample_token</span><span class=\"p\">(</span><span class=\"n\">logits</span><span class=\"p\">,</span> <span class=\"n\">temperature</span><span class=\"p\">:</span> <span class=\"nb\">float</span><span class=\"p\">):</span>\n",
       "<span class=\"w\">        </span><span class=\"sd\">&quot;&quot;&quot;Convert logits to probs with softmax temperature.&quot;&quot;&quot;</span>\n",
       "        <span class=\"n\">p</span> <span class=\"o\">=</span> <span class=\"n\">F</span><span class=\"o\">.</span><span class=\"n\">softmax</span><span class=\"p\">(</span><span class=\"n\">logits</span> <span class=\"o\">/</span> <span class=\"n\">temperature</span><span class=\"p\">,</span> <span class=\"n\">dim</span><span class=\"o\">=</span><span class=\"mi\">1</span><span class=\"p\">)</span>  <span class=\"c1\"># T = ∞ =&gt; exp ~ 1 =&gt; p ~ U[0, 1]</span>\n",
       "        <span class=\"k\">return</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">multinomial</span><span class=\"p\">(</span><span class=\"n\">p</span><span class=\"p\">,</span> <span class=\"n\">num_samples</span><span class=\"o\">=</span><span class=\"mi\">1</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"n\">item</span><span class=\"p\">()</span>\n",
       "\n",
       "    <span class=\"k\">def</span> <span class=\"nf\">predict</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">,</span> <span class=\"n\">prompt</span><span class=\"p\">:</span> <span class=\"nb\">str</span><span class=\"p\">,</span> <span class=\"n\">num_preds</span><span class=\"p\">:</span> <span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"n\">temperature</span><span class=\"o\">=</span><span class=\"mf\">1.0</span><span class=\"p\">):</span>\n",
       "<span class=\"w\">        </span><span class=\"sd\">&quot;&quot;&quot;Simulate character generation one at a time.&quot;&quot;&quot;</span>\n",
       "\n",
       "        <span class=\"c1\"># Iterate over warmup text. RNN cell outputs final state</span>\n",
       "        <span class=\"n\">warmup_indices</span> <span class=\"o\">=</span> <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">tokenizer</span><span class=\"o\">.</span><span class=\"n\">encode</span><span class=\"p\">(</span><span class=\"n\">prompt</span><span class=\"o\">.</span><span class=\"n\">lower</span><span class=\"p\">())</span><span class=\"o\">.</span><span class=\"n\">tolist</span><span class=\"p\">()</span>\n",
       "        <span class=\"n\">outs</span><span class=\"p\">,</span> <span class=\"n\">state</span> <span class=\"o\">=</span> <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">model</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">_inp</span><span class=\"p\">(</span><span class=\"n\">warmup_indices</span><span class=\"p\">),</span> <span class=\"n\">return_state</span><span class=\"o\">=</span><span class=\"kc\">True</span><span class=\"p\">)</span>\n",
       "\n",
       "        <span class=\"c1\"># Sample next token and update state</span>\n",
       "        <span class=\"n\">indices</span> <span class=\"o\">=</span> <span class=\"p\">[]</span>\n",
       "        <span class=\"k\">for</span> <span class=\"n\">_</span> <span class=\"ow\">in</span> <span class=\"nb\">range</span><span class=\"p\">(</span><span class=\"n\">num_preds</span><span class=\"p\">):</span>\n",
       "            <span class=\"n\">i</span> <span class=\"o\">=</span> <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">sample_token</span><span class=\"p\">(</span><span class=\"n\">outs</span><span class=\"p\">[</span><span class=\"o\">-</span><span class=\"mi\">1</span><span class=\"p\">],</span> <span class=\"n\">temperature</span><span class=\"p\">)</span>\n",
       "            <span class=\"n\">indices</span><span class=\"o\">.</span><span class=\"n\">append</span><span class=\"p\">(</span><span class=\"n\">i</span><span class=\"p\">)</span>\n",
       "            <span class=\"n\">outs</span><span class=\"p\">,</span> <span class=\"n\">state</span> <span class=\"o\">=</span> <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">model</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">_inp</span><span class=\"p\">([</span><span class=\"n\">i</span><span class=\"p\">]),</span> <span class=\"n\">state</span><span class=\"p\">,</span> <span class=\"n\">return_state</span><span class=\"o\">=</span><span class=\"kc\">True</span><span class=\"p\">)</span>\n",
       "\n",
       "        <span class=\"k\">return</span> <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">tokenizer</span><span class=\"o\">.</span><span class=\"n\">decode</span><span class=\"p\">(</span><span class=\"n\">warmup_indices</span> <span class=\"o\">+</span> <span class=\"n\">indices</span><span class=\"p\">)</span>\n",
       "</pre></div>\n"
      ],
      "text/latex": [
       "\\begin{Verbatim}[commandchars=\\\\\\{\\}]\n",
       "\\PY{k+kn}{import} \\PY{n+nn}{torch}\n",
       "\\PY{k+kn}{import} \\PY{n+nn}{torch}\\PY{n+nn}{.}\\PY{n+nn}{nn}\\PY{n+nn}{.}\\PY{n+nn}{functional} \\PY{k}{as} \\PY{n+nn}{F}\n",
       "\n",
       "\\PY{k}{class} \\PY{n+nc}{TextGenerator}\\PY{p}{:}\n",
       "    \\PY{k}{def} \\PY{n+nf+fm}{\\PYZus{}\\PYZus{}init\\PYZus{}\\PYZus{}}\\PY{p}{(}\\PY{n+nb+bp}{self}\\PY{p}{,} \\PY{n}{model}\\PY{p}{,} \\PY{n}{tokenizer}\\PY{p}{,} \\PY{n}{device}\\PY{o}{=}\\PY{l+s+s2}{\\PYZdq{}}\\PY{l+s+s2}{cpu}\\PY{l+s+s2}{\\PYZdq{}}\\PY{p}{)}\\PY{p}{:}\n",
       "        \\PY{n+nb+bp}{self}\\PY{o}{.}\\PY{n}{model} \\PY{o}{=} \\PY{n}{model}\\PY{o}{.}\\PY{n}{to}\\PY{p}{(}\\PY{n}{device}\\PY{p}{)}\n",
       "        \\PY{n+nb+bp}{self}\\PY{o}{.}\\PY{n}{device} \\PY{o}{=} \\PY{n}{device}\n",
       "        \\PY{n+nb+bp}{self}\\PY{o}{.}\\PY{n}{tokenizer} \\PY{o}{=} \\PY{n}{tokenizer}\n",
       "\n",
       "    \\PY{k}{def} \\PY{n+nf}{\\PYZus{}inp}\\PY{p}{(}\\PY{n+nb+bp}{self}\\PY{p}{,} \\PY{n}{indices}\\PY{p}{:} \\PY{n+nb}{list}\\PY{p}{[}\\PY{n+nb}{int}\\PY{p}{]}\\PY{p}{)}\\PY{p}{:}\n",
       "\\PY{+w}{        }\\PY{l+s+sd}{\\PYZdq{}\\PYZdq{}\\PYZdq{}Preprocess indices (T,) to (T, 1, V) shape with B=1.\\PYZdq{}\\PYZdq{}\\PYZdq{}}\n",
       "        \\PY{n}{VOCAB\\PYZus{}SIZE} \\PY{o}{=} \\PY{n+nb+bp}{self}\\PY{o}{.}\\PY{n}{tokenizer}\\PY{o}{.}\\PY{n}{vocab\\PYZus{}size}\n",
       "        \\PY{n}{x} \\PY{o}{=} \\PY{n}{F}\\PY{o}{.}\\PY{n}{one\\PYZus{}hot}\\PY{p}{(}\\PY{n}{torch}\\PY{o}{.}\\PY{n}{tensor}\\PY{p}{(}\\PY{n}{indices}\\PY{p}{)}\\PY{p}{,} \\PY{n}{VOCAB\\PYZus{}SIZE}\\PY{p}{)}\\PY{o}{.}\\PY{n}{float}\\PY{p}{(}\\PY{p}{)}\n",
       "        \\PY{k}{return} \\PY{n}{x}\\PY{o}{.}\\PY{n}{view}\\PY{p}{(}\\PY{o}{\\PYZhy{}}\\PY{l+m+mi}{1}\\PY{p}{,} \\PY{l+m+mi}{1}\\PY{p}{,} \\PY{n}{VOCAB\\PYZus{}SIZE}\\PY{p}{)}\\PY{o}{.}\\PY{n}{to}\\PY{p}{(}\\PY{n+nb+bp}{self}\\PY{o}{.}\\PY{n}{device}\\PY{p}{)}\n",
       "\n",
       "    \\PY{n+nd}{@staticmethod}\n",
       "    \\PY{k}{def} \\PY{n+nf}{sample\\PYZus{}token}\\PY{p}{(}\\PY{n}{logits}\\PY{p}{,} \\PY{n}{temperature}\\PY{p}{:} \\PY{n+nb}{float}\\PY{p}{)}\\PY{p}{:}\n",
       "\\PY{+w}{        }\\PY{l+s+sd}{\\PYZdq{}\\PYZdq{}\\PYZdq{}Convert logits to probs with softmax temperature.\\PYZdq{}\\PYZdq{}\\PYZdq{}}\n",
       "        \\PY{n}{p} \\PY{o}{=} \\PY{n}{F}\\PY{o}{.}\\PY{n}{softmax}\\PY{p}{(}\\PY{n}{logits} \\PY{o}{/} \\PY{n}{temperature}\\PY{p}{,} \\PY{n}{dim}\\PY{o}{=}\\PY{l+m+mi}{1}\\PY{p}{)}  \\PY{c+c1}{\\PYZsh{} T = ∞ =\\PYZgt{} exp \\PYZti{} 1 =\\PYZgt{} p \\PYZti{} U[0, 1]}\n",
       "        \\PY{k}{return} \\PY{n}{torch}\\PY{o}{.}\\PY{n}{multinomial}\\PY{p}{(}\\PY{n}{p}\\PY{p}{,} \\PY{n}{num\\PYZus{}samples}\\PY{o}{=}\\PY{l+m+mi}{1}\\PY{p}{)}\\PY{o}{.}\\PY{n}{item}\\PY{p}{(}\\PY{p}{)}\n",
       "\n",
       "    \\PY{k}{def} \\PY{n+nf}{predict}\\PY{p}{(}\\PY{n+nb+bp}{self}\\PY{p}{,} \\PY{n}{prompt}\\PY{p}{:} \\PY{n+nb}{str}\\PY{p}{,} \\PY{n}{num\\PYZus{}preds}\\PY{p}{:} \\PY{n+nb}{int}\\PY{p}{,} \\PY{n}{temperature}\\PY{o}{=}\\PY{l+m+mf}{1.0}\\PY{p}{)}\\PY{p}{:}\n",
       "\\PY{+w}{        }\\PY{l+s+sd}{\\PYZdq{}\\PYZdq{}\\PYZdq{}Simulate character generation one at a time.\\PYZdq{}\\PYZdq{}\\PYZdq{}}\n",
       "\n",
       "        \\PY{c+c1}{\\PYZsh{} Iterate over warmup text. RNN cell outputs final state}\n",
       "        \\PY{n}{warmup\\PYZus{}indices} \\PY{o}{=} \\PY{n+nb+bp}{self}\\PY{o}{.}\\PY{n}{tokenizer}\\PY{o}{.}\\PY{n}{encode}\\PY{p}{(}\\PY{n}{prompt}\\PY{o}{.}\\PY{n}{lower}\\PY{p}{(}\\PY{p}{)}\\PY{p}{)}\\PY{o}{.}\\PY{n}{tolist}\\PY{p}{(}\\PY{p}{)}\n",
       "        \\PY{n}{outs}\\PY{p}{,} \\PY{n}{state} \\PY{o}{=} \\PY{n+nb+bp}{self}\\PY{o}{.}\\PY{n}{model}\\PY{p}{(}\\PY{n+nb+bp}{self}\\PY{o}{.}\\PY{n}{\\PYZus{}inp}\\PY{p}{(}\\PY{n}{warmup\\PYZus{}indices}\\PY{p}{)}\\PY{p}{,} \\PY{n}{return\\PYZus{}state}\\PY{o}{=}\\PY{k+kc}{True}\\PY{p}{)}\n",
       "\n",
       "        \\PY{c+c1}{\\PYZsh{} Sample next token and update state}\n",
       "        \\PY{n}{indices} \\PY{o}{=} \\PY{p}{[}\\PY{p}{]}\n",
       "        \\PY{k}{for} \\PY{n}{\\PYZus{}} \\PY{o+ow}{in} \\PY{n+nb}{range}\\PY{p}{(}\\PY{n}{num\\PYZus{}preds}\\PY{p}{)}\\PY{p}{:}\n",
       "            \\PY{n}{i} \\PY{o}{=} \\PY{n+nb+bp}{self}\\PY{o}{.}\\PY{n}{sample\\PYZus{}token}\\PY{p}{(}\\PY{n}{outs}\\PY{p}{[}\\PY{o}{\\PYZhy{}}\\PY{l+m+mi}{1}\\PY{p}{]}\\PY{p}{,} \\PY{n}{temperature}\\PY{p}{)}\n",
       "            \\PY{n}{indices}\\PY{o}{.}\\PY{n}{append}\\PY{p}{(}\\PY{n}{i}\\PY{p}{)}\n",
       "            \\PY{n}{outs}\\PY{p}{,} \\PY{n}{state} \\PY{o}{=} \\PY{n+nb+bp}{self}\\PY{o}{.}\\PY{n}{model}\\PY{p}{(}\\PY{n+nb+bp}{self}\\PY{o}{.}\\PY{n}{\\PYZus{}inp}\\PY{p}{(}\\PY{p}{[}\\PY{n}{i}\\PY{p}{]}\\PY{p}{)}\\PY{p}{,} \\PY{n}{state}\\PY{p}{,} \\PY{n}{return\\PYZus{}state}\\PY{o}{=}\\PY{k+kc}{True}\\PY{p}{)}\n",
       "\n",
       "        \\PY{k}{return} \\PY{n+nb+bp}{self}\\PY{o}{.}\\PY{n}{tokenizer}\\PY{o}{.}\\PY{n}{decode}\\PY{p}{(}\\PY{n}{warmup\\PYZus{}indices} \\PY{o}{+} \\PY{n}{indices}\\PY{p}{)}\n",
       "\\end{Verbatim}\n"
      ],
      "text/plain": [
       "import torch\n",
       "import torch.nn.functional as F\n",
       "\n",
       "class TextGenerator:\n",
       "    def __init__(self, model, tokenizer, device=\"cpu\"):\n",
       "        self.model = model.to(device)\n",
       "        self.device = device\n",
       "        self.tokenizer = tokenizer\n",
       "\n",
       "    def _inp(self, indices: list[int]):\n",
       "        \"\"\"Preprocess indices (T,) to (T, 1, V) shape with B=1.\"\"\"\n",
       "        VOCAB_SIZE = self.tokenizer.vocab_size\n",
       "        x = F.one_hot(torch.tensor(indices), VOCAB_SIZE).float()\n",
       "        return x.view(-1, 1, VOCAB_SIZE).to(self.device)\n",
       "\n",
       "    @staticmethod\n",
       "    def sample_token(logits, temperature: float):\n",
       "        \"\"\"Convert logits to probs with softmax temperature.\"\"\"\n",
       "        p = F.softmax(logits / temperature, dim=1)  # T = ∞ => exp ~ 1 => p ~ U[0, 1]\n",
       "        return torch.multinomial(p, num_samples=1).item()\n",
       "\n",
       "    def predict(self, prompt: str, num_preds: int, temperature=1.0):\n",
       "        \"\"\"Simulate character generation one at a time.\"\"\"\n",
       "\n",
       "        # Iterate over warmup text. RNN cell outputs final state\n",
       "        warmup_indices = self.tokenizer.encode(prompt.lower()).tolist()\n",
       "        outs, state = self.model(self._inp(warmup_indices), return_state=True)\n",
       "\n",
       "        # Sample next token and update state\n",
       "        indices = []\n",
       "        for _ in range(num_preds):\n",
       "            i = self.sample_token(outs[-1], temperature)\n",
       "            indices.append(i)\n",
       "            outs, state = self.model(self._inp([i]), state, return_state=True)\n",
       "\n",
       "        return self.tokenizer.decode(warmup_indices + indices)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%save\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class TextGenerator:\n",
    "    def __init__(self, model, tokenizer, device=\"cpu\"):\n",
    "        self.model = model.to(device)\n",
    "        self.device = device\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def _inp(self, indices: list[int]):\n",
    "        \"\"\"Preprocess indices (T,) to (T, 1, V) shape with B=1.\"\"\"\n",
    "        VOCAB_SIZE = self.tokenizer.vocab_size\n",
    "        x = F.one_hot(torch.tensor(indices), VOCAB_SIZE).float()\n",
    "        return x.view(-1, 1, VOCAB_SIZE).to(self.device)\n",
    "\n",
    "    @staticmethod\n",
    "    def sample_token(logits, temperature: float):\n",
    "        \"\"\"Convert logits to probs with softmax temperature.\"\"\"\n",
    "        p = F.softmax(logits / temperature, dim=1)  # T = ∞ => exp ~ 1 => p ~ U[0, 1]\n",
    "        return torch.multinomial(p, num_samples=1).item()\n",
    "\n",
    "    def predict(self, prompt: str, num_preds: int, temperature=1.0):\n",
    "        \"\"\"Simulate character generation one at a time.\"\"\"\n",
    "\n",
    "        # Iterate over warmup text. RNN cell outputs final state\n",
    "        warmup_indices = self.tokenizer.encode(prompt.lower()).tolist()\n",
    "        outs, state = self.model(self._inp(warmup_indices), return_state=True)\n",
    "\n",
    "        # Sample next token and update state\n",
    "        indices = []\n",
    "        for _ in range(num_preds):\n",
    "            i = self.sample_token(outs[-1], temperature)\n",
    "            indices.append(i)\n",
    "            outs, state = self.model(self._inp([i]), state, return_state=True)\n",
    "\n",
    "        return self.tokenizer.decode(warmup_indices + indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42ba1dfa",
   "metadata": {
    "papermill": {
     "duration": 0.001393,
     "end_time": "2025-01-13T14:54:07.815248",
     "exception": false,
     "start_time": "2025-01-13T14:54:07.813855",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**Sanity test.** Completing 'thank you':"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b9a7b406",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-13T14:54:07.818620Z",
     "iopub.status.busy": "2025-01-13T14:54:07.818384Z",
     "iopub.status.idle": "2025-01-13T14:54:07.833344Z",
     "shell.execute_reply": "2025-01-13T14:54:07.833051Z"
    },
    "papermill": {
     "duration": 0.017737,
     "end_time": "2025-01-13T14:54:07.834231",
     "exception": false,
     "start_time": "2025-01-13T14:54:07.816494",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "textgen = TextGenerator(model, tokenizer, device=\"cpu\")\n",
    "s = [textgen.predict(\"thank y\", num_preds=2, temperature=0.4) for i in range(20)]\n",
    "(np.array(s) == \"thank you\").mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e112370",
   "metadata": {
    "papermill": {
     "duration": 0.001357,
     "end_time": "2025-01-13T14:54:07.837696",
     "exception": false,
     "start_time": "2025-01-13T14:54:07.836339",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**Example.** The network can generate output given warmup prompt of arbitrary length. Here we also look at the effect of temperature on the generated text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3f3d06dd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-13T14:54:07.841147Z",
     "iopub.status.busy": "2025-01-13T14:54:07.840988Z",
     "iopub.status.idle": "2025-01-13T14:54:07.874343Z",
     "shell.execute_reply": "2025-01-13T14:54:07.873844Z"
    },
    "papermill": {
     "duration": 0.036593,
     "end_time": "2025-01-13T14:54:07.875514",
     "exception": false,
     "start_time": "2025-01-13T14:54:07.838921",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "warmup = \"mr williams i underst\"\n",
    "text = []\n",
    "temperature = []\n",
    "for i in range(1, 6):\n",
    "    t = 0.20 * i\n",
    "    s = textgen.predict(warmup, num_preds=100, temperature=t)\n",
    "    text.append(s)\n",
    "    temperature.append(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "14e2cc94",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-13T14:54:07.879669Z",
     "iopub.status.busy": "2025-01-13T14:54:07.879535Z",
     "iopub.status.idle": "2025-01-13T14:54:08.483176Z",
     "shell.execute_reply": "2025-01-13T14:54:08.482831Z"
    },
    "papermill": {
     "duration": 0.609122,
     "end_time": "2025-01-13T14:54:08.486355",
     "exception": false,
     "start_time": "2025-01-13T14:54:07.877233",
     "status": "completed"
    },
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_d20cd_row0_col0, #T_d20cd_row0_col1, #T_d20cd_row1_col0, #T_d20cd_row1_col1, #T_d20cd_row2_col0, #T_d20cd_row2_col1, #T_d20cd_row3_col0, #T_d20cd_row3_col1, #T_d20cd_row4_col0, #T_d20cd_row4_col1 {\n",
       "  text-align: left;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_d20cd\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_d20cd_level0_col0\" class=\"col_heading level0 col0\" >temp</th>\n",
       "      <th id=\"T_d20cd_level0_col1\" class=\"col_heading level0 col1\" >text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_d20cd_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_d20cd_row0_col0\" class=\"data row0 col0\" >0.2</td>\n",
       "      <td id=\"T_d20cd_row0_col1\" class=\"data row0 col1\" >mr williams i underst in the thing in the morlocks and the the the machine and the strange and the morlocks of the the th</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_d20cd_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "      <td id=\"T_d20cd_row1_col0\" class=\"data row1 col0\" >0.4</td>\n",
       "      <td id=\"T_d20cd_row1_col1\" class=\"data row1 col1\" >mr williams i understed a strange the from the limporle i was and and i had into my our in the morlocks of the machine th</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_d20cd_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
       "      <td id=\"T_d20cd_row2_col0\" class=\"data row2 col0\" >0.6</td>\n",
       "      <td id=\"T_d20cd_row2_col1\" class=\"data row2 col1\" >mr williams i underst and she of the fire and myself in the morlocks on a cliltion and i saw the mame on a great the out </td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_d20cd_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
       "      <td id=\"T_d20cd_row3_col0\" class=\"data row3 col0\" >0.8</td>\n",
       "      <td id=\"T_d20cd_row3_col1\" class=\"data row3 col1\" >mr williams i understoo to and to durken the fast and shaps of find prestain into the medied and frrances to the excheti </td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_d20cd_level0_row4\" class=\"row_heading level0 row4\" >4</th>\n",
       "      <td id=\"T_d20cd_row4_col0\" class=\"data row4 col0\" >1.0</td>\n",
       "      <td id=\"T_d20cd_row4_col1\" class=\"data row4 col1\" >mr williams i understolfurully the sakent way and white to minute screal side cleading in the noffst most i ranntly of th</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x10ed52e90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display\n",
    "import pandas as pd\n",
    "\n",
    "pd.set_option(\"display.max_colwidth\", None)\n",
    "df = pd.DataFrame({\"temp\": [f\"{t:.1f}\" for t in temperature], \"text\": text})\n",
    "df = df.style.set_properties(**{\"text-align\": \"left\"})\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41c3ad44",
   "metadata": {
    "papermill": {
     "duration": 0.001571,
     "end_time": "2025-01-13T14:54:08.490606",
     "exception": false,
     "start_time": "2025-01-13T14:54:08.489035",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "The generated text appear more random as we increase the sampling temperature[^1]. Conversely, as the temperature decreases, the softmax function behaves more like an argmax. In this scenario, the sampling algorithm selects the token with the highest probability, which increases the likelihood of cycles.\n",
    "\n",
    "[^1]: That is, $e^s \\approx 1 + x$ for $|x| \\ll 1$, so that $p_k = \\frac{e^{s_k}}{\\sum_j e^{s_j}} \\approx \\frac{1 + s_k}{K + \\sum_j s_j}.$ "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc682eec",
   "metadata": {
    "papermill": {
     "duration": 0.001271,
     "end_time": "2025-01-13T14:54:08.493182",
     "exception": false,
     "start_time": "2025-01-13T14:54:08.491911",
     "status": "completed"
    },
    "tags": []
   },
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 3.258508,
   "end_time": "2025-01-13T14:54:09.015651",
   "environment_variables": {},
   "exception": null,
   "input_path": "05d-textgen.ipynb",
   "output_path": "05d-textgen.ipynb",
   "parameters": {},
   "start_time": "2025-01-13T14:54:05.757143",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}