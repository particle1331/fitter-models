
<!DOCTYPE html>


<html lang="en" data-content_root="../../" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>Activations and Gradients &#8212; OK Transformer</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../../_static/styles/theme.css?digest=3ee479438cf8b5e0d341" rel="stylesheet" />
<link href="../../_static/styles/bootstrap.css?digest=3ee479438cf8b5e0d341" rel="stylesheet" />
<link href="../../_static/styles/pydata-sphinx-theme.css?digest=3ee479438cf8b5e0d341" rel="stylesheet" />

  
  <link href="../../_static/vendor/fontawesome/6.5.2/css/all.min.css?digest=3ee479438cf8b5e0d341" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.5.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.5.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.5.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../../_static/pygments.css?v=fa44fd50" />
    <link rel="stylesheet" type="text/css" href="../../_static/styles/sphinx-book-theme.css?v=a3416100" />
    <link rel="stylesheet" type="text/css" href="../../_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="../../_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="../../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css?v=be8a1c11" />
    <link rel="stylesheet" type="text/css" href="../../_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="../../_static/sphinx-design.min.css?v=87e54e7c" />
    <link rel="stylesheet" type="text/css" href="../../_static/custom.css?v=564be945" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../../_static/scripts/bootstrap.js?digest=3ee479438cf8b5e0d341" />
<link rel="preload" as="script" href="../../_static/scripts/pydata-sphinx-theme.js?digest=3ee479438cf8b5e0d341" />
  <script src="../../_static/vendor/fontawesome/6.5.2/js/all.min.js?digest=3ee479438cf8b5e0d341"></script>

    <script src="../../_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="../../_static/doctools.js?v=9a2dae69"></script>
    <script src="../../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../../_static/copybutton.js?v=f281be69"></script>
    <script src="../../_static/scripts/sphinx-book-theme.js?v=887ef09a"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../../_static/togglebutton.js?v=4a39c7ea"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../../_static/design-tabs.js?v=f930bc37"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="../../_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'nb/dl/05-training';</script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    <link rel="icon" href="../../_static/favicon.png"/>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="Attention and Transformers" href="07-attention.html" />
    <link rel="prev" title="Language Modeling" href="04-lm.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-primary-sidebar-checkbox"/>
  <label class="overlay overlay-primary" for="pst-primary-sidebar-checkbox"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-secondary-sidebar-checkbox"/>
  <label class="overlay overlay-secondary" for="pst-secondary-sidebar-checkbox"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search..."
         aria-label="Search..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  

<a class="navbar-brand logo" href="../../intro.html">
  
  
  
  
  
    
    
      
    
    
    <img src="../../_static/logo.png" class="logo__image only-light" alt="OK Transformer - Home"/>
    <script>document.write(`<img src="../../_static/logo.png" class="logo__image only-dark" alt="OK Transformer - Home"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item">

 <script>
 document.write(`
   <button class="btn navbar-btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">Deep Learning</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="01-intro.html">Introduction to NNs</a></li>
<li class="toctree-l1"><a class="reference internal" href="02-optim.html">Optimization</a></li>
<li class="toctree-l1"><a class="reference internal" href="00-backprop.html">Backpropagation</a></li>
<li class="toctree-l1"><a class="reference internal" href="03-cnn.html">Convolutional Networks</a></li>
<li class="toctree-l1"><a class="reference internal" href="04-lm.html">Language Modeling</a></li>
<li class="toctree-l1 current active"><a class="current reference internal" href="#">Activations and Gradients</a></li>
<li class="toctree-l1"><a class="reference internal" href="07-attention.html">Attention and Transformers</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">ML Engineering &amp; MLOps</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../mlops/01-intro.html">Preliminaries</a></li>
<li class="toctree-l1"><a class="reference internal" href="../mlops/02-package.html">Packaging Modeling Code</a></li>
<li class="toctree-l1"><a class="reference internal" href="../mlops/03-mlflow.html">Experiment Tracking</a></li>
<li class="toctree-l1"><a class="reference internal" href="../mlops/04-tasks.html">Distributed Task Queues</a></li>
<li class="toctree-l1"><a class="reference internal" href="../mlops/04-deployment/notes.html">Model Deployment</a></li>
<li class="toctree-l1"><a class="reference internal" href="../mlops/06-best-practices/notes.html">Best Engineering Practices</a></li>
<li class="toctree-l1"><a class="reference internal" href="../mle/cicd-pipelines.html">Continuous Integration and Deployment Pipelines</a></li>
<li class="toctree-l1"><a class="reference internal" href="../mle/model-serving-api.html">Prediction Serving API</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Notes</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../notes/containers.html">Docker Containers</a></li>
<li class="toctree-l1"><a class="reference internal" href="../notes/tf-course.html">TensorFlow Crash Course</a></li>
<li class="toctree-l1"><a class="reference internal" href="../notes/benchmarking.html">Benchmarking and Profiling</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><button class="sidebar-toggle primary-toggle btn btn-sm" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</button></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/particle1331/ok-transformer" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/particle1331/ok-transformer/issues/new?title=Issue%20on%20page%20%2Fnb/dl/05-training.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="Open an issue"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm navbar-btn theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="theme-switch nav-link" data-mode="light"><i class="fa-solid fa-sun fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="dark"><i class="fa-solid fa-moon fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="auto"><i class="fa-solid fa-circle-half-stroke fa-lg"></i></span>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm navbar-btn search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<button class="sidebar-toggle secondary-toggle btn btn-sm" title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</button>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Activations and Gradients</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#introduction">Introduction</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#preliminaries">Preliminaries</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#datasets">Datasets</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#weight-initialization">Weight initialization</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#training-with-hooks">Training with hooks</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#histograms">Histograms</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#dying-units">Dying units</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#layer-normalization">Layer normalization</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#immediate-effects">Immediate effects</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#gradient-normalization">Gradient normalization</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#weight-update-ratio">Weight update ratio</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#appendix-activations">Appendix: Activations</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#appendix-rank-collapse">Appendix: Rank collapse</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#appendix-residual-connections">Appendix: Residual connections</a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="activations-and-gradients">
<span id="dl-05-training"></span><h1>Activations and Gradients<a class="headerlink" href="#activations-and-gradients" title="Link to this heading">#</a></h1>
<p><img alt="Status" src="https://img.shields.io/static/v1.svg?label=Status&amp;message=Finished&amp;color=brightgreen" />
<a class="reference external" href="https://github.com/particle1331/ok-transformer/blob/master/docs/nb/dl/05-training.ipynb"><img alt="Source" src="https://img.shields.io/static/v1.svg?label=GitHub&amp;message=Source&amp;color=181717&amp;logo=GitHub" /></a>
<a class="reference external" href="https://github.com/particle1331/ok-transformer"><img alt="Stars" src="https://img.shields.io/github/stars/particle1331/ok-transformer?style=social" /></a></p>
<hr class="docutils" />
<p><strong>Readings:</strong>  <span id="id1">[<a class="reference internal" href="../../intro.html#id37" title="Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E. Hinton. Layer normalization. 2016. URL: https://arxiv.org/abs/1607.06450, doi:10.48550/ARXIV.1607.06450.">BKH16</a>]</span> <span id="id2">[<a class="reference internal" href="../../intro.html#id27" title="Sergey Ioffe and Christian Szegedy. Batch normalization: accelerating deep network training by reducing internal covariate shift. CoRR, 2015. URL: http://arxiv.org/abs/1502.03167, arXiv:1502.03167.">IS15</a>]</span></p>
<section id="introduction">
<h2>Introduction<a class="headerlink" href="#introduction" title="Link to this heading">#</a></h2>
<p>Training neural nets involves computation across millions of weights and activations. Experience tells us that <a class="reference external" href="https://lossfunctions.tumblr.com/">this process is fragile</a>. In this notebook, we attach hooks in order to deep neural nets to analyze the statistics of activations and gradients during training, and consider pitfalls when they are improperly scaled. Finally, we introduce <strong>layer normalization</strong> (LN) <span id="id3">[<a class="reference internal" href="../../intro.html#id37" title="Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E. Hinton. Layer normalization. 2016. URL: https://arxiv.org/abs/1607.06450, doi:10.48550/ARXIV.1607.06450.">BKH16</a>]</span> which allows stable propagation of activations and gradients across layers. This makes training deep networks so much easier (e.g. without a lot of hyperparameter tuning).</p>
<p>In the appendix, we consider the effects of the choice of <strong>activation function</strong> on training dynamics. Deep MLPs suffer from <strong>rank collapse</strong> where the dimensionality of the output space of each layer degenerates with depth. We find empirically that <strong>batch normalization</strong> <span id="id4">[<a class="reference internal" href="../../intro.html#id27" title="Sergey Ioffe and Christian Szegedy. Batch normalization: accelerating deep network training by reducing internal covariate shift. CoRR, 2015. URL: http://arxiv.org/abs/1502.03167, arXiv:1502.03167.">IS15</a>]</span> avoids rank collapse in MLPs, while LN does not. This is consistent with theoretical results in <span id="id5">[<a class="reference internal" href="../../intro.html#id43" title="Hadi Daneshmand, Jonas Kohler, Francis Bach, Thomas Hofmann, and Aurelien Lucchi. Batch normalization provably avoids rank collapse for randomly initialised deep networks. 2020. arXiv:2003.01652.">DKB+20</a>]</span> and <span id="id6">[<a class="reference internal" href="../../intro.html#id107" title="Yihe Dong, Jean-Baptiste Cordonnier, and Andreas Loukas. Attention is not all you need: pure attention loses rank doubly exponentially with depth. CoRR, 2021. URL: https://arxiv.org/abs/2103.03404, arXiv:2103.03404.">DCL21</a>]</span>. Finally, we introduce <strong>residual connections</strong> which forces the network to model residuals to the identity function and allow multiple paths for the gradient to flow to earlier layers. It will be shown that residual connections diminish rank collapse.</p>
</section>
<section id="preliminaries">
<h2>Preliminaries<a class="headerlink" href="#preliminaries" title="Link to this heading">#</a></h2>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">math</span>
<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">random</span>
<span class="kn">import</span> <span class="nn">warnings</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">from</span> <span class="nn">pathlib</span> <span class="kn">import</span> <span class="n">Path</span>
<span class="kn">from</span> <span class="nn">matplotlib_inline</span> <span class="kn">import</span> <span class="n">backend_inline</span>

<span class="n">DATASET_DIR</span> <span class="o">=</span> <span class="n">Path</span><span class="p">(</span><span class="s2">&quot;./data&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">absolute</span><span class="p">()</span>
<span class="n">RANDOM_SEED</span> <span class="o">=</span> <span class="mi">1</span>
<span class="n">DEBUG</span> <span class="o">=</span> <span class="kc">False</span>
<span class="n">MATPLOTLIB_FORMAT</span> <span class="o">=</span> <span class="s2">&quot;png&quot;</span> <span class="k">if</span> <span class="n">DEBUG</span> <span class="k">else</span> <span class="s2">&quot;svg&quot;</span>

<span class="k">def</span> <span class="nf">set_seed</span><span class="p">(</span><span class="n">s</span><span class="p">:</span> <span class="nb">int</span><span class="p">):</span>
    <span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="n">s</span><span class="p">)</span>
    <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="n">s</span><span class="p">)</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="n">s</span><span class="p">)</span>

<span class="n">set_seed</span><span class="p">(</span><span class="n">RANDOM_SEED</span><span class="p">)</span>
<span class="n">warnings</span><span class="o">.</span><span class="n">simplefilter</span><span class="p">(</span><span class="n">action</span><span class="o">=</span><span class="s2">&quot;ignore&quot;</span><span class="p">)</span>
<span class="n">backend_inline</span><span class="o">.</span><span class="n">set_matplotlib_formats</span><span class="p">(</span><span class="n">MATPLOTLIB_FORMAT</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>Still using the names dataset from the <a class="reference internal" href="04-lm.html#dl-04-lm"><span class="std std-ref">previous notebook</span></a>:</p>
<div class="cell tag_hide-output docutils container">
<div class="cell_input above-output-prompt docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">os</span>
<span class="k">if</span> <span class="ow">not</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">isfile</span><span class="p">(</span><span class="s2">&quot;./data/surnames_freq_ge_100.csv&quot;</span><span class="p">):</span>
    <span class="o">!</span>wget<span class="w"> </span>-O<span class="w"> </span>./data/surnames_freq_ge_100.csv<span class="w"> </span>https://raw.githubusercontent.com/particle1331/spanish-names-surnames/master/surnames_freq_ge_100.csv
    <span class="o">!</span>wget<span class="w"> </span>-O<span class="w"> </span>./data/surnames_freq_ge_20_le_99.csv<span class="w"> </span>https://raw.githubusercontent.com/particle1331/spanish-names-surnames/master/surnames_freq_ge_20_le_99.csv
<span class="k">else</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Data files already exist.&quot;</span><span class="p">)</span>

<span class="n">col</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;surname&quot;</span><span class="p">,</span> <span class="s2">&quot;frequency_first&quot;</span><span class="p">,</span> <span class="s2">&quot;frequency_second&quot;</span><span class="p">,</span> <span class="s2">&quot;frequency_both&quot;</span><span class="p">]</span>
<span class="n">df1</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="n">DATASET_DIR</span> <span class="o">/</span> <span class="s2">&quot;surnames_freq_ge_100.csv&quot;</span><span class="p">,</span> <span class="n">names</span><span class="o">=</span><span class="n">col</span><span class="p">,</span> <span class="n">header</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">df2</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="n">DATASET_DIR</span> <span class="o">/</span> <span class="s2">&quot;surnames_freq_ge_20_le_99.csv&quot;</span><span class="p">,</span> <span class="n">names</span><span class="o">=</span><span class="n">col</span><span class="p">,</span> <span class="n">header</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
</pre></div>
</div>
</div>
<details class="hide below-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell output</span>
<span class="expanded">Hide code cell output</span>
</summary>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Data files already exist.
</pre></div>
</div>
</div>
</details>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">FRAC_LIMIT</span> <span class="o">=</span> <span class="mf">0.10</span> <span class="k">if</span> <span class="n">DEBUG</span> <span class="k">else</span> <span class="mf">1.0</span>
<span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">concat</span><span class="p">([</span><span class="n">df1</span><span class="p">,</span> <span class="n">df2</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)[[</span><span class="s2">&quot;surname&quot;</span><span class="p">]]</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="n">frac</span><span class="o">=</span><span class="n">FRAC_LIMIT</span><span class="p">)</span>
<span class="n">df</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">reset_index</span><span class="p">(</span><span class="n">drop</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">df</span><span class="p">[</span><span class="s2">&quot;surname&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="s2">&quot;surname&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="k">lambda</span> <span class="n">s</span><span class="p">:</span> <span class="n">s</span><span class="o">.</span><span class="n">lower</span><span class="p">())</span>
<span class="n">df</span><span class="p">[</span><span class="s2">&quot;surname&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="s2">&quot;surname&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="k">lambda</span> <span class="n">s</span><span class="p">:</span> <span class="n">s</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="s2">&quot;de la&quot;</span><span class="p">,</span> <span class="s2">&quot;dela&quot;</span><span class="p">))</span>
<span class="n">df</span><span class="p">[</span><span class="s2">&quot;surname&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="s2">&quot;surname&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="k">lambda</span> <span class="n">s</span><span class="p">:</span> <span class="n">s</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="s2">&quot; &quot;</span><span class="p">,</span> <span class="s2">&quot;_&quot;</span><span class="p">))</span>
<span class="n">df</span> <span class="o">=</span> <span class="n">df</span><span class="p">[[</span><span class="s2">&quot;surname&quot;</span><span class="p">]]</span><span class="o">.</span><span class="n">dropna</span><span class="p">()</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="nb">str</span><span class="p">)</span>

<span class="n">names</span> <span class="o">=</span> <span class="p">[</span>
    <span class="n">n</span> <span class="k">for</span> <span class="n">n</span> <span class="ow">in</span> <span class="n">df</span><span class="o">.</span><span class="n">surname</span><span class="o">.</span><span class="n">tolist</span><span class="p">()</span> 
    <span class="k">if</span> <span class="p">(</span><span class="s2">&quot;&#39;&quot;</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">n</span><span class="p">)</span> <span class="ow">and</span> <span class="p">(</span><span class="s1">&#39;รง&#39;</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">n</span><span class="p">)</span> <span class="ow">and</span> <span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">n</span><span class="p">)</span> <span class="o">&gt;=</span> <span class="mi">2</span><span class="p">)</span>
<span class="p">]</span>

<span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">5</span><span class="p">):</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">names</span><span class="p">[</span><span class="n">j</span><span class="p">])</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>agredano
poblador
girba
rabanales
yucra
</pre></div>
</div>
</div>
</div>
<section id="datasets">
<h3>Datasets<a class="headerlink" href="#datasets" title="Link to this heading">#</a></h3>
<p>Defining here the dataset class used in the <a class="reference internal" href="04-lm.html#dl-04-lm"><span class="std std-ref">previous notebook</span></a>:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">from</span> <span class="nn">torch.utils.data</span> <span class="kn">import</span> <span class="n">Dataset</span>

<span class="k">class</span> <span class="nc">CharDataset</span><span class="p">(</span><span class="n">Dataset</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">contexts</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="nb">str</span><span class="p">],</span> <span class="n">targets</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="nb">str</span><span class="p">],</span> <span class="n">chars</span><span class="p">:</span> <span class="nb">str</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">chars</span> <span class="o">=</span> <span class="n">chars</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">ys</span> <span class="o">=</span> <span class="n">targets</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">xs</span> <span class="o">=</span> <span class="n">contexts</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">block_size</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">contexts</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">itos</span> <span class="o">=</span> <span class="p">{</span><span class="n">i</span><span class="p">:</span> <span class="n">c</span> <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">c</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">chars</span><span class="p">)}</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">stoi</span> <span class="o">=</span> <span class="p">{</span><span class="n">c</span><span class="p">:</span> <span class="n">i</span> <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">c</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">itos</span><span class="o">.</span><span class="n">items</span><span class="p">()}</span>

    <span class="k">def</span> <span class="nf">get_vocab_size</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">chars</span><span class="p">)</span>
    
    <span class="k">def</span> <span class="fm">__len__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">xs</span><span class="p">)</span>
    
    <span class="k">def</span> <span class="fm">__getitem__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">idx</span><span class="p">):</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">xs</span><span class="p">[</span><span class="n">idx</span><span class="p">])</span>
        <span class="n">y</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">stoi</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">ys</span><span class="p">[</span><span class="n">idx</span><span class="p">]])</span><span class="o">.</span><span class="n">long</span><span class="p">()</span>
        <span class="k">return</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span> 
        
    <span class="k">def</span> <span class="nf">decode</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
        <span class="k">return</span> <span class="s2">&quot;&quot;</span><span class="o">.</span><span class="n">join</span><span class="p">([</span><span class="bp">self</span><span class="o">.</span><span class="n">itos</span><span class="p">[</span><span class="n">c</span><span class="o">.</span><span class="n">item</span><span class="p">()]</span> <span class="k">for</span> <span class="n">c</span> <span class="ow">in</span> <span class="n">x</span><span class="p">])</span>

    <span class="k">def</span> <span class="nf">encode</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">word</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="bp">self</span><span class="o">.</span><span class="n">stoi</span><span class="p">[</span><span class="n">c</span><span class="p">]</span> <span class="k">for</span> <span class="n">c</span> <span class="ow">in</span> <span class="n">word</span><span class="p">])</span><span class="o">.</span><span class="n">long</span><span class="p">()</span>


<span class="k">def</span> <span class="nf">build_dataset</span><span class="p">(</span><span class="n">names</span><span class="p">,</span> <span class="n">block_size</span><span class="o">=</span><span class="mi">3</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Build word context -&gt; next char target lists from names.&quot;&quot;&quot;</span>
    <span class="n">xs</span> <span class="o">=</span> <span class="p">[]</span>     <span class="c1"># context list</span>
    <span class="n">ys</span> <span class="o">=</span> <span class="p">[]</span>     <span class="c1"># target list</span>
    <span class="k">for</span> <span class="n">name</span> <span class="ow">in</span> <span class="n">names</span><span class="p">:</span>
        <span class="n">context</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;.&quot;</span><span class="p">]</span> <span class="o">*</span> <span class="n">block_size</span>
        <span class="k">for</span> <span class="n">c</span> <span class="ow">in</span> <span class="n">name</span> <span class="o">+</span> <span class="s2">&quot;.&quot;</span><span class="p">:</span>
            <span class="n">xs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">context</span><span class="p">)</span>
            <span class="n">ys</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">c</span><span class="p">)</span>
            <span class="n">context</span> <span class="o">=</span> <span class="n">context</span><span class="p">[</span><span class="mi">1</span><span class="p">:]</span> <span class="o">+</span> <span class="p">[</span><span class="n">c</span><span class="p">]</span>
    
    <span class="n">chars</span> <span class="o">=</span> <span class="nb">sorted</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="nb">set</span><span class="p">(</span><span class="s2">&quot;&quot;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">ys</span><span class="p">))))</span>
    <span class="k">return</span> <span class="n">CharDataset</span><span class="p">(</span><span class="n">contexts</span><span class="o">=</span><span class="n">xs</span><span class="p">,</span> <span class="n">targets</span><span class="o">=</span><span class="n">ys</span><span class="p">,</span> <span class="n">chars</span><span class="o">=</span><span class="n">chars</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>Example dataset with block size 3:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">dataset</span> <span class="o">=</span> <span class="n">build_dataset</span><span class="p">(</span><span class="n">names</span><span class="p">,</span> <span class="n">block_size</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
<span class="n">xs</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">ys</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">7</span><span class="p">):</span>
    <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">dataset</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
    <span class="n">xs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">ys</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;&quot;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">dataset</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">x</span><span class="p">)),</span> <span class="s2">&quot;--&gt;&quot;</span><span class="p">,</span> <span class="n">dataset</span><span class="o">.</span><span class="n">itos</span><span class="p">[</span><span class="n">y</span><span class="o">.</span><span class="n">item</span><span class="p">()])</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>... --&gt; a
..a --&gt; g
.ag --&gt; r
agr --&gt; e
gre --&gt; d
red --&gt; a
eda --&gt; n
</pre></div>
</div>
</div>
</div>
<p>Ideally, we should use a stratified <em>k</em>-fold that ensures character distribution is the same between splits. But too lazy. Here we just partition the names dataset by index to create the validation and train datasets.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">SPLIT_RATIO</span> <span class="o">=</span> <span class="mf">0.30</span>
<span class="n">split_point</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">SPLIT_RATIO</span> <span class="o">*</span> <span class="nb">len</span><span class="p">(</span><span class="n">names</span><span class="p">))</span>
<span class="n">names_train</span> <span class="o">=</span> <span class="n">names</span><span class="p">[:</span><span class="n">split_point</span><span class="p">]</span>
<span class="n">names_valid</span> <span class="o">=</span> <span class="n">names</span><span class="p">[</span><span class="n">split_point</span><span class="p">:]</span>
<span class="n">train_dataset</span> <span class="o">=</span> <span class="n">build_dataset</span><span class="p">(</span><span class="n">names_train</span><span class="p">,</span> <span class="n">block_size</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
<span class="n">valid_dataset</span> <span class="o">=</span> <span class="n">build_dataset</span><span class="p">(</span><span class="n">names_valid</span><span class="p">,</span> <span class="n">block_size</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>

<span class="nb">len</span><span class="p">(</span><span class="n">train_dataset</span><span class="p">),</span> <span class="nb">len</span><span class="p">(</span><span class="n">valid_dataset</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>(186339, 433815)
</pre></div>
</div>
</div>
</div>
</section>
</section>
<section id="weight-initialization">
<h2>Weight initialization<a class="headerlink" href="#weight-initialization" title="Link to this heading">#</a></h2>
<p>SGD requires choosing an arbitrary starting point <span class="math notranslate nohighlight">\(\boldsymbol{\Theta}_{\text{init}}.\)</span> Setting all weights to zero or some constant does not work as symmetry in the neurons of the network will make it difficult (if not impossible) to train the model. Hence, setting the weights randomly to break symmetry is a good starting point. However, this is still not enough since the variance of every neuron is <a class="reference external" href="https://www.deeplearning.ai/ai-notes/initialization/index.html#IV">additive</a>  (again due to symmetry and some assumptions):</p>
<div class="math notranslate nohighlight">
\[{\sigma_{\boldsymbol{\mathsf{y}}}} = \sqrt{n} \cdot {\sigma_{\boldsymbol{\mathsf{w}}}} \, {\sigma_{\boldsymbol{\mathsf{x}}}}\]</div>
<p>where <span class="math notranslate nohighlight">\(n = |\boldsymbol{\mathsf{x}}|.\)</span></p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">seaborn</span> <span class="k">as</span> <span class="nn">sns</span>

<span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1000</span><span class="p">,</span> <span class="mi">100</span><span class="p">)</span>
<span class="n">w</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span> <span class="mi">100</span><span class="p">,</span> <span class="mi">200</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">x</span> <span class="o">@</span> <span class="n">w</span>
</pre></div>
</div>
</div>
</div>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>
<span class="n">sns</span><span class="o">.</span><span class="n">distplot</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">),</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;C0&quot;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;$\mathbf</span><span class="si">{x}</span><span class="s1">$ (input)&#39;</span><span class="p">);</span>
<span class="n">sns</span><span class="o">.</span><span class="n">distplot</span><span class="p">(</span><span class="n">y</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">),</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;C1&quot;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;$\mathbf</span><span class="si">{y}</span><span class="s1">$ (output)&#39;</span><span class="p">);</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">();</span>
</pre></div>
</div>
</div>
</details>
<div class="cell_output docutils container">
<img alt="../../_images/d7b4c92897bb594963577478edc77cfe73c3d4a600562ba1dbf69dbdd4078068.svg" src="../../_images/d7b4c92897bb594963577478edc77cfe73c3d4a600562ba1dbf69dbdd4078068.svg" /></div>
</div>
<p>Observe that the output <span class="math notranslate nohighlight">\(\boldsymbol{\mathsf{y}} = \boldsymbol{\mathsf{x}}^\top \boldsymbol{\mathsf{w}}\)</span> for normally distributed <span class="math notranslate nohighlight">\(\boldsymbol{\mathsf{x}}\)</span> and <span class="math notranslate nohighlight">\(\boldsymbol{\mathsf{w}}\)</span> starts to spread out. This makes sense since we are adding <span class="math notranslate nohighlight">\(n\)</span> terms where where <span class="math notranslate nohighlight">\(n = |\boldsymbol{\mathsf{x}}|.\)</span> Ideally, we want <span class="math notranslate nohighlight">\(\boldsymbol{\mathsf{y}}\)</span> to have the same standard deviation to maintain stable activation flow. Otherwise, activations will recursively grow at each layer. This becomes increasingly problematic with depth.</p>
<br>
<p><strong>Dead neurons.</strong> Activations have regions where their gradients saturate, so it is very important to control the range of preactivations. A dead neuron has vanishingly small gradients for most training examples. This means that the weights for this neuron will change very slowly compared to other neurons.</p>
<p>A neuron can be dead at initialization or die in the course of training.
For instance, a high learning rate can result in large weights that saturate the neurons similar to the situation above. For dense layers, dead neurons tend to remain dead for a long time since weight gradients are too small to significantly change the existing weights for that neuron.</p>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">activations</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s2">&quot;tanh&quot;</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tanh</span><span class="p">,</span>
    <span class="s2">&quot;relu&quot;</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">relu</span><span class="p">,</span>
<span class="p">}</span>

<span class="k">def</span> <span class="nf">d</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Compute derivative of activation with respect to x.&quot;&quot;&quot;</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">f</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">y</span><span class="o">.</span><span class="n">backward</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">ones_like</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">x</span><span class="o">.</span><span class="n">grad</span>

<span class="k">def</span> <span class="nf">plot_activation</span><span class="p">(</span><span class="n">act</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">ax</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
    <span class="n">x</span><span class="o">.</span><span class="n">grad</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="n">f</span> <span class="o">=</span> <span class="n">activations</span><span class="p">[</span><span class="n">act</span><span class="p">]</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">f</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">df</span> <span class="o">=</span> <span class="n">d</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>
    <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">(),</span> <span class="n">y</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
    
    <span class="c1"># Plotting</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span>  <span class="n">linewidth</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;red&#39;</span><span class="p">,</span>   <span class="n">label</span><span class="o">=</span><span class="s2">&quot;f(x)&quot;</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">df</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;black&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;f&#39;(x)&quot;</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="n">act</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="n">linestyle</span><span class="o">=</span><span class="s2">&quot;dashed&quot;</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_ylim</span><span class="p">(</span><span class="o">-</span><span class="mf">1.5</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">act</span> <span class="o">==</span> <span class="s2">&quot;tanh&quot;</span><span class="p">:</span>
        <span class="n">ax</span><span class="o">.</span><span class="n">axvspan</span><span class="p">(</span><span class="o">-</span><span class="mi">5</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.8183</span><span class="p">,</span> <span class="o">-</span><span class="mi">10</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;lightgray&#39;</span><span class="p">)</span>
        <span class="n">ax</span><span class="o">.</span><span class="n">axvspan</span><span class="p">(</span><span class="mf">1.8183</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span>   <span class="o">-</span><span class="mi">10</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;lightgray&#39;</span><span class="p">)</span>
    <span class="k">elif</span> <span class="n">act</span> <span class="o">==</span> <span class="s2">&quot;relu&quot;</span><span class="p">:</span>
        <span class="n">ax</span><span class="o">.</span><span class="n">axvspan</span><span class="p">(</span><span class="o">-</span><span class="mi">5</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="o">-</span><span class="mi">10</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;lightgray&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;|f&#39;(x)| โค 0.1&quot;</span><span class="p">)</span>
        <span class="n">ax</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="s1">&#39;lower right&#39;</span><span class="p">)</span>
    

<span class="c1"># Plotting</span>
<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mf">3.5</span><span class="p">))</span>

<span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">1000</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">act_name</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">activations</span><span class="o">.</span><span class="n">keys</span><span class="p">()):</span>
    <span class="n">plot_activation</span><span class="p">(</span><span class="n">act_name</span><span class="p">,</span> <span class="n">ax</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">x</span><span class="p">)</span>  <span class="c1"># divmod(m, n) = m // n, m % n</span>

<span class="n">fig</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
</pre></div>
</div>
</div>
</details>
<div class="cell_output docutils container">
<img alt="../../_images/c46977541bb3ceb362a277f9c486e0f142ea0eda0f5e02a65f6403dfb3bfcd92.svg" src="../../_images/c46977541bb3ceb362a277f9c486e0f142ea0eda0f5e02a65f6403dfb3bfcd92.svg" /></div>
</div>
<p><strong>Figure.</strong> Plotting activations and activation gradients. Saturation regions where derivatives are small (here set to <code class="docutils literal notranslate"><span class="pre">0.1</span></code>) are highlighted gray.</p>
<br>
<p><strong>Xavier init.</strong> <span id="id7">[<a class="reference internal" href="../../intro.html#id40" title="Xavier Glorot and Yoshua Bengio. Understanding the difficulty of training deep feedforward neural networks. In Yee Whye Teh and Mike Titterington, editors, Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics, volume 9 of Proceedings of Machine Learning Research, 249โ256. Chia Laguna Resort, Sardinia, Italy, 13โ15 May 2010. PMLR. URL: https://proceedings.mlr.press/v9/glorot10a.html.">GB10</a>]</span> Since the <span class="math notranslate nohighlight">\(\sigma_{\boldsymbol{\mathsf{y}}} = \sqrt{n} \cdot \sigma_{\boldsymbol{\mathsf{w}}} \, \sigma_{\boldsymbol{\mathsf{x}}}\)</span>, one straightforward fix is to initialize the weights <span class="math notranslate nohighlight">\(\boldsymbol{\mathsf{w}}\)</span> with a distribution having <span class="math notranslate nohighlight">\({\sigma_{\boldsymbol{\mathsf{w}}}}=\frac{1}{\sqrt{n}}\)</span> where <span class="math notranslate nohighlight">\(n = |\boldsymbol{\mathsf{x}}|.\)</span> Moreover, we set biases to zero. Note that setting the standard deviation turns out to be equivalent to just scaling the random variable with <span class="math notranslate nohighlight">\(\frac{1}{\sqrt{n}}\)</span> by linearity of expectation:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1000</span><span class="p">,</span> <span class="mi">100</span><span class="p">)</span>
<span class="n">w</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span> <span class="mi">100</span><span class="p">,</span> <span class="mi">200</span><span class="p">)</span>  <span class="c1"># Xavier normal. For xavier uniform, set w ~ U[-a, a] with a = (3 / n) ** 0.5</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">x</span> <span class="o">@</span> <span class="p">(</span><span class="n">w</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="mi">100</span><span class="p">))</span>
</pre></div>
</div>
</div>
</div>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>
<span class="n">sns</span><span class="o">.</span><span class="n">distplot</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">),</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;C0&quot;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;$\mathbf</span><span class="si">{x}</span><span class="s1">$ (input)&#39;</span><span class="p">)</span>
<span class="n">sns</span><span class="o">.</span><span class="n">distplot</span><span class="p">(</span><span class="n">y</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">),</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;C1&quot;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;$\mathbf</span><span class="si">{y}</span><span class="s1">$ (output)&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mf">0.6</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">();</span>
</pre></div>
</div>
</div>
</details>
<div class="cell_output docutils container">
<img alt="../../_images/d6af6bda97431cf740559390857db8ebc067fbb0f80edd29c88709e6f1cdee2d.svg" src="../../_images/d6af6bda97431cf740559390857db8ebc067fbb0f80edd29c88709e6f1cdee2d.svg" /></div>
</div>
<p><strong>Remark.</strong> For dense layers, the BP equation that relates input and output gradients is linear with <span class="math notranslate nohighlight">\(\boldsymbol{\mathsf{w}}^\top.\)</span>
Improper scaling of weights can therefore cause <strong>vanishing</strong> or <strong>exploding gradients</strong> for MLPs at initialization, i.e. recursive multiplication of random weight matrices can increase or decrease gradients exponentially with depth.
Hence, we can alternatively sample weights with <span class="math notranslate nohighlight">\(\sigma_{\boldsymbol{\mathsf{w}}} = \frac{1} {\sqrt{n_\text{out}}}.\)</span> Indeed, weight initialization is sometimes implemented with <a class="reference external" href="https://pytorch.org/docs/stable/nn.init.html#nn.init.xavier_normal_">fan-average</a> mode:</p>
<div class="math notranslate nohighlight">
\[\sigma_{\boldsymbol{\mathsf{w}}} = \sqrt{\frac{2}{{n_\text{in} + n_\text{out}}}}.\]</div>
<br>
<p><strong>Gain.</strong> Note that this scale factor only holds for linear layers. Nonlinear activations squashes its input which compounds as we stack layers in deep networks. The factor <span class="math notranslate nohighlight">\(\mathsf{g}\)</span> such that <span class="math notranslate nohighlight">\({\sigma_{\boldsymbol{\mathsf{y}}}} = \mathsf{g} \cdot {\sigma_{\boldsymbol{\mathsf{x}}}}\)</span> in called <a class="reference external" href="https://pytorch.org/docs/stable/nn.init.html#nn.init.calculate_gain">gain</a>. This can be introduced as a factor on the standard deviation of the weights, i.e. setting the parameters of the distribution such that <span class="math notranslate nohighlight">\(\sigma_{\boldsymbol{\mathsf{w}}} = \mathsf{g} \frac{1}{\sqrt{n}}\)</span> where <span class="math notranslate nohighlight">\(n = |\boldsymbol{\mathsf{x}}|\)</span> for some <span class="math notranslate nohighlight">\(\mathsf{g} &gt; 0.\)</span>
Typically, weights are sampled from either normal or uniform distributions.</p>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>

<span class="n">n</span> <span class="o">=</span> <span class="mi">100</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1000</span><span class="p">,</span> <span class="n">n</span><span class="p">)</span>
<span class="n">z</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tanh</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>   <span class="c1"># Note: input of a hidden layer is an activation</span>
<span class="n">w</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="mi">200</span><span class="p">)</span> <span class="o">/</span> <span class="n">n</span> <span class="o">**</span> <span class="mf">0.5</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">z</span> <span class="o">@</span> <span class="n">w</span>

<span class="n">sns</span><span class="o">.</span><span class="n">distplot</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">),</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;C0&quot;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;$\mathbf</span><span class="si">{x}</span><span class="s1">$ (input)&#39;</span><span class="p">);</span>
<span class="n">sns</span><span class="o">.</span><span class="n">distplot</span><span class="p">(</span><span class="n">y</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">),</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;C1&quot;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;$\mathbf</span><span class="si">{y}</span><span class="s1">$ (output)&#39;</span><span class="p">);</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="sa">r</span><span class="s2">&quot;$w \sim N(0, \sigma^2),$ $\sigma = n ^{-0.5}$&quot;</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="mi">12</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylim</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mf">0.8</span><span class="p">)</span>

<span class="n">g</span> <span class="o">=</span> <span class="mi">5</span> <span class="o">/</span> <span class="mi">3</span>   <span class="c1"># tanh gain</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1000</span><span class="p">,</span> <span class="n">n</span><span class="p">)</span>
<span class="n">z</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tanh</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="n">w</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="mi">200</span><span class="p">)</span> <span class="o">*</span> <span class="n">g</span> <span class="o">/</span> <span class="n">n</span> <span class="o">**</span> <span class="mf">0.5</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">z</span> <span class="o">@</span> <span class="n">w</span>

<span class="n">sns</span><span class="o">.</span><span class="n">distplot</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">),</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;C0&quot;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;$\mathbf</span><span class="si">{x}</span><span class="s1">$ (input)&#39;</span><span class="p">);</span>
<span class="n">sns</span><span class="o">.</span><span class="n">distplot</span><span class="p">(</span><span class="n">y</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">),</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;C1&quot;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;$\mathbf</span><span class="si">{y}</span><span class="s1">$ (output)&#39;</span><span class="p">);</span>

<span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="sa">r</span><span class="s2">&quot;$w \sim N(0, \sigma^2),$ $\sigma = \frac</span><span class="si">{5}{3}</span><span class="s2"> </span><span class="si">{n}</span><span class="s2">^{-0.5}$&quot;</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="mi">12</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylim</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mf">0.8</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">fig</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">();</span>
</pre></div>
</div>
</div>
</details>
<div class="cell_output docutils container">
<img alt="../../_images/18eb2db81382ab276018d6fa8975bb8969deb37f6b5e1611ce9c833d604c9111.svg" src="../../_images/18eb2db81382ab276018d6fa8975bb8969deb37f6b5e1611ce9c833d604c9111.svg" /></div>
</div>
<br>
<p><strong>Kaiming init.</strong> Note that <span class="math notranslate nohighlight">\(\mathsf{g}\)</span> for an activation is usually obtained using some heuristic or by performing <a class="reference external" href="https://github.com/pytorch/pytorch/issues/24991">empirical tests</a>, e.g. <span id="id8">[<a class="reference internal" href="../../intro.html#id46" title="Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Delving deep into rectifiers: surpassing human-level performance on imagenet classification. CoRR, 2015. URL: http://arxiv.org/abs/1502.01852, arXiv:1502.01852.">HZRS15b</a>]</span> sets <span class="math notranslate nohighlight">\(\mathsf{g} = \sqrt{2}\)</span> since half of ReLU outputs are zero in the calculation of variance (see <a class="reference external" href="https://courses.cs.washington.edu/courses/cse543/23au/schedule/Kaiming_init.pdf">proof</a>). See also <a class="reference external" href="https://pytorch.org/docs/stable/nn.init.html#nn.init.calculate_gain"><code class="docutils literal notranslate"><span class="pre">calculate_gain</span></code></a> for other commonly used activations.</p>
<p>PyTorch default for <code class="docutils literal notranslate"><span class="pre">nn.Linear</span></code> is <a class="reference external" href="https://github.com/pytorch/pytorch/issues/57109#issuecomment-1310430244">essentially</a> <span class="math notranslate nohighlight">\(\mathsf{g} = \frac{1}{\sqrt{3}}\)</span> with Xavier uniform fan-in initialization. Note that the standard deviation of the uniform distribution <span class="math notranslate nohighlight">\(U[-a, a]\)</span> is specified by the bound <span class="math notranslate nohighlight">\(a\)</span>, i.e. <span class="math notranslate nohighlight">\(\sigma = \frac{a}{\sqrt{3}}.\)</span> The Pytorch implementation sets <span class="math notranslate nohighlight">\(a = \frac{1}{\sqrt{n}}\)</span> getting an effective gain of <span class="math notranslate nohighlight">\(\frac{1}{\sqrt{3}}.\)</span></p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="nn">nn</span>

<span class="n">fan_in</span> <span class="o">=</span> <span class="mi">100</span>
<span class="n">lin</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">fan_in</span><span class="p">,</span> <span class="mi">1234</span><span class="p">)</span>
<span class="n">w</span> <span class="o">=</span> <span class="n">lin</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">data</span>
<span class="n">w</span><span class="o">.</span><span class="n">std</span><span class="p">(),</span> <span class="mi">1</span> <span class="o">/</span> <span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="mi">3</span><span class="p">)</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">fan_in</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>(tensor(0.0576), 0.05773502691896258)
</pre></div>
</div>
</div>
</div>
<p><strong>Remark.</strong> The formula for <span class="math notranslate nohighlight">\(\sigma\)</span> of <span class="math notranslate nohighlight">\(U[-a, a]\)</span> also explains the form of <a class="reference external" href="https://pytorch.org/docs/stable/nn.init.html#nn.init.kaiming_uniform_"><code class="docutils literal notranslate"><span class="pre">nn.init.kaiming_uniform_</span></code></a>.</p>
<br>
<p><strong>Logits.</strong> Xavier initialization can also be applied to the logits layer. It essentially acts as softmax <strong>temperature</strong>:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">torch.nn.functional</span> <span class="k">as</span> <span class="nn">F</span>

<span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">30</span><span class="p">)</span>
<span class="n">w</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">30</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>
<span class="n">y0</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">x</span> <span class="o">@</span> <span class="n">w</span><span class="p">)</span>
<span class="n">y1</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">x</span> <span class="o">@</span> <span class="p">(</span><span class="n">w</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="mi">30</span><span class="p">)))</span>
</pre></div>
</div>
</div>
</div>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="sa">r</span><span class="s2">&quot;$\sigma = 1$&quot;</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="mi">11</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="sa">r</span><span class="s2">&quot;$\sigma = </span><span class="si">{1}</span><span class="s2">/{\sqrt</span><span class="si">{30}</span><span class="s2">}$&quot;</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="mi">11</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s2">&quot;class index&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s2">&quot;class index&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">bar</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">10</span><span class="p">),</span> <span class="n">y0</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">bar</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">10</span><span class="p">),</span> <span class="n">y1</span><span class="p">[</span><span class="mi">0</span><span class="p">]);</span>
</pre></div>
</div>
</div>
</details>
<div class="cell_output docutils container">
<img alt="../../_images/b528bb32f706b03180b6d00dfd99332a93cde9258062b1601509e345abdc931d.svg" src="../../_images/b528bb32f706b03180b6d00dfd99332a93cde9258062b1601509e345abdc931d.svg" /></div>
</div>
</section>
<section id="training-with-hooks">
<h2>Training with hooks<a class="headerlink" href="#training-with-hooks" title="Link to this heading">#</a></h2>
<p>In our experiments, we use total steps per run instead of epochs.
The following class wraps a data loader so that the iterator resets whenever the number of steps exceed one epoch (i.e. whenever a single pass over the entire training dataset has been reached):</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">InfiniteDataLoader</span><span class="p">:</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">data_loader</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">data_loader</span> <span class="o">=</span> <span class="n">data_loader</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">data_iterator</span> <span class="o">=</span> <span class="nb">iter</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">data_loader</span><span class="p">)</span>

    <span class="k">def</span> <span class="fm">__iter__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span>

    <span class="k">def</span> <span class="fm">__next__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">try</span><span class="p">:</span>
            <span class="n">batch</span> <span class="o">=</span> <span class="nb">next</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">data_iterator</span><span class="p">)</span>
        <span class="k">except</span> <span class="ne">StopIteration</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">data_iterator</span> <span class="o">=</span> <span class="nb">iter</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">data_loader</span><span class="p">)</span>
            <span class="n">batch</span> <span class="o">=</span> <span class="nb">next</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">data_iterator</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">batch</span>
</pre></div>
</div>
</div>
</div>
<p>The <code class="docutils literal notranslate"><span class="pre">Trainer</span></code> class below streamlines model training (see <a class="reference internal" href="03-cnn.html#dl-03-cnn"><span class="std std-ref">previously notebook</span></a>). Note that the class is augmented with  <strong>hooks</strong> which we will use to obtain activation and gradient statistics during forward and backward passes. The hooks are attached to the model before training, then removed after. We skip validation during training since our current concern is training dynamics (not generalization).</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">tqdm.notebook</span> <span class="kn">import</span> <span class="n">tqdm</span>
<span class="kn">from</span> <span class="nn">contextlib</span> <span class="kn">import</span> <span class="n">contextmanager</span>
<span class="kn">from</span> <span class="nn">torch.utils.data</span> <span class="kn">import</span> <span class="n">DataLoader</span>

<span class="n">DEVICE</span> <span class="o">=</span> <span class="s2">&quot;mps&quot;</span>


<span class="nd">@contextmanager</span>
<span class="k">def</span> <span class="nf">eval_context</span><span class="p">(</span><span class="n">model</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Temporarily set to eval mode inside context.&quot;&quot;&quot;</span>
    <span class="n">state</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">training</span>
    <span class="n">model</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>
    <span class="k">try</span><span class="p">:</span>
        <span class="k">yield</span>
    <span class="k">finally</span><span class="p">:</span>
        <span class="n">model</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="n">state</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">Trainer</span><span class="p">:</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
                 <span class="n">model</span><span class="p">,</span> <span class="n">optim</span><span class="p">,</span> <span class="n">loss_fn</span><span class="p">,</span> 
                 <span class="n">scheduler</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                 <span class="n">callbacks</span><span class="o">=</span><span class="p">[],</span>
                 <span class="n">device</span><span class="o">=</span><span class="n">DEVICE</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
        
        <span class="bp">self</span><span class="o">.</span><span class="n">model</span> <span class="o">=</span> <span class="n">model</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">optim</span> <span class="o">=</span> <span class="n">optim</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">device</span> <span class="o">=</span> <span class="n">device</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">loss_fn</span> <span class="o">=</span> <span class="n">loss_fn</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">logs</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;train&quot;</span><span class="p">:</span> <span class="p">{</span><span class="s2">&quot;loss&quot;</span><span class="p">:</span> <span class="p">[]}}</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">verbose</span> <span class="o">=</span> <span class="n">verbose</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">scheduler</span> <span class="o">=</span> <span class="n">scheduler</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">callbacks</span> <span class="o">=</span> <span class="n">callbacks</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">forward_hooks</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">backward_hooks</span> <span class="o">=</span> <span class="p">[]</span>
    
    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">))</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">):</span>
        <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">batch</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
        <span class="n">y</span> <span class="o">=</span> <span class="n">y</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="n">y</span>

    <span class="k">def</span> <span class="nf">train_step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">):</span>
        <span class="n">preds</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">batch</span><span class="p">)</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">loss_fn</span><span class="p">(</span><span class="n">preds</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
        <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
        <span class="k">return</span> <span class="n">loss</span>

    <span class="nd">@torch</span><span class="o">.</span><span class="n">inference_mode</span><span class="p">()</span>
    <span class="k">def</span> <span class="nf">valid_step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">):</span>
        <span class="n">preds</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">batch</span><span class="p">)</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">loss_fn</span><span class="p">(</span><span class="n">preds</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">reduction</span><span class="o">=</span><span class="s2">&quot;sum&quot;</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">loss</span>

    <span class="k">def</span> <span class="nf">register_hooks</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>            <span class="c1"># !!!</span>
                       <span class="n">fwd_hooks</span><span class="o">=</span><span class="p">[],</span> 
                       <span class="n">bwd_hooks</span><span class="o">=</span><span class="p">[],</span> 
                       <span class="n">layer_types</span><span class="o">=</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">)):</span>
        
        <span class="k">for</span> <span class="n">_</span><span class="p">,</span> <span class="n">module</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">named_modules</span><span class="p">():</span>
            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="n">layer_types</span><span class="p">):</span>
                <span class="k">for</span> <span class="n">f</span> <span class="ow">in</span> <span class="n">fwd_hooks</span><span class="p">:</span>
                    <span class="n">h</span> <span class="o">=</span> <span class="n">module</span><span class="o">.</span><span class="n">register_forward_hook</span><span class="p">(</span><span class="n">f</span><span class="p">)</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">forward_hooks</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">h</span><span class="p">)</span>
                
                <span class="k">for</span> <span class="n">g</span> <span class="ow">in</span> <span class="n">bwd_hooks</span><span class="p">:</span>
                    <span class="n">h</span> <span class="o">=</span> <span class="n">module</span><span class="o">.</span><span class="n">register_backward_hook</span><span class="p">(</span><span class="n">g</span><span class="p">)</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">backward_hooks</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">h</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">remove_hooks</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">h</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">forward_hooks</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">backward_hooks</span><span class="p">:</span>
            <span class="n">h</span><span class="o">.</span><span class="n">remove</span><span class="p">()</span>
    
    <span class="k">def</span> <span class="nf">run</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">steps</span><span class="p">,</span> <span class="n">train_loader</span><span class="p">):</span>
        <span class="n">train_loader</span> <span class="o">=</span> <span class="n">InfiniteDataLoader</span><span class="p">(</span><span class="n">train_loader</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="n">tqdm</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="n">steps</span><span class="p">)):</span>
            <span class="c1"># optim and lr step</span>
            <span class="n">batch</span> <span class="o">=</span> <span class="nb">next</span><span class="p">(</span><span class="n">train_loader</span><span class="p">)</span>
            <span class="n">loss</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">train_step</span><span class="p">(</span><span class="n">batch</span><span class="p">)</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">scheduler</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">scheduler</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>

            <span class="c1"># step callbacks</span>
            <span class="k">for</span> <span class="n">callback</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">callbacks</span><span class="p">:</span>
                <span class="n">callback</span><span class="p">()</span>

            <span class="c1"># logs @ train step</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">logs</span><span class="p">[</span><span class="s2">&quot;train&quot;</span><span class="p">][</span><span class="s2">&quot;loss&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">())</span>

    <span class="k">def</span> <span class="nf">evaluate</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">data_loader</span><span class="p">):</span>
        <span class="k">with</span> <span class="n">eval_context</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="p">):</span>
            <span class="n">valid_loss</span> <span class="o">=</span> <span class="mf">0.0</span>
            <span class="k">for</span> <span class="n">batch</span> <span class="ow">in</span> <span class="n">data_loader</span><span class="p">:</span>
                <span class="n">loss</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">valid_step</span><span class="p">(</span><span class="n">batch</span><span class="p">)</span>
                <span class="n">valid_loss</span> <span class="o">+=</span> <span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>

        <span class="k">return</span> <span class="p">{</span><span class="s2">&quot;loss&quot;</span><span class="p">:</span> <span class="n">valid_loss</span> <span class="o">/</span> <span class="nb">len</span><span class="p">(</span><span class="n">data_loader</span><span class="o">.</span><span class="n">dataset</span><span class="p">)}</span>
</pre></div>
</div>
</div>
</div>
<p>Forward hooks are expected to look like:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">hook</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="nb">input</span><span class="p">,</span> <span class="n">output</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span> <span class="ow">or</span> <span class="n">modified</span> <span class="n">output</span>
</pre></div>
</div>
<p>On the other hand, backward hook functions should look like:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">hook</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="n">grad_in</span><span class="p">,</span> <span class="n">grad_out</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span> <span class="ow">or</span> <span class="n">modified</span> <span class="n">grad_in</span>
</pre></div>
</div>
<br>
<p>The following classes are for handling outputs of the hook functions:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">HookHandler</span><span class="p">:</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">records</span> <span class="o">=</span> <span class="p">{}</span>

    <span class="k">def</span> <span class="nf">hook_fn</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">module</span><span class="p">,</span> <span class="nb">input</span><span class="p">,</span> <span class="n">output</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">NotImplementedError</span>


<span class="k">class</span> <span class="nc">OutputStats</span><span class="p">(</span><span class="n">HookHandler</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">hook_fn</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">module</span><span class="p">,</span> <span class="nb">input</span><span class="p">,</span> <span class="n">output</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">records</span><span class="p">[</span><span class="n">module</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">records</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="p">[])</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">records</span><span class="p">[</span><span class="n">module</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">output</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">WeightGradientStats</span><span class="p">(</span><span class="n">HookHandler</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">hook_fn</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">module</span><span class="p">,</span> <span class="n">grad_in</span><span class="p">,</span> <span class="n">grad_out</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">records</span><span class="p">[</span><span class="n">module</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">records</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="p">[])</span>
        <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="n">grad_in</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">t</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">t</span><span class="o">.</span><span class="n">shape</span> <span class="o">==</span> <span class="n">module</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">shape</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">records</span><span class="p">[</span><span class="n">module</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">t</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">ActivationGradientStats</span><span class="p">(</span><span class="n">HookHandler</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">hook_fn</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">module</span><span class="p">,</span> <span class="n">grad_in</span><span class="p">,</span> <span class="n">grad_out</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">records</span><span class="p">[</span><span class="n">module</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">records</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="p">[])</span>
        <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="n">grad_in</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">t</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">records</span><span class="p">[</span><span class="n">module</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">grad_in</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>


<span class="k">class</span> <span class="nc">DyingReluStats</span><span class="p">(</span><span class="n">HookHandler</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">sat_threshold</span><span class="o">=</span><span class="mf">1e-8</span><span class="p">,</span> <span class="n">frac_threshold</span><span class="o">=</span><span class="mf">0.95</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">sat_threshold</span> <span class="o">=</span> <span class="n">sat_threshold</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">frac_threshold</span> <span class="o">=</span> <span class="n">frac_threshold</span>

    <span class="k">def</span> <span class="nf">hook_fn</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">module</span><span class="p">,</span> <span class="nb">input</span><span class="p">,</span> <span class="n">output</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">records</span><span class="p">[</span><span class="n">module</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">records</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="p">[])</span>
        <span class="n">B</span> <span class="o">=</span> <span class="n">output</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="n">count_dead</span> <span class="o">=</span> <span class="p">((</span><span class="n">output</span> <span class="o">&lt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">sat_threshold</span><span class="p">)</span><span class="o">.</span><span class="n">float</span><span class="p">()</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span> <span class="o">/</span> <span class="n">B</span><span class="p">)</span> <span class="o">&gt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">frac_threshold</span>
        <span class="n">frac_dead</span> <span class="o">=</span> <span class="n">count_dead</span><span class="o">.</span><span class="n">float</span><span class="p">()</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">records</span><span class="p">[</span><span class="n">module</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">frac_dead</span><span class="o">.</span><span class="n">item</span><span class="p">())</span>
</pre></div>
</div>
</div>
</div>
<p>Setting up our experiment harness:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">get_model</span><span class="p">(</span><span class="n">vocab_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
              <span class="n">block_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
              <span class="n">emb_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
              <span class="n">width</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
              <span class="n">num_linear</span><span class="p">:</span> <span class="nb">int</span><span class="p">):</span>

    <span class="n">layers</span> <span class="o">=</span> <span class="p">[</span>
        <span class="n">nn</span><span class="o">.</span><span class="n">Embedding</span><span class="p">(</span><span class="n">vocab_size</span><span class="p">,</span> <span class="n">emb_size</span><span class="p">),</span> 
        <span class="n">nn</span><span class="o">.</span><span class="n">Flatten</span><span class="p">(),</span>
        <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">block_size</span> <span class="o">*</span> <span class="n">emb_size</span><span class="p">,</span> <span class="n">width</span><span class="p">),</span> 
        <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">()</span>
    <span class="p">]</span> 
    
    <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_linear</span><span class="p">):</span>
        <span class="n">layers</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">width</span><span class="p">,</span> <span class="n">width</span><span class="p">))</span>
        <span class="n">layers</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">())</span>

    <span class="n">layers</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">width</span><span class="p">,</span> <span class="n">vocab_size</span><span class="p">))</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span><span class="o">*</span><span class="n">layers</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">model</span>
    

<span class="k">def</span> <span class="nf">init_model</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">fn</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Expected `fn` modifies linear layer weights in-place. See </span>
<span class="sd">    https://pytorch.org/docs/stable/nn.init.html for init functions.&quot;&quot;&quot;</span>
    <span class="k">for</span> <span class="n">m</span> <span class="ow">in</span> <span class="n">model</span><span class="o">.</span><span class="n">modules</span><span class="p">():</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">m</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">):</span>
            <span class="n">fn</span><span class="p">(</span><span class="n">m</span><span class="o">.</span><span class="n">weight</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">m</span><span class="o">.</span><span class="n">bias</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">zeros_</span><span class="p">(</span><span class="n">m</span><span class="o">.</span><span class="n">bias</span><span class="p">)</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">m</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">Embedding</span><span class="p">):</span>
            <span class="n">g</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">Generator</span><span class="p">()</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="n">RANDOM_SEED</span><span class="p">)</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">normal_</span><span class="p">(</span><span class="n">m</span><span class="o">.</span><span class="n">weight</span><span class="p">,</span> <span class="n">generator</span><span class="o">=</span><span class="n">g</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">model</span>


<span class="k">def</span> <span class="nf">run</span><span class="p">(</span><span class="n">model</span><span class="p">,</span>
        <span class="n">train_dataset</span><span class="p">:</span> <span class="n">Dataset</span><span class="p">,</span>
        <span class="n">batch_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">hooks</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="nb">dict</span><span class="p">],</span> 
        <span class="n">steps</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> 
        <span class="n">lr</span><span class="p">:</span> <span class="nb">float</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Trainer</span><span class="p">:</span>

    <span class="c1"># Setup optimization and data loader</span>
    <span class="n">set_seed</span><span class="p">(</span><span class="n">RANDOM_SEED</span><span class="p">)</span>
    <span class="n">loss_fn</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">cross_entropy</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">DEVICE</span><span class="p">)</span>
    <span class="n">optim</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="n">lr</span><span class="p">)</span>
    <span class="n">trainer</span> <span class="o">=</span> <span class="n">Trainer</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">optim</span><span class="p">,</span> <span class="n">loss_fn</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">DEVICE</span><span class="p">)</span>
    <span class="n">train_loader</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span><span class="n">train_dataset</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">drop_last</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

    <span class="c1"># Finally, training...</span>
    <span class="k">try</span><span class="p">:</span>
        <span class="k">for</span> <span class="n">hook_dict</span> <span class="ow">in</span> <span class="n">hooks</span><span class="p">:</span>
            <span class="n">trainer</span><span class="o">.</span><span class="n">register_hooks</span><span class="p">(</span>
                <span class="n">fwd_hooks</span><span class="o">=</span><span class="p">[</span><span class="n">h</span><span class="o">.</span><span class="n">hook_fn</span> <span class="k">for</span> <span class="n">h</span> <span class="ow">in</span> <span class="n">hook_dict</span><span class="p">[</span><span class="s2">&quot;fwd_hooks&quot;</span><span class="p">]],</span> 
                <span class="n">bwd_hooks</span><span class="o">=</span><span class="p">[</span><span class="n">h</span><span class="o">.</span><span class="n">hook_fn</span> <span class="k">for</span> <span class="n">h</span> <span class="ow">in</span> <span class="n">hook_dict</span><span class="p">[</span><span class="s2">&quot;bwd_hooks&quot;</span><span class="p">]],</span> 
                <span class="n">layer_types</span><span class="o">=</span><span class="n">hook_dict</span><span class="p">[</span><span class="s2">&quot;layer_types&quot;</span><span class="p">]</span>
            <span class="p">)</span>

        <span class="n">trainer</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">steps</span><span class="o">=</span><span class="n">steps</span><span class="p">,</span> <span class="n">train_loader</span><span class="o">=</span><span class="n">train_loader</span><span class="p">)</span>

    <span class="k">except</span> <span class="ne">Exception</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="n">e</span><span class="p">)</span>
    <span class="k">finally</span><span class="p">:</span>
        <span class="n">trainer</span><span class="o">.</span><span class="n">remove_hooks</span><span class="p">()</span>

    <span class="k">return</span> <span class="n">trainer</span>
</pre></div>
</div>
</div>
</div>
<p>The task and model used here is from the previous notebook. In the <code class="docutils literal notranslate"><span class="pre">init_model</span></code> function, we iterate over the layers and apply a function from the <a class="reference external" href="https://pytorch.org/docs/stable/nn.init.html"><code class="docutils literal notranslate"><span class="pre">nn.init</span></code></a> library to modify layer weights in-place. The <a class="reference external" href="https://pytorch.org/docs/stable/nn.init.html#nn.init.xavier_uniform_"><code class="docutils literal notranslate"><span class="pre">xavier_uniform_</span></code></a> implementation uses fan average mode. This is applied to linear layer weights, while biases are set to zero. Finally, the embedding layer is initialized with weights from a Gaussian distribution.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">init_hooks</span><span class="p">()</span> <span class="o">-&gt;</span> <span class="nb">list</span><span class="p">[</span><span class="nb">dict</span><span class="p">]:</span>
    <span class="k">global</span> <span class="n">dead_relu_stats</span>
    <span class="k">global</span> <span class="n">relu_outs_stats</span>
    <span class="k">global</span> <span class="n">relu_grad_stats</span>
    <span class="k">global</span> <span class="n">linear_outs_stats</span>
    <span class="k">global</span> <span class="n">linear_grad_stats</span>

    <span class="n">dead_relu_stats</span> <span class="o">=</span> <span class="n">DyingReluStats</span><span class="p">(</span><span class="n">sat_threshold</span><span class="o">=</span><span class="mf">1e-8</span><span class="p">,</span> <span class="n">frac_threshold</span><span class="o">=</span><span class="mf">0.95</span><span class="p">)</span>
    <span class="n">relu_outs_stats</span> <span class="o">=</span> <span class="n">OutputStats</span><span class="p">()</span>
    <span class="n">relu_grad_stats</span> <span class="o">=</span> <span class="n">ActivationGradientStats</span><span class="p">()</span>
    <span class="n">linear_outs_stats</span> <span class="o">=</span> <span class="n">OutputStats</span><span class="p">()</span>
    <span class="n">linear_grad_stats</span> <span class="o">=</span> <span class="n">WeightGradientStats</span><span class="p">()</span>
    
    <span class="k">return</span> <span class="p">[</span>
        <span class="p">{</span>
            <span class="s2">&quot;fwd_hooks&quot;</span><span class="p">:</span> <span class="p">[</span><span class="n">linear_outs_stats</span><span class="p">],</span> 
            <span class="s2">&quot;bwd_hooks&quot;</span><span class="p">:</span> <span class="p">[</span><span class="n">linear_grad_stats</span><span class="p">],</span> 
            <span class="s2">&quot;layer_types&quot;</span><span class="p">:</span> <span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">,)</span>
        <span class="p">},</span>
        <span class="p">{</span>
            <span class="s2">&quot;fwd_hooks&quot;</span><span class="p">:</span> <span class="p">[</span><span class="n">dead_relu_stats</span><span class="p">,</span> <span class="n">relu_outs_stats</span><span class="p">],</span> 
            <span class="s2">&quot;bwd_hooks&quot;</span><span class="p">:</span> <span class="p">[</span><span class="n">relu_grad_stats</span><span class="p">],</span> 
            <span class="s2">&quot;layer_types&quot;</span><span class="p">:</span> <span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">,)</span>
        <span class="p">}</span>
    <span class="p">]</span>
    

<span class="n">model_params</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s2">&quot;vocab_size&quot;</span><span class="p">:</span> <span class="mi">29</span><span class="p">,</span>
    <span class="s2">&quot;block_size&quot;</span><span class="p">:</span> <span class="mi">3</span><span class="p">,</span>
    <span class="s2">&quot;emb_size&quot;</span><span class="p">:</span> <span class="mi">10</span><span class="p">,</span>
    <span class="s2">&quot;width&quot;</span><span class="p">:</span> <span class="mi">30</span><span class="p">,</span>
    <span class="s2">&quot;num_linear&quot;</span><span class="p">:</span> <span class="mi">2</span>
<span class="p">}</span>

<span class="n">train_params</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s2">&quot;steps&quot;</span><span class="p">:</span> <span class="mi">1000</span><span class="p">,</span>
    <span class="s2">&quot;batch_size&quot;</span><span class="p">:</span> <span class="mi">32</span><span class="p">,</span>
    <span class="s2">&quot;lr&quot;</span><span class="p">:</span> <span class="mf">0.1</span>
<span class="p">}</span>

<span class="n">g</span> <span class="o">=</span> <span class="k">lambda</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Generator</span><span class="p">()</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="n">RANDOM_SEED</span><span class="p">)</span>
<span class="n">init_fn</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">w</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">xavier_uniform_</span><span class="p">(</span><span class="n">w</span><span class="p">,</span> <span class="n">gain</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="mi">2</span><span class="p">),</span> <span class="n">generator</span><span class="o">=</span><span class="n">g</span><span class="p">())</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">init_model</span><span class="p">(</span><span class="n">get_model</span><span class="p">(</span><span class="o">**</span><span class="n">model_params</span><span class="p">),</span> <span class="n">init_fn</span><span class="p">)</span>
<span class="n">hooks</span> <span class="o">=</span> <span class="n">init_hooks</span><span class="p">()</span>
<span class="n">trainer</span> <span class="o">=</span> <span class="n">run</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">train_dataset</span><span class="p">,</span> <span class="o">**</span><span class="n">train_params</span><span class="p">,</span> <span class="n">hooks</span><span class="o">=</span><span class="n">hooks</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<script type="application/vnd.jupyter.widget-view+json">{"model_id": "b085466f276c4fc69d47c73c5642386c", "version_major": 2, "version_minor": 0}</script></div>
</div>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">moving_avg</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">window</span><span class="o">=</span><span class="mf">0.05</span><span class="p">):</span>
    <span class="n">ma</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">steps</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>
    <span class="n">w</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">window</span> <span class="o">*</span> <span class="n">steps</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">s</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">steps</span><span class="p">):</span>
        <span class="n">ma</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">a</span><span class="p">[</span><span class="o">-</span><span class="n">w</span> <span class="o">+</span> <span class="n">s</span> <span class="o">+</span> <span class="mi">1</span><span class="p">:</span> <span class="n">s</span> <span class="o">+</span> <span class="mi">1</span><span class="p">]))</span>
    <span class="k">return</span> <span class="n">ma</span>

<span class="k">def</span> <span class="nf">plot_training_loss</span><span class="p">(</span><span class="n">trainer</span><span class="p">,</span> <span class="n">window</span><span class="o">=</span><span class="mf">0.05</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">3</span><span class="p">)):</span>
    <span class="n">train_loss</span> <span class="o">=</span> <span class="n">trainer</span><span class="o">.</span><span class="n">logs</span><span class="p">[</span><span class="s2">&quot;train&quot;</span><span class="p">][</span><span class="s2">&quot;loss&quot;</span><span class="p">]</span>
    <span class="n">train_loss_ma</span> <span class="o">=</span> <span class="n">moving_avg</span><span class="p">(</span><span class="n">train_loss</span><span class="p">,</span> <span class="n">window</span><span class="p">)</span>

    <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="n">figsize</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">trainer</span><span class="o">.</span><span class="n">logs</span><span class="p">[</span><span class="s2">&quot;train&quot;</span><span class="p">][</span><span class="s2">&quot;loss&quot;</span><span class="p">],</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;train&quot;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">train_loss_ma</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;C0&quot;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;train (MA)&quot;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;loss&quot;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;steps&quot;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">(</span><span class="nb">min</span><span class="p">(</span><span class="n">trainer</span><span class="o">.</span><span class="n">logs</span><span class="p">[</span><span class="s2">&quot;train&quot;</span><span class="p">][</span><span class="s2">&quot;loss&quot;</span><span class="p">])</span> <span class="o">-</span> <span class="mf">0.2</span><span class="p">,</span> <span class="nb">min</span><span class="p">(</span><span class="nb">max</span><span class="p">(</span><span class="n">trainer</span><span class="o">.</span><span class="n">logs</span><span class="p">[</span><span class="s2">&quot;train&quot;</span><span class="p">][</span><span class="s2">&quot;loss&quot;</span><span class="p">])</span> <span class="o">+</span> <span class="mf">0.2</span><span class="p">,</span> <span class="mf">5.0</span><span class="p">))</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="n">linestyle</span><span class="o">=</span><span class="s2">&quot;dotted&quot;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.6</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">();</span>
</pre></div>
</div>
</div>
</details>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plot_training_loss</span><span class="p">(</span><span class="n">trainer</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/4ce47bb5a83884e7ba7e4bb06eb9f53a9c403a9d832915979d8955517ed1bfe9.svg" src="../../_images/4ce47bb5a83884e7ba7e4bb06eb9f53a9c403a9d832915979d8955517ed1bfe9.svg" /></div>
</div>
<section id="histograms">
<h3>Histograms<a class="headerlink" href="#histograms" title="Link to this heading">#</a></h3>
<p>The hooks stored activation and gradient statistics during training which we now visualize:</p>
<aside class="margin sidebar">
<p class="sidebar-title"></p>
<p><a class="reference external" href="https://forums.fast.ai/t/the-colorful-dimension/42908/5">Colorful dimension</a></p>
</aside>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">plot_training_histograms</span><span class="p">(</span><span class="n">stats</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="n">HookHandler</span><span class="p">],</span> 
                             <span class="n">bins</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">100</span><span class="p">,</span> 
                             <span class="n">num_stds</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">4</span><span class="p">,</span> 
                             <span class="n">cmaps</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> 
                             <span class="n">labels</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
                             <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> 
                             <span class="n">aspect</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">5</span><span class="p">):</span>
    
    <span class="n">rows</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">stats</span><span class="p">)</span>
    <span class="n">cols</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">stats</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">records</span><span class="o">.</span><span class="n">keys</span><span class="p">())</span>
    <span class="n">cmaps</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;viridis&quot;</span><span class="p">]</span> <span class="o">*</span> <span class="n">rows</span> <span class="k">if</span> <span class="n">cmaps</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">cmaps</span>
    <span class="n">H</span><span class="p">,</span> <span class="n">W</span> <span class="o">=</span> <span class="n">figsize</span>
    <span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">rows</span><span class="p">,</span> <span class="n">cols</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="n">H</span> <span class="o">*</span> <span class="n">rows</span><span class="p">,</span> <span class="n">W</span> <span class="o">*</span> <span class="n">cols</span><span class="p">))</span>

    <span class="c1"># estimate a good range per row (i.e. per handler)</span>
    <span class="c1"># same range =&gt; show vanishing / exploding over layers</span>
    <span class="n">h</span> <span class="o">=</span> <span class="p">{}</span>
    <span class="n">acts</span> <span class="o">=</span> <span class="p">{}</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">rows</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">j</span><span class="p">,</span> <span class="n">m</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">stats</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">records</span><span class="o">.</span><span class="n">keys</span><span class="p">()):</span>
            <span class="k">if</span> <span class="n">m</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">stats</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">records</span><span class="p">:</span>
                <span class="k">continue</span>
            <span class="n">acts</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">stack</span><span class="p">([</span><span class="n">t</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span> <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="n">stats</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">records</span><span class="p">[</span><span class="n">m</span><span class="p">]],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span>
        
        <span class="n">all_data</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">concat</span><span class="p">([</span><span class="n">acts</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">]</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span> <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">stats</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">records</span><span class="p">))])</span>
        <span class="n">h</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">all_data</span><span class="o">.</span><span class="n">std</span><span class="p">()</span><span class="o">.</span><span class="n">item</span><span class="p">()</span> <span class="o">*</span> <span class="n">num_stds</span>

    <span class="c1"># calculate histograms</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">rows</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">j</span><span class="p">,</span> <span class="n">m</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">stats</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">records</span><span class="o">.</span><span class="n">keys</span><span class="p">()):</span>    
            <span class="k">if</span> <span class="n">m</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">stats</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">records</span><span class="p">:</span>
                <span class="k">continue</span>

            <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
                <span class="c1"># Add one count -&gt; log = more colorful, note: log 1 = 0.</span>
                <span class="c1"># total count (i.e. sum per col) == batch_size * num_neurons, .: normalized</span>
                <span class="n">hists</span> <span class="o">=</span> <span class="p">[(</span><span class="n">torch</span><span class="o">.</span><span class="n">histc</span><span class="p">(</span><span class="n">acts</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">][</span><span class="n">t</span><span class="p">],</span> <span class="n">bins</span><span class="o">=</span><span class="n">bins</span><span class="p">,</span> <span class="nb">min</span><span class="o">=-</span><span class="n">h</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="nb">max</span><span class="o">=</span><span class="n">h</span><span class="p">[</span><span class="n">i</span><span class="p">])</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">log</span><span class="p">()</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span> <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">acts</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])]</span>

            <span class="c1"># histogram image</span>
            <span class="n">axf</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">:</span> <span class="n">ax</span><span class="p">[</span><span class="n">j</span><span class="p">]</span> <span class="k">if</span> <span class="n">rows</span> <span class="o">==</span> <span class="mi">1</span> <span class="k">else</span> <span class="n">ax</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">]</span>
            <span class="n">axf</span><span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">)</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="n">m</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span> <span class="o">+</span> <span class="s2">&quot;.&quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">j</span><span class="p">),</span> <span class="n">size</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
            <span class="n">axf</span><span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">)</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">flip</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">concatenate</span><span class="p">(</span><span class="n">hists</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">),</span> <span class="n">aspect</span><span class="o">=</span><span class="s1">&#39;auto&#39;</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="n">cmaps</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>     <span class="c1"># (!)</span>
            <span class="n">axf</span><span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">)</span><span class="o">.</span><span class="n">set_aspect</span><span class="p">(</span><span class="n">aspect</span><span class="p">)</span>                                                                 <span class="c1"># transpose =&gt; positive vals = down </span>
            <span class="n">axf</span><span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">)</span><span class="o">.</span><span class="n">set_yticks</span><span class="p">([])</span>                                                                     <span class="c1"># flip =&gt; positive vals = up</span>
            <span class="k">if</span> <span class="n">j</span> <span class="o">==</span> <span class="mi">0</span> <span class="ow">and</span> <span class="n">labels</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">axf</span><span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">)</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">labels</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="si">}</span><span class="se">\n</span><span class="s2">[-</span><span class="si">{</span><span class="n">h</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="si">:</span><span class="s2">.1e</span><span class="si">}</span><span class="s2">, </span><span class="si">{</span><span class="n">h</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="si">:</span><span class="s2">.1e</span><span class="si">}</span><span class="s2">]&quot;</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>                                                           
                <span class="n">axf</span><span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">)</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;[-</span><span class="si">{</span><span class="n">h</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="si">:</span><span class="s2">.1e</span><span class="si">}</span><span class="s2">, </span><span class="si">{</span><span class="n">h</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="si">:</span><span class="s2">.1e</span><span class="si">}</span><span class="s2">]&quot;</span><span class="p">)</span>
            <span class="n">axf</span><span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">)</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s2">&quot;steps&quot;</span><span class="p">)</span>
    
    <span class="n">fig</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
</details>
</div>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">plot_training_stats</span><span class="p">(</span><span class="n">stats</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="n">HookHandler</span><span class="p">],</span> 
                        <span class="n">labels</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> 
                        <span class="n">colors</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> 
                        <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span> <span class="mi">2</span><span class="p">)):</span>
    
    <span class="n">rows</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">stats</span><span class="p">)</span>
    <span class="n">cols</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">stats</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">records</span><span class="o">.</span><span class="n">keys</span><span class="p">())</span>
    <span class="n">colors</span> <span class="o">=</span> <span class="p">[</span><span class="sa">f</span><span class="s2">&quot;C</span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s2">&quot;</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">rows</span><span class="p">)]</span> <span class="k">if</span> <span class="n">colors</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">colors</span>
    <span class="n">H</span><span class="p">,</span> <span class="n">W</span> <span class="o">=</span> <span class="n">figsize</span>
    <span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">rows</span><span class="p">,</span> <span class="n">cols</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="n">H</span> <span class="o">*</span> <span class="n">rows</span><span class="p">,</span> <span class="n">W</span> <span class="o">*</span> <span class="n">cols</span><span class="p">))</span>

    <span class="c1"># calculate mean and std</span>
    <span class="n">axf</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">:</span> <span class="n">ax</span><span class="p">[</span><span class="n">j</span><span class="p">]</span> <span class="k">if</span> <span class="n">rows</span> <span class="o">==</span> <span class="mi">1</span> <span class="k">else</span> <span class="n">ax</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">]</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">rows</span><span class="p">):</span>
        <span class="n">h</span> <span class="o">=</span> <span class="mi">0</span>  <span class="c1"># y lim</span>
        <span class="k">for</span> <span class="n">j</span><span class="p">,</span> <span class="n">m</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">stats</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">records</span><span class="o">.</span><span class="n">keys</span><span class="p">()):</span>    
            <span class="k">if</span> <span class="n">m</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">stats</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">records</span><span class="p">:</span>
                <span class="k">continue</span>

            <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
                <span class="n">r</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">stack</span><span class="p">([</span><span class="n">t</span> <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="n">stats</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">records</span><span class="p">[</span><span class="n">m</span><span class="p">]])</span>
                <span class="n">r</span> <span class="o">=</span> <span class="n">r</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">r</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
                <span class="n">u</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">percentile</span><span class="p">(</span><span class="n">r</span><span class="p">,</span> <span class="n">q</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>  <span class="c1"># [a, b] contains 68% of data</span>
                <span class="n">a</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">percentile</span><span class="p">(</span><span class="n">r</span><span class="p">,</span> <span class="n">q</span><span class="o">=</span><span class="mi">16</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>  <span class="c1"># 50 - 34 = 16</span>
                <span class="n">b</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">percentile</span><span class="p">(</span><span class="n">r</span><span class="p">,</span> <span class="n">q</span><span class="o">=</span><span class="mi">84</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>  <span class="c1"># 50 + 34 = 84</span>

            <span class="n">h</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="n">h</span><span class="p">,</span> <span class="nb">max</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">a</span><span class="p">)</span><span class="o">.</span><span class="n">mean</span><span class="p">(),</span> <span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">b</span><span class="p">)</span><span class="o">.</span><span class="n">mean</span><span class="p">())</span><span class="o">.</span><span class="n">item</span><span class="p">())</span>

            <span class="c1"># plot</span>
            <span class="n">axf</span><span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">)</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="n">m</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span> <span class="o">+</span> <span class="s2">&quot;.&quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">j</span><span class="p">),</span> <span class="n">size</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
            <span class="n">axf</span><span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">)</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="n">colors</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>
            <span class="n">axf</span><span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">)</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">b</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="n">colors</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>
            <span class="n">axf</span><span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">)</span><span class="o">.</span><span class="n">fill_between</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">u</span><span class="p">)),</span> <span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="n">colors</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.3</span><span class="p">)</span>
            <span class="n">axf</span><span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">)</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">u</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;black&quot;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sa">f</span><span class="s2">&quot;$\mu$&quot;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.8</span><span class="p">)</span>
            
            <span class="n">axf</span><span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">)</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s2">&quot;steps&quot;</span><span class="p">)</span>
            <span class="n">axf</span><span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">)</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="n">linestyle</span><span class="o">=</span><span class="s2">&quot;dashed&quot;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.6</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">j</span> <span class="o">==</span> <span class="mi">0</span> <span class="ow">and</span> <span class="n">labels</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">axf</span><span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">)</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="n">labels</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>

        <span class="c1"># set y-lims</span>
        <span class="k">for</span> <span class="n">j</span><span class="p">,</span> <span class="n">m</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">stats</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">records</span><span class="o">.</span><span class="n">keys</span><span class="p">()):</span>    
            <span class="k">if</span> <span class="n">m</span> <span class="ow">in</span> <span class="n">stats</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">records</span><span class="p">:</span>
                <span class="n">axf</span><span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">)</span><span class="o">.</span><span class="n">set_ylim</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span> <span class="o">*</span> <span class="n">h</span><span class="p">,</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">h</span><span class="p">)</span>
                
    <span class="n">fig</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
</details>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s2">&quot;loss (last 100 steps):&quot;</span><span class="p">,</span> <span class="nb">sum</span><span class="p">(</span><span class="n">trainer</span><span class="o">.</span><span class="n">logs</span><span class="p">[</span><span class="s2">&quot;train&quot;</span><span class="p">][</span><span class="s2">&quot;loss&quot;</span><span class="p">][</span><span class="o">-</span><span class="mi">100</span><span class="p">:])</span> <span class="o">/</span> <span class="mi">100</span><span class="p">)</span>
<span class="n">plot_training_histograms</span><span class="p">([</span><span class="n">linear_outs_stats</span><span class="p">,</span> <span class="n">linear_grad_stats</span><span class="p">],</span> <span class="n">labels</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;Outputs&quot;</span><span class="p">,</span> <span class="s2">&quot;Gradients&quot;</span><span class="p">],</span> <span class="n">cmaps</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;viridis&quot;</span><span class="p">,</span> <span class="s2">&quot;inferno&quot;</span><span class="p">],</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span> <span class="mf">1.2</span><span class="p">))</span>
<span class="n">plot_training_stats</span><span class="p">([</span><span class="n">linear_outs_stats</span><span class="p">,</span> <span class="n">linear_grad_stats</span><span class="p">],</span> <span class="n">labels</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;Outputs&quot;</span><span class="p">,</span> <span class="s2">&quot;Gradients&quot;</span><span class="p">],</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span> <span class="mf">1.5</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>loss (last 100 steps): 2.557870318889618
</pre></div>
</div>
<img alt="../../_images/5763858969bbefd96af1c109263442ec62d05ec8605137572b790344bdca18db.svg" src="../../_images/5763858969bbefd96af1c109263442ec62d05ec8605137572b790344bdca18db.svg" /><img alt="../../_images/f8e20f68b30ea8c3f3209032fb542bbdb1fd7c0bd4cc75f53cd73d8b07d87c24.svg" src="../../_images/f8e20f68b30ea8c3f3209032fb542bbdb1fd7c0bd4cc75f53cd73d8b07d87c24.svg" /></div>
</div>
<p><strong>Figure.</strong> First plot shows activation and gradient histograms over time. Range of values shown is <span class="math notranslate nohighlight">\(\mu \pm 4 \sigma\)</span> over all time steps, units, and layers.
Considering each layer separately, the network is training well.
But between layers we see that the gradient distribution changes.
Since the distributions are not symmetric, the
second plot shows the 16-50-84th percentiles of samples at each step.</p>
<br><div class="cell tag_hide-output docutils container">
<div class="cell_input above-output-prompt docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># can also check relu stats</span>
<span class="n">plot_training_histograms</span><span class="p">([</span><span class="n">relu_outs_stats</span><span class="p">,</span> <span class="n">relu_grad_stats</span><span class="p">],</span> <span class="n">labels</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;Outputs&quot;</span><span class="p">,</span> <span class="s2">&quot;Gradients&quot;</span><span class="p">],</span> <span class="n">cmaps</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;viridis&quot;</span><span class="p">,</span> <span class="s2">&quot;inferno&quot;</span><span class="p">],</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mf">5.0</span><span class="p">,</span> <span class="mf">1.5</span><span class="p">))</span>
<span class="n">plot_training_stats</span><span class="p">([</span><span class="n">relu_outs_stats</span><span class="p">,</span> <span class="n">relu_grad_stats</span><span class="p">],</span> <span class="n">labels</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;Outputs&quot;</span><span class="p">,</span> <span class="s2">&quot;Gradients&quot;</span><span class="p">],</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mf">1.5</span><span class="p">))</span>
</pre></div>
</div>
</div>
<details class="hide below-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell output</span>
<span class="expanded">Hide code cell output</span>
</summary>
<div class="cell_output docutils container">
<img alt="../../_images/111204c5504ce7f541dcf0bd21525757b75d56b4eef3afd4cba34231051ff49e.svg" src="../../_images/111204c5504ce7f541dcf0bd21525757b75d56b4eef3afd4cba34231051ff49e.svg" /><img alt="../../_images/9957fffb65077c8cf214b5b5fdbb778731dd55c16294e93258b9383e8b44d708.svg" src="../../_images/9957fffb65077c8cf214b5b5fdbb778731dd55c16294e93258b9383e8b44d708.svg" /></div>
</details>
</div>
</section>
<section id="dying-units">
<h3>Dying units<a class="headerlink" href="#dying-units" title="Link to this heading">#</a></h3>
<p>Good initialization prevents dead units at the start of training:</p>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">plot_dying_relus</span><span class="p">(</span><span class="n">trainer</span><span class="p">,</span> <span class="n">stats</span><span class="p">):</span>
    <span class="n">module_list</span> <span class="o">=</span> <span class="p">[</span><span class="n">m</span> <span class="k">for</span> <span class="n">m</span> <span class="ow">in</span> <span class="n">trainer</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">modules</span><span class="p">()</span> <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">m</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">)]</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>
    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">m</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">module_list</span><span class="p">):</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">m</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">):</span>
            <span class="n">moving_avg</span> <span class="o">=</span> <span class="p">[]</span>
            <span class="n">steps</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">stats</span><span class="o">.</span><span class="n">records</span><span class="p">[</span><span class="n">m</span><span class="p">])</span>
            <span class="n">w</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="mf">0.05</span> <span class="o">*</span> <span class="n">steps</span><span class="p">)</span>
            <span class="k">for</span> <span class="n">s</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">steps</span><span class="p">):</span>
                <span class="n">moving_avg</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">stats</span><span class="o">.</span><span class="n">records</span><span class="p">[</span><span class="n">m</span><span class="p">][</span><span class="o">-</span><span class="n">w</span> <span class="o">+</span> <span class="n">s</span> <span class="o">+</span> <span class="mi">1</span><span class="p">:</span> <span class="n">s</span> <span class="o">+</span> <span class="mi">1</span><span class="p">]))</span>

            <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">moving_avg</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sa">f</span><span class="s2">&quot;ReLU.</span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
        
    <span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="n">linestyle</span><span class="o">=</span><span class="s2">&quot;dashed&quot;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.6</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;sat_threshold=</span><span class="si">{</span><span class="n">stats</span><span class="o">.</span><span class="n">sat_threshold</span><span class="si">:</span><span class="s2">.1e</span><span class="si">}</span><span class="s2">, frac_treshold=</span><span class="si">{</span><span class="n">stats</span><span class="o">.</span><span class="n">frac_threshold</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;step&quot;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;frac dead relu (MA)&quot;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">(</span><span class="o">-</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">1.1</span><span class="p">)</span>
</pre></div>
</div>
</div>
</details>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s2">&quot;loss (last 100 steps):&quot;</span><span class="p">,</span> <span class="nb">sum</span><span class="p">(</span><span class="n">trainer</span><span class="o">.</span><span class="n">logs</span><span class="p">[</span><span class="s2">&quot;train&quot;</span><span class="p">][</span><span class="s2">&quot;loss&quot;</span><span class="p">][</span><span class="o">-</span><span class="mi">100</span><span class="p">:])</span> <span class="o">/</span> <span class="mi">100</span><span class="p">)</span>
<span class="n">plot_dying_relus</span><span class="p">(</span><span class="n">trainer</span><span class="p">,</span> <span class="n">dead_relu_stats</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>loss (last 100 steps): 2.557870318889618
</pre></div>
</div>
<img alt="../../_images/8701a90feea0264366de74e39c87e34d36b742baf594bdf9fc3e8148e78a4484.svg" src="../../_images/8701a90feea0264366de74e39c87e34d36b742baf594bdf9fc3e8148e78a4484.svg" /></div>
</div>
<br>
<p>Increasing LR from <code class="docutils literal notranslate"><span class="pre">0.1</span></code> to <code class="docutils literal notranslate"><span class="pre">2.0</span></code>.
As discussed above, this can push weights to large values where the activations saturate,
knocking off the model to a flat region in the loss surface where any sample
of the data does not affect the shape of the loss surface. Hence, the model barely trains with further SGD steps.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">copy</span> <span class="kn">import</span> <span class="n">copy</span>

<span class="k">def</span> <span class="nf">_update</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">dict</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Helper to make stateless updates to params.&quot;&quot;&quot;</span>
    <span class="n">params</span> <span class="o">=</span> <span class="n">copy</span><span class="p">(</span><span class="n">params</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">key</span><span class="p">,</span> <span class="n">val</span> <span class="ow">in</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
        <span class="n">params</span><span class="p">[</span><span class="n">key</span><span class="p">]</span> <span class="o">=</span> <span class="n">val</span>
    <span class="k">return</span> <span class="n">params</span>


<span class="n">init_fn</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">w</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">xavier_uniform_</span><span class="p">(</span><span class="n">w</span><span class="p">,</span> <span class="n">gain</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="mi">2</span><span class="p">),</span> <span class="n">generator</span><span class="o">=</span><span class="n">g</span><span class="p">())</span>
<span class="n">hooks</span> <span class="o">=</span> <span class="n">init_hooks</span><span class="p">()</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">init_model</span><span class="p">(</span><span class="n">get_model</span><span class="p">(</span><span class="o">**</span><span class="n">model_params</span><span class="p">),</span> <span class="n">init_fn</span><span class="p">)</span>
<span class="n">trainer</span> <span class="o">=</span> <span class="n">run</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">train_dataset</span><span class="p">,</span> <span class="o">**</span><span class="n">_update</span><span class="p">(</span><span class="n">train_params</span><span class="p">,</span> <span class="n">lr</span><span class="o">=</span><span class="mf">2.0</span><span class="p">),</span> <span class="n">hooks</span><span class="o">=</span><span class="n">hooks</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;loss (last 100 steps):&quot;</span><span class="p">,</span> <span class="nb">sum</span><span class="p">(</span><span class="n">trainer</span><span class="o">.</span><span class="n">logs</span><span class="p">[</span><span class="s2">&quot;train&quot;</span><span class="p">][</span><span class="s2">&quot;loss&quot;</span><span class="p">][</span><span class="o">-</span><span class="mi">100</span><span class="p">:])</span> <span class="o">/</span> <span class="mi">100</span><span class="p">)</span>
<span class="n">plot_dying_relus</span><span class="p">(</span><span class="n">trainer</span><span class="p">,</span> <span class="n">dead_relu_stats</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<script type="application/vnd.jupyter.widget-view+json">{"model_id": "6834eb6ea2544b4e92bcb1c3055066dc", "version_major": 2, "version_minor": 0}</script><div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>loss (last 100 steps): 2.830463263988495
</pre></div>
</div>
<img alt="../../_images/d141703fb543e7f958e94bd975ff995bc2e302b308fa6cf7ac2081ab36e3cb78.svg" src="../../_images/d141703fb543e7f958e94bd975ff995bc2e302b308fa6cf7ac2081ab36e3cb78.svg" /></div>
</div>
<div class="cell tag_hide-output docutils container">
<div class="cell_input above-output-prompt docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plot_training_loss</span><span class="p">(</span><span class="n">trainer</span><span class="p">)</span>
</pre></div>
</div>
</div>
<details class="hide below-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell output</span>
<span class="expanded">Hide code cell output</span>
</summary>
<div class="cell_output docutils container">
<img alt="../../_images/c3276a776c383b1187042a407c35ca287e25472c913c2bf3cd4cd22a20c5ddf5.svg" src="../../_images/c3276a776c383b1187042a407c35ca287e25472c913c2bf3cd4cd22a20c5ddf5.svg" /></div>
</details>
</div>
<div class="cell tag_hide-output docutils container">
<div class="cell_input above-output-prompt docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plot_training_histograms</span><span class="p">([</span><span class="n">linear_outs_stats</span><span class="p">,</span> <span class="n">linear_grad_stats</span><span class="p">],</span> <span class="n">labels</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;Outputs&quot;</span><span class="p">,</span> <span class="s2">&quot;Gradients&quot;</span><span class="p">],</span> <span class="n">cmaps</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;viridis&quot;</span><span class="p">,</span> <span class="s2">&quot;inferno&quot;</span><span class="p">],</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span> <span class="mf">1.2</span><span class="p">))</span>
</pre></div>
</div>
</div>
<details class="hide below-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell output</span>
<span class="expanded">Hide code cell output</span>
</summary>
<div class="cell_output docutils container">
<img alt="../../_images/c41c1873d9f639a67e6a6427727ab5e24dba83577293de3073f1a2a7e854bd16.svg" src="../../_images/c41c1873d9f639a67e6a6427727ab5e24dba83577293de3073f1a2a7e854bd16.svg" /></div>
</details>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plot_training_stats</span><span class="p">([</span><span class="n">linear_outs_stats</span><span class="p">,</span> <span class="n">linear_grad_stats</span><span class="p">],</span> <span class="n">labels</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;Outputs&quot;</span><span class="p">,</span> <span class="s2">&quot;Gradients&quot;</span><span class="p">],</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span> <span class="mf">1.2</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/0e91fdf181f3763208f63ad75b839efa4329cf700e593bfe545c611929b48740.svg" src="../../_images/0e91fdf181f3763208f63ad75b839efa4329cf700e593bfe545c611929b48740.svg" /></div>
</div>
</section>
</section>
<section id="layer-normalization">
<h2>Layer normalization<a class="headerlink" href="#layer-normalization" title="Link to this heading">#</a></h2>
<p>Recall that for weight initialization we have to specify the gain for each activation. It would be nice to learn
the gain instead of setting it heuristically, or deriving it from equations.
Moreover, from the above pathological examples, we find cases where initialization cannot help past the early steps (e.g. when we have large learning rates). To get stable activations during training, we want to normalize layer preactivations dynamically.</p>
<p>This can be done using normalization layers. In particular, we look at <strong>layer normalization</strong> <span id="id9">[<a class="reference internal" href="../../intro.html#id37" title="Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E. Hinton. Layer normalization. 2016. URL: https://arxiv.org/abs/1607.06450, doi:10.48550/ARXIV.1607.06450.">BKH16</a>]</span>. The following equations define the action of LayerNorm on an input <span class="math notranslate nohighlight">\(\boldsymbol{\mathsf{z}}\)</span> for one instance fed into the network:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
{\mu} &amp;= \frac{1}{n}\sum_{i} {\mathsf{z}}_{i} \\
{\sigma}^2 &amp;= \frac{1}{n}\sum_{i} ({\mathsf{z}}_{i} - {\mu})^2 \\
\hat{{\mathsf{z}}}_{j} &amp;= \frac{{\mathsf{z}}_{j} - {\mu}}{\sqrt{{\sigma}^2 + \epsilon}} \\
{{\mathsf{y}}}_{j} &amp;= {\gamma}_j\, \hat{{\mathsf{z}}}_{j} + {\beta}_j
\end{aligned}
\end{split}\]</div>
<p>where <span class="math notranslate nohighlight">\(n = |\boldsymbol{\mathsf{z}}|.\)</span> This makes the outputs of each neuron lie in the same range. Consequently, this also helps control the magnitude of weights and weight gradients. Here we introduce <strong>trainable</strong> affine parameters <span class="math notranslate nohighlight">\(\boldsymbol{\gamma}\)</span> and <span class="math notranslate nohighlight">\(\boldsymbol{\beta}\)</span> that are applied per unit. This allows the output distributions to scale and shift, otherwise network expressiveness is degraded with preactivations mostly in <span class="math notranslate nohighlight">\([-1, 1]\)</span>. It can be easily shown that LN is invariant to  rescaling and recentering of the weight and data matrices.</p>
<br><div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">n</span> <span class="o">=</span> <span class="mi">5</span>
<span class="n">layernorm</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">LayerNorm</span><span class="p">(</span><span class="n">n</span><span class="p">)</span>

<span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="n">n</span><span class="p">)</span>
<span class="n">u</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">v</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">var</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">unbiased</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">eps</span> <span class="o">=</span> <span class="mf">0.00001</span>
<span class="n">gamma</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">n</span><span class="p">)</span>   <span class="c1"># params init</span>
<span class="n">beta</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">n</span><span class="p">)</span>

<span class="n">gamma</span> <span class="o">*</span> <span class="p">(</span><span class="n">x</span> <span class="o">-</span> <span class="n">u</span><span class="p">)</span> <span class="o">/</span> <span class="n">torch</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">v</span> <span class="o">+</span> <span class="n">eps</span><span class="p">)</span> <span class="o">+</span> <span class="n">beta</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor([[ 0.4980,  1.2870, -0.2261,  0.1768, -1.7358],
        [ 1.5455, -1.1316, -0.2491, -0.8746,  0.7098],
        [-0.0341, -1.5814,  1.2074,  0.8957, -0.4876]])
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">layernorm</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor([[ 0.4980,  1.2870, -0.2261,  0.1768, -1.7358],
        [ 1.5455, -1.1316, -0.2491, -0.8746,  0.7098],
        [-0.0341, -1.5814,  1.2074,  0.8957, -0.4876]],
       grad_fn=&lt;NativeLayerNormBackward0&gt;)
</pre></div>
</div>
</div>
</div>
<p><strong>Remark.</strong> The affine parameters have the same shape as <code class="docutils literal notranslate"><span class="pre">normalized_shape</span></code> (i.e. main argument of <code class="docutils literal notranslate"><span class="pre">LayerNorm</span></code>). This is the
as the last <code class="docutils literal notranslate"><span class="pre">D</span></code> dimensions of the input tensor that are normalized over, where <code class="docutils literal notranslate"><span class="pre">D</span></code> is the rank of <code class="docutils literal notranslate"><span class="pre">normalized_shape</span></code>.</p>
<section id="immediate-effects">
<h3>Immediate effects<a class="headerlink" href="#immediate-effects" title="Link to this heading">#</a></h3>
<p>Updating our architecture with LN applied before ReLU so that preactivations are normalized around ReLUโs active region, followed by a unit-wise shift and scale.
Note that we also add normalization on logits:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">get_model</span><span class="p">(</span><span class="n">vocab_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
              <span class="n">block_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
              <span class="n">emb_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
              <span class="n">width</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
              <span class="n">num_linear</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
              <span class="n">layernorm</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">):</span>

    <span class="n">layers</span> <span class="o">=</span> <span class="p">[</span>
        <span class="n">nn</span><span class="o">.</span><span class="n">Embedding</span><span class="p">(</span><span class="n">vocab_size</span><span class="p">,</span> <span class="n">emb_size</span><span class="p">),</span> 
        <span class="n">nn</span><span class="o">.</span><span class="n">Flatten</span><span class="p">(),</span>
        <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">block_size</span> <span class="o">*</span> <span class="n">emb_size</span><span class="p">,</span> <span class="n">width</span><span class="p">)</span>
    <span class="p">]</span>
    <span class="n">layers</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">LayerNorm</span><span class="p">(</span><span class="n">width</span><span class="p">))</span> <span class="k">if</span> <span class="n">layernorm</span> <span class="k">else</span> <span class="s2">&quot;&quot;</span>
    <span class="n">layers</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">())</span>
    
    <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_linear</span><span class="p">):</span>
        <span class="n">layers</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">width</span><span class="p">,</span> <span class="n">width</span><span class="p">))</span>
        <span class="n">layers</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">LayerNorm</span><span class="p">(</span><span class="n">width</span><span class="p">))</span> <span class="k">if</span> <span class="n">layernorm</span> <span class="k">else</span> <span class="s2">&quot;&quot;</span>
        <span class="n">layers</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">())</span>

    <span class="n">layers</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">width</span><span class="p">,</span> <span class="n">vocab_size</span><span class="p">))</span>
    <span class="n">layers</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">LayerNorm</span><span class="p">(</span><span class="n">vocab_size</span><span class="p">))</span> <span class="k">if</span> <span class="n">layernorm</span> <span class="k">else</span> <span class="s2">&quot;&quot;</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span><span class="o">*</span><span class="n">layers</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">model</span>
</pre></div>
</div>
</div>
</div>
<br>
<p><strong>Initialization.</strong> Having layer norm makes training less sensitive to weight initialization:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">losses</span> <span class="o">=</span> <span class="p">{</span><span class="mf">0.30</span><span class="p">:</span> <span class="p">{},</span> <span class="mf">1.41</span><span class="p">:</span> <span class="p">{},</span> <span class="mf">3.0</span><span class="p">:</span> <span class="p">{}}</span>  <span class="c1"># sqrt(2) = 1.414...</span>

<span class="k">for</span> <span class="n">gain</span> <span class="ow">in</span> <span class="p">[</span><span class="mf">0.30</span><span class="p">,</span> <span class="mf">1.41</span><span class="p">,</span> <span class="mf">3.0</span><span class="p">]:</span>
    <span class="n">init_fn</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">w</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">xavier_uniform_</span><span class="p">(</span><span class="n">w</span><span class="p">,</span> <span class="n">gain</span><span class="o">=</span><span class="n">gain</span><span class="p">,</span> <span class="n">generator</span><span class="o">=</span><span class="n">g</span><span class="p">())</span>
    <span class="n">model_ln</span> <span class="o">=</span> <span class="n">init_model</span><span class="p">(</span><span class="n">get_model</span><span class="p">(</span><span class="o">**</span><span class="n">model_params</span><span class="p">,</span> <span class="n">layernorm</span><span class="o">=</span><span class="kc">True</span><span class="p">),</span> <span class="n">init_fn</span><span class="p">)</span>
    <span class="n">model_nf</span> <span class="o">=</span> <span class="n">init_model</span><span class="p">(</span><span class="n">get_model</span><span class="p">(</span><span class="o">**</span><span class="n">model_params</span><span class="p">),</span> <span class="n">init_fn</span><span class="p">)</span>
    
    <span class="n">trainer_ln</span> <span class="o">=</span> <span class="n">run</span><span class="p">(</span><span class="n">model_ln</span><span class="p">,</span> <span class="n">train_dataset</span><span class="p">,</span> <span class="n">hooks</span><span class="o">=</span><span class="p">[],</span> <span class="o">**</span><span class="n">train_params</span><span class="p">)</span>   <span class="c1"># no hooks</span>
    <span class="n">trainer_nf</span> <span class="o">=</span> <span class="n">run</span><span class="p">(</span><span class="n">model_nf</span><span class="p">,</span> <span class="n">train_dataset</span><span class="p">,</span> <span class="n">hooks</span><span class="o">=</span><span class="p">[],</span> <span class="o">**</span><span class="n">train_params</span><span class="p">)</span>   <span class="c1"># NF = norm free</span>
    <span class="n">losses</span><span class="p">[</span><span class="n">gain</span><span class="p">][</span><span class="s2">&quot;ln&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">trainer_ln</span><span class="o">.</span><span class="n">logs</span><span class="p">[</span><span class="s2">&quot;train&quot;</span><span class="p">][</span><span class="s2">&quot;loss&quot;</span><span class="p">]</span>
    <span class="n">losses</span><span class="p">[</span><span class="n">gain</span><span class="p">][</span><span class="s2">&quot;nf&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">trainer_nf</span><span class="o">.</span><span class="n">logs</span><span class="p">[</span><span class="s2">&quot;train&quot;</span><span class="p">][</span><span class="s2">&quot;loss&quot;</span><span class="p">]</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<script type="application/vnd.jupyter.widget-view+json">{"model_id": "44acb652d1d6487c998adeab62384a47", "version_major": 2, "version_minor": 0}</script><script type="application/vnd.jupyter.widget-view+json">{"model_id": "aeb44337431b42ff9be06d1bf69c1f74", "version_major": 2, "version_minor": 0}</script><script type="application/vnd.jupyter.widget-view+json">{"model_id": "7b57c9230ac04e3f96db68c186341c24", "version_major": 2, "version_minor": 0}</script><script type="application/vnd.jupyter.widget-view+json">{"model_id": "3d642a5759d74cbbb05131cef0bb0477", "version_major": 2, "version_minor": 0}</script><script type="application/vnd.jupyter.widget-view+json">{"model_id": "f3e8c35c78a64c2ba7444d60b0de45a6", "version_major": 2, "version_minor": 0}</script><script type="application/vnd.jupyter.widget-view+json">{"model_id": "4c844423ff3f48ddb5b0efbef8ffb6a0", "version_major": 2, "version_minor": 0}</script></div>
</div>
<p>Observe that networks with LN layers start with better loss values:</p>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span>
<span class="n">window</span> <span class="o">=</span> <span class="mf">0.03</span>

<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">gain</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">losses</span><span class="o">.</span><span class="n">keys</span><span class="p">()):</span>
    <span class="n">train_loss</span> <span class="o">=</span> <span class="n">losses</span><span class="p">[</span><span class="n">gain</span><span class="p">][</span><span class="s2">&quot;nf&quot;</span><span class="p">]</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">moving_avg</span><span class="p">(</span><span class="n">train_loss</span><span class="p">,</span> <span class="n">window</span><span class="p">),</span> <span class="n">color</span><span class="o">=</span><span class="sa">f</span><span class="s2">&quot;C</span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sa">f</span><span class="s2">&quot;gain=</span><span class="si">{</span><span class="n">gain</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.6</span><span class="p">)</span>

<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">gain</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">losses</span><span class="o">.</span><span class="n">keys</span><span class="p">()):</span>
    <span class="n">train_loss</span> <span class="o">=</span> <span class="n">losses</span><span class="p">[</span><span class="n">gain</span><span class="p">][</span><span class="s2">&quot;ln&quot;</span><span class="p">]</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">moving_avg</span><span class="p">(</span><span class="n">train_loss</span><span class="p">,</span> <span class="n">window</span><span class="p">),</span> <span class="n">color</span><span class="o">=</span><span class="sa">f</span><span class="s2">&quot;C</span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sa">f</span><span class="s2">&quot;gain=</span><span class="si">{</span><span class="n">gain</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2"> (LN)&quot;</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mf">2.0</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;loss&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;steps&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s2">&quot;dotted&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">(</span><span class="mf">2.2</span><span class="p">,</span> <span class="mf">4.5</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">();</span>
</pre></div>
</div>
</div>
</details>
<div class="cell_output docutils container">
<img alt="../../_images/83f482fb39ff43153ffea529979c62241633aeba39fd2c05acea00036b54ed53.svg" src="../../_images/83f482fb39ff43153ffea529979c62241633aeba39fd2c05acea00036b54ed53.svg" /></div>
</div>
<br>
<p>LN can be thought of as automatically and dynamically finding per unit gain:</p>
<div class="cell tag_hide-output docutils container">
<div class="cell_input above-output-prompt docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Using suboptimal PyTorch default gain:</span>
<span class="n">init_fn</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">w</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">xavier_uniform_</span><span class="p">(</span><span class="n">w</span><span class="p">,</span> <span class="n">gain</span><span class="o">=</span><span class="mi">1</span><span class="o">/</span><span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="mi">3</span><span class="p">),</span> <span class="n">generator</span><span class="o">=</span><span class="n">g</span><span class="p">())</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">init_model</span><span class="p">(</span><span class="n">get_model</span><span class="p">(</span><span class="n">layernorm</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="o">**</span><span class="n">model_params</span><span class="p">),</span> <span class="n">init_fn</span><span class="p">)</span>
<span class="n">hooks</span> <span class="o">=</span> <span class="n">init_hooks</span><span class="p">()</span>
<span class="n">trainer</span> <span class="o">=</span> <span class="n">run</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">train_dataset</span><span class="p">,</span> <span class="o">**</span><span class="n">train_params</span><span class="p">,</span> <span class="n">hooks</span><span class="o">=</span><span class="n">hooks</span><span class="p">)</span>

<span class="n">plot_training_histograms</span><span class="p">([</span><span class="n">linear_outs_stats</span><span class="p">,</span> <span class="n">linear_grad_stats</span><span class="p">],</span> <span class="n">labels</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;Outputs&quot;</span><span class="p">,</span> <span class="s2">&quot;Gradients&quot;</span><span class="p">],</span> <span class="n">cmaps</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;viridis&quot;</span><span class="p">,</span> <span class="s2">&quot;inferno&quot;</span><span class="p">],</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span> <span class="mf">1.2</span><span class="p">))</span>
</pre></div>
</div>
</div>
<details class="hide below-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell output</span>
<span class="expanded">Hide code cell output</span>
</summary>
<div class="cell_output docutils container">
<script type="application/vnd.jupyter.widget-view+json">{"model_id": "970970aad7f64707b198055305109288", "version_major": 2, "version_minor": 0}</script><img alt="../../_images/09322207c94669d424ee197e69196ce711e7ae3e5aad86876cf7776039363fe1.svg" src="../../_images/09322207c94669d424ee197e69196ce711e7ae3e5aad86876cf7776039363fe1.svg" /></div>
</details>
</div>
<p>Note that shrinking activation is immediately corrected (more gradual without LN):</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s2">&quot;loss (last 100 steps):&quot;</span><span class="p">,</span> <span class="nb">sum</span><span class="p">(</span><span class="n">trainer</span><span class="o">.</span><span class="n">logs</span><span class="p">[</span><span class="s2">&quot;train&quot;</span><span class="p">][</span><span class="s2">&quot;loss&quot;</span><span class="p">][</span><span class="o">-</span><span class="mi">100</span><span class="p">:])</span> <span class="o">/</span> <span class="mi">100</span><span class="p">)</span>
<span class="n">plot_training_stats</span><span class="p">([</span><span class="n">linear_outs_stats</span><span class="p">,</span> <span class="n">linear_grad_stats</span><span class="p">],</span> <span class="n">labels</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;Outputs&quot;</span><span class="p">,</span> <span class="s2">&quot;Gradients&quot;</span><span class="p">],</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span> <span class="mf">1.5</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>loss (last 100 steps): 2.5370990657806396
</pre></div>
</div>
<img alt="../../_images/fdaf6d2e94c489b62cadf74a86aedc54f55fa85f33a52f0e9af2eb6720aceb1d.svg" src="../../_images/fdaf6d2e94c489b62cadf74a86aedc54f55fa85f33a52f0e9af2eb6720aceb1d.svg" /></div>
</div>
<br>
<p><strong>Large learning rate.</strong> LN allows the network to tolerate larger weight updates:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">init_fn</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">w</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">xavier_uniform_</span><span class="p">(</span><span class="n">w</span><span class="p">,</span> <span class="n">gain</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="mi">2</span><span class="p">),</span> <span class="n">generator</span><span class="o">=</span><span class="n">g</span><span class="p">())</span>
<span class="n">hooks</span> <span class="o">=</span> <span class="n">init_hooks</span><span class="p">()</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">init_model</span><span class="p">(</span><span class="n">get_model</span><span class="p">(</span><span class="n">layernorm</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="o">**</span><span class="n">model_params</span><span class="p">),</span> <span class="n">init_fn</span><span class="p">)</span>
<span class="n">trainer</span> <span class="o">=</span> <span class="n">run</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">train_dataset</span><span class="p">,</span> <span class="o">**</span><span class="n">_update</span><span class="p">(</span><span class="n">train_params</span><span class="p">,</span> <span class="n">lr</span><span class="o">=</span><span class="mf">2.0</span><span class="p">),</span> <span class="n">hooks</span><span class="o">=</span><span class="n">hooks</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;loss (last 100 steps):&quot;</span><span class="p">,</span> <span class="nb">sum</span><span class="p">(</span><span class="n">trainer</span><span class="o">.</span><span class="n">logs</span><span class="p">[</span><span class="s2">&quot;train&quot;</span><span class="p">][</span><span class="s2">&quot;loss&quot;</span><span class="p">][</span><span class="o">-</span><span class="mi">100</span><span class="p">:])</span> <span class="o">/</span> <span class="mi">100</span><span class="p">)</span>
<span class="n">plot_dying_relus</span><span class="p">(</span><span class="n">trainer</span><span class="p">,</span> <span class="n">dead_relu_stats</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<script type="application/vnd.jupyter.widget-view+json">{"model_id": "f494288e7e9248cfa5f5430ef3ba0fed", "version_major": 2, "version_minor": 0}</script><div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>loss (last 100 steps): 2.5750694608688356
</pre></div>
</div>
<img alt="../../_images/b10974e8c9b1a048451562dd0fe68af624ad216826fdb51131ff1c6802c8b2ae.svg" src="../../_images/b10974e8c9b1a048451562dd0fe68af624ad216826fdb51131ff1c6802c8b2ae.svg" /></div>
</div>
<p>Looks better behaved compared to the previous plot (unnormalized with <code class="docutils literal notranslate"><span class="pre">lr=2.0</span></code>):</p>
<div class="cell tag_hide-output docutils container">
<div class="cell_input above-output-prompt docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plot_training_histograms</span><span class="p">([</span><span class="n">linear_outs_stats</span><span class="p">,</span> <span class="n">linear_grad_stats</span><span class="p">],</span> <span class="n">labels</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;Outputs&quot;</span><span class="p">,</span> <span class="s2">&quot;Gradients&quot;</span><span class="p">],</span> <span class="n">cmaps</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;viridis&quot;</span><span class="p">,</span> <span class="s2">&quot;inferno&quot;</span><span class="p">],</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span> <span class="mf">1.2</span><span class="p">))</span>
</pre></div>
</div>
</div>
<details class="hide below-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell output</span>
<span class="expanded">Hide code cell output</span>
</summary>
<div class="cell_output docutils container">
<img alt="../../_images/ba0376363cb6b6180a880098ab99c77fdf959bab7e2b526012f6d08c0f885b14.svg" src="../../_images/ba0376363cb6b6180a880098ab99c77fdf959bab7e2b526012f6d08c0f885b14.svg" /></div>
</details>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plot_training_stats</span><span class="p">([</span><span class="n">linear_outs_stats</span><span class="p">,</span> <span class="n">linear_grad_stats</span><span class="p">],</span> <span class="n">labels</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;Outputs&quot;</span><span class="p">,</span> <span class="s2">&quot;Gradients&quot;</span><span class="p">],</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span> <span class="mf">1.2</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/f4055f17c2d9ed16616820d08374fa7e9728e31c8e7c58b96218a5206c500074.svg" src="../../_images/f4055f17c2d9ed16616820d08374fa7e9728e31c8e7c58b96218a5206c500074.svg" /></div>
</div>
</section>
<section id="gradient-normalization">
<h3>Gradient normalization<a class="headerlink" href="#gradient-normalization" title="Link to this heading">#</a></h3>
<p>If we look at the above plot for the gradient distribution over time, LN seems to help not only with forward propagation (i.e. the model still making meaningful predictions despite large weights), but also with backward propagation since the gradient distributions look better compared to the unnormalized network.
This makes sense, since the preactivation neurons <span class="math notranslate nohighlight">\(\boldsymbol{\mathsf{z}}\)</span> are now interacting (<a class="reference internal" href="#layer-norm-compgraph"><span class="std std-numref">Fig. 50</span></a>), these neurons and consequently their weights are then forced to co-adapt.
The exact dependency can be seen in the BP equations for LN which we derive below.</p>
<figure class="align-default" id="layer-norm-compgraph">
<a class="reference internal image-reference" href="../../_images/05-layer-norm-compgraph.svg"><img alt="../../_images/05-layer-norm-compgraph.svg" src="../../_images/05-layer-norm-compgraph.svg" width="250px" /></a>
<figcaption>
<p><span class="caption-number">Fig. 50 </span><span class="caption-text">Computational graph of layer normalization. Dependencies are indicated by arrows.</span><a class="headerlink" href="#layer-norm-compgraph" title="Link to this image">#</a></p>
</figcaption>
</figure>
<p>Let <span class="math notranslate nohighlight">\(\boldsymbol{\mathsf{y}} = \text{LN}_{\gamma, \beta}(\boldsymbol{\mathsf{z}}).\)</span> Parameters are straightforward:</p>
<div class="math notranslate nohighlight">
\[
\frac{\partial\mathcal{L}}{\partial \gamma_j} = \frac{\partial \mathcal L}{\partial {\mathsf{y}}_{j}} \hat{{\mathsf{z}}}_j \quad \text{and} \quad \frac{\partial\mathcal{L}}{\partial \beta_j} = \frac{\partial \mathcal L}{\partial {\mathsf{y}}_{j}}.
\]</div>
<p>Next, we calculate the layer stats:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
\frac{\partial \mathcal L}{\partial \sigma^2} &amp;= \sum_i\frac{\partial \mathcal L}{\partial {\mathsf{y}}_i} \frac{\partial {\mathsf{y}}_i}{\partial \sigma^2}
\\
&amp;= \sum_i \frac{\partial \mathcal L}{\partial {\mathsf{y}}_i} \gamma_i\left({\mathsf{z}}_i-\mu\right) \cdot-\frac{1}{2}\left(\sigma^2+\epsilon\right)^{-\frac{3}{2}} 
\\
\\
\frac{\partial \mathcal{L}}{\partial \mu}&amp;=\frac{\partial \mathcal{L}}{\partial \sigma^2} \frac{\partial \sigma^2}{\partial \mu}+\sum_i \frac{\partial \mathcal{L}}{\partial {\mathsf{y}}_i}  \frac{\partial {\mathsf{y}}_i}{\partial \mu} \\
\\
&amp;=\frac{\partial \mathcal{L}}{\partial \sigma^2} \frac{2}{n} \underbrace{\sum_i \left({\mathsf{z}}_i-\mu\right)}_{0} \cdot-1 +
\sum_i \frac{\partial \mathcal{L}}{\partial {\mathsf{y}}_i} \frac{\partial {\mathsf{y}}_i}{\partial \mu}
\\
&amp;= \sum_i \frac{\partial \mathcal{L}}{\partial {\mathsf{y}}_i} \frac{\partial {\mathsf{y}}_i}{\partial \mu} \\
&amp;= \sum_i \frac{\partial \mathcal{L}}{\partial {\mathsf{y}}_i} \frac{\partial}{\partial \mu}\left[\frac{\gamma_i({\mathsf{z}}_i-\mu)}{\sqrt{\sigma^2+\epsilon}}+\beta_i\right] 
= \sum_i \frac{\partial \mathcal{L}}{\partial {\mathsf{y}}_i}  \frac{-\gamma_i}{\sqrt{\sigma^2+\epsilon}}
\end{aligned}
\end{split}\]</div>
<p>Pushing these gradients to the input node:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
\frac{\partial \mathcal{L}}{\partial {\mathsf{z}}_j} &amp;=\frac{\partial \mathcal{L}}{\partial {\mathsf{y}}_j} \frac{\partial {\mathsf{y}}_j}{\partial {\mathsf{z}}_j}+\frac{\partial \mathcal{L}}{\partial \sigma^2} \frac{\partial \sigma^2}{\partial {\mathsf{z}}_j}+\frac{\partial \mathcal{L}}{\partial \mu} \frac{\partial \mu}{\partial {\mathsf{z}}_j} \\
&amp;= \frac{\partial \mathcal{L}}{\partial {\mathsf{y}}_j} \frac{\gamma_j}{\sqrt{\sigma^2+\varepsilon}}+\frac{\partial \mathcal{L}}{\partial \sigma^2} \frac{2}{n}\left({\mathsf{z}}_j-\mu\right)+\frac{\partial \mathcal{L}}{\partial \mu} \frac{1}{n} \\
&amp;= \frac{\partial \mathcal{L}}{\partial {\mathsf{y}}_j} \frac{\gamma_j}{\sqrt{\sigma^2+\varepsilon}}
+
\frac{1}{n}\frac{{\mathsf{z}}_j-\mu}{\sqrt{\sigma^2+\varepsilon}}
\sum_{i} \frac{\partial \mathcal L}{\partial {\mathsf{y}}_{i}} \frac{-\gamma_i}{\sqrt{\sigma^2+\varepsilon}}\frac{{\mathsf{z}}_{i}-\mu}{\sqrt{\sigma^2+\varepsilon} }
+\frac{1}{n}  \sum_{i} \frac{\partial \mathcal{L}}{\partial {\mathsf{y}}_{i}} \frac{-\gamma_i}{\sqrt{\sigma^2+\varepsilon}} \\
&amp;=
\frac{1}{\sqrt{\sigma^2+\varepsilon}} \left(
\gamma_j\frac{\partial \mathcal{L}}{\partial {\mathsf{y}}_j} 
-
\frac{1}{{n}}\hat{{\mathsf{z}}}_j
\sum_{i} \gamma_i \frac{\partial \mathcal L}{\partial {\mathsf{y}}_{i}} \hat{{\mathsf{z}}}_{i}
- \frac{1}{n} \sum_{i} \gamma_i \frac{\partial \mathcal{L}}{\partial {\mathsf{y}}_{i}} \right).
\end{aligned}
\end{split}\]</div>
<p>This centers the output gradients and the middle term removes the component of the output gradient along the standardized preactivation <span class="math notranslate nohighlight">\(\hat{\boldsymbol{\mathsf{z}}}\)</span> in the layer dimension.
Observe that <span class="math notranslate nohighlight">\(\sum_{j} \frac{\partial \mathcal{L}}{\partial {\mathsf{z}}_j} = 0\)</span> so that the mean of downstream gradients is zero. Furthermore, it can be shown (see <span id="id10">[<a class="reference internal" href="../../intro.html#id105" title="Jingjing Xu, Xu Sun, Zhiyuan Zhang, Guangxiang Zhao, and Junyang Lin. Understanding and improving layer normalization. CoRR, 2019. URL: http://arxiv.org/abs/1911.07013, arXiv:1911.07013.">XSZ+19</a>]</span>) that:</p>
<aside class="margin sidebar">
<p class="sidebar-title"></p>
<p>Note: <span class="math notranslate nohighlight">\(\frac{1}{d}\left\|\frac{\partial \mathcal{L}}{\partial {\mathsf{z}}}\right\|_2^2 = \text{var}_j\left[\frac{\partial \mathcal{L}}{\partial {\mathsf{z}_j}}\right]\)</span></p>
</aside>
<div class="math notranslate nohighlight">
\[\left\|\frac{\partial \mathcal{L}}{\partial {\mathsf{z}}}\right\|_2 \leq \frac{\sqrt{d}}{\sigma}\;\underset{j}{\text{std}}\left[\gamma_j \frac{\partial \mathcal{L}}{\partial {\mathsf{y}_j}}\right]\]</div>
<p>where <span class="math notranslate nohighlight">\(d = |{\mathsf{z}}|.\)</span> Hence, a larger variance of preactivations result in a reduction in the variance of gradients, and vice-versa.
This centering and rescaling of gradients can be thought of as <strong>gradient normalization</strong>, which extends to weight gradients since <span class="math notranslate nohighlight">\(\frac{\partial\mathcal{L}}{\partial \boldsymbol{\mathsf{w}}} = \boldsymbol{\mathsf{x}}^\top \frac{\partial\mathcal{L}}{\partial \boldsymbol{\mathsf{z}}}\)</span> where both factors are centered with controlled variance.
This property can then be interpreted as <strong>auto-tuning</strong> property of LN, which allows training even with large learning rate. A large weight update can increase the magnitude of the weights, resulting in a large variance in <span class="math notranslate nohighlight">\(\boldsymbol{\mathsf{z}}.\)</span> LN counteracts this in the forward direction by construction, but the above inequality implies that the effective LR for the weights is reduced by <span class="math notranslate nohighlight">\(\frac{1}{\sigma}.\)</span></p>
<br>
<p><strong>Gradient checking.</strong> Verifying the formulas empirically using <code class="docutils literal notranslate"><span class="pre">autograd</span></code>. Here we asumme <code class="docutils literal notranslate"><span class="pre">x</span></code> is some output of a previous activation layer. Then, <code class="docutils literal notranslate"><span class="pre">z</span></code> is the current preactivation which we pass through LN to get <code class="docutils literal notranslate"><span class="pre">y</span></code>. The outputs <code class="docutils literal notranslate"><span class="pre">y</span></code> are passed as logits to calculate cross-entropy, with respect to some random labels, which we use to backpropagate.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">n</span> <span class="o">=</span> <span class="mi">128</span>
<span class="n">m</span> <span class="o">=</span> <span class="mi">256</span>

<span class="c1"># Forward pass graph</span>
<span class="n">B</span> <span class="o">=</span> <span class="mi">10000</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tanh</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">((</span><span class="n">B</span><span class="p">,</span> <span class="n">m</span><span class="p">),</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">))</span>
<span class="n">w</span> <span class="o">=</span> <span class="mi">3</span> <span class="o">*</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">((</span><span class="n">m</span><span class="p">,</span> <span class="n">n</span><span class="p">),</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">z</span> <span class="o">=</span> <span class="n">x</span> <span class="o">@</span> <span class="n">w</span>

<span class="c1"># Nudge params to make sure 1 and 0 do not hide bugs</span>
<span class="n">ฮต</span> <span class="o">=</span> <span class="mf">1e-5</span>
<span class="n">ฮณ</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">n</span><span class="p">)</span><span class="o">.</span><span class="n">requires_grad_</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>
<span class="n">ฮฒ</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">n</span><span class="p">)</span><span class="o">.</span><span class="n">requires_grad_</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>
<span class="n">v</span> <span class="o">=</span> <span class="n">z</span><span class="o">.</span><span class="n">var</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">unbiased</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>  <span class="c1"># (!) Hours were spent finding an error in the calculation. Forgot default: unbiased=True</span>
<span class="n">ฮผ</span> <span class="o">=</span> <span class="n">z</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span> 
<span class="n">zฬ</span> <span class="o">=</span> <span class="p">(</span><span class="n">z</span> <span class="o">-</span> <span class="n">ฮผ</span><span class="p">)</span> <span class="o">/</span> <span class="n">torch</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">v</span> <span class="o">+</span> <span class="n">ฮต</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">ฮณ</span> <span class="o">*</span> <span class="n">zฬ</span> <span class="o">+</span> <span class="n">ฮฒ</span>

<span class="k">for</span> <span class="n">u</span> <span class="ow">in</span> <span class="p">[</span><span class="n">z</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">w</span><span class="p">,</span> <span class="n">ฮณ</span><span class="p">,</span> <span class="n">ฮฒ</span><span class="p">,</span> <span class="n">v</span><span class="p">,</span> <span class="n">ฮผ</span><span class="p">]:</span>
    <span class="n">u</span><span class="o">.</span><span class="n">retain_grad</span><span class="p">()</span>

<span class="n">t</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="n">B</span><span class="p">,))</span>
<span class="n">loss</span> <span class="o">=</span> <span class="o">-</span><span class="n">torch</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)[</span><span class="nb">range</span><span class="p">(</span><span class="n">B</span><span class="p">),</span> <span class="n">t</span><span class="p">])</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span> <span class="o">/</span> <span class="n">B</span>
<span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<p>Computing gradients by hand:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">dy</span> <span class="o">=</span> <span class="n">y</span><span class="o">.</span><span class="n">grad</span>
<span class="n">dฮณ</span> <span class="o">=</span> <span class="p">(</span><span class="n">dy</span> <span class="o">*</span> <span class="n">zฬ</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="n">dฮฒ</span> <span class="o">=</span> <span class="n">dy</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="n">dv</span> <span class="o">=</span> <span class="o">-</span><span class="p">(</span><span class="n">ฮณ</span> <span class="o">*</span> <span class="n">dy</span> <span class="o">*</span> <span class="p">(</span><span class="n">z</span> <span class="o">-</span> <span class="n">ฮผ</span><span class="p">))</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span> <span class="o">*</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="p">(</span><span class="n">v</span> <span class="o">+</span> <span class="n">ฮต</span><span class="p">)</span> <span class="o">**</span> <span class="o">-</span><span class="mf">1.5</span>
<span class="n">dฮผ</span> <span class="o">=</span> <span class="o">-</span><span class="p">(</span><span class="n">ฮณ</span> <span class="o">*</span> <span class="n">dy</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="n">v</span> <span class="o">+</span> <span class="n">ฮต</span><span class="p">)</span> <span class="o">**</span> <span class="mf">0.5</span>
<span class="n">dz</span> <span class="o">=</span> <span class="n">ฮณ</span> <span class="o">*</span> <span class="n">dy</span> <span class="o">-</span> <span class="p">(</span><span class="mi">1</span> <span class="o">/</span> <span class="n">n</span><span class="p">)</span> <span class="o">*</span> <span class="n">zฬ</span> <span class="o">*</span> <span class="p">(</span><span class="n">ฮณ</span> <span class="o">*</span> <span class="n">dy</span> <span class="o">*</span> <span class="n">zฬ</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span> <span class="o">-</span> <span class="p">(</span><span class="mi">1</span> <span class="o">/</span> <span class="n">n</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="n">ฮณ</span> <span class="o">*</span> <span class="n">dy</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">dz</span> <span class="o">*=</span> <span class="p">(</span><span class="mi">1</span> <span class="o">/</span> <span class="n">torch</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">v</span> <span class="o">+</span> <span class="n">ฮต</span><span class="p">))</span>
<span class="n">dx</span> <span class="o">=</span> <span class="n">dz</span> <span class="o">@</span> <span class="n">w</span><span class="o">.</span><span class="n">T</span>
<span class="n">dw</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">T</span> <span class="o">@</span> <span class="n">dz</span>
</pre></div>
</div>
</div>
</div>
<p>Gradient check:</p>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">compare</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="n">dt</span><span class="p">,</span> <span class="n">t</span><span class="p">):</span>
    <span class="n">exact</span>  <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">all</span><span class="p">(</span><span class="n">dt</span> <span class="o">==</span> <span class="n">t</span><span class="o">.</span><span class="n">grad</span><span class="p">)</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
    <span class="n">maxdiff</span> <span class="o">=</span> <span class="p">(</span><span class="n">dt</span> <span class="o">-</span> <span class="n">t</span><span class="o">.</span><span class="n">grad</span><span class="p">)</span><span class="o">.</span><span class="n">abs</span><span class="p">()</span><span class="o">.</span><span class="n">max</span><span class="p">()</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
    <span class="n">approx</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">allclose</span><span class="p">(</span><span class="n">dt</span><span class="p">,</span> <span class="n">t</span><span class="o">.</span><span class="n">grad</span><span class="p">,</span> <span class="n">rtol</span><span class="o">=</span><span class="mf">1e-7</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;</span><span class="si">{</span><span class="n">name</span><span class="si">}</span><span class="s1"> | exact: </span><span class="si">{</span><span class="nb">str</span><span class="p">(</span><span class="n">exact</span><span class="p">)</span><span class="si">:</span><span class="s1">5s</span><span class="si">}</span><span class="s1"> | approx: </span><span class="si">{</span><span class="nb">str</span><span class="p">(</span><span class="n">approx</span><span class="p">)</span><span class="si">:</span><span class="s1">5s</span><span class="si">}</span><span class="s1"> | maxdiff: </span><span class="si">{</span><span class="n">maxdiff</span><span class="si">:</span><span class="s1">.2e</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>

<span class="n">compare</span><span class="p">(</span><span class="s1">&#39;y&#39;</span><span class="p">,</span> <span class="n">dy</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="n">compare</span><span class="p">(</span><span class="s1">&#39;ฮณ&#39;</span><span class="p">,</span> <span class="n">dฮณ</span><span class="p">,</span> <span class="n">ฮณ</span><span class="p">)</span>
<span class="n">compare</span><span class="p">(</span><span class="s1">&#39;ฮฒ&#39;</span><span class="p">,</span> <span class="n">dฮฒ</span><span class="p">,</span> <span class="n">ฮฒ</span><span class="p">)</span>
<span class="n">compare</span><span class="p">(</span><span class="s1">&#39;v&#39;</span><span class="p">,</span> <span class="n">dv</span><span class="p">,</span> <span class="n">v</span><span class="p">)</span>
<span class="n">compare</span><span class="p">(</span><span class="s1">&#39;ฮผ&#39;</span><span class="p">,</span> <span class="n">dฮผ</span><span class="p">,</span> <span class="n">ฮผ</span><span class="p">)</span>
<span class="n">compare</span><span class="p">(</span><span class="s1">&#39;z&#39;</span><span class="p">,</span> <span class="n">dz</span><span class="p">,</span> <span class="n">z</span><span class="p">)</span>
<span class="n">compare</span><span class="p">(</span><span class="s1">&#39;w&#39;</span><span class="p">,</span> <span class="n">dw</span><span class="p">,</span> <span class="n">w</span><span class="p">)</span>
<span class="n">compare</span><span class="p">(</span><span class="s1">&#39;x&#39;</span><span class="p">,</span> <span class="n">dx</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>
</pre></div>
</div>
</div>
</details>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>y | exact: True  | approx: True  | maxdiff: 0.00e+00
ฮณ | exact: True  | approx: True  | maxdiff: 0.00e+00
ฮฒ | exact: True  | approx: True  | maxdiff: 0.00e+00
v | exact: False | approx: True  | maxdiff: 8.53e-14
ฮผ | exact: False | approx: True  | maxdiff: 3.64e-12
z | exact: False | approx: True  | maxdiff: 1.82e-12
w | exact: False | approx: True  | maxdiff: 2.04e-10
x | exact: False | approx: True  | maxdiff: 2.18e-11
</pre></div>
</div>
</div>
</div>
<br>
<p><strong>Demonstrating the variance relation.</strong> Note the factor <span class="math notranslate nohighlight">\(\gamma_j\)</span> is obtained by <a class="reference external" href="https://ocw.mit.edu/courses/18-098-street-fighting-mathematics-january-iap-2008/">dimensional analysis</a> since the proof in <span id="id11">[<a class="reference internal" href="../../intro.html#id105" title="Jingjing Xu, Xu Sun, Zhiyuan Zhang, Guangxiang Zhao, and Junyang Lin. Understanding and improving layer normalization. CoRR, 2019. URL: http://arxiv.org/abs/1911.07013, arXiv:1911.07013.">XSZ+19</a>]</span> sets <span class="math notranslate nohighlight">\(\gamma_j = 1\)</span> for convenience. Moreover, <span class="math notranslate nohighlight">\(\gamma_j \frac{\partial \mathcal L}{\partial {\mathsf{y}}_j}\)</span> seems to always appear together in the equations.  But this seems to be correct:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">n</span> <span class="o">=</span> <span class="n">dz</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
<span class="n">dz_norm_2</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">dz</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="o">**</span> <span class="mi">2</span>
<span class="p">((</span><span class="mi">1</span> <span class="o">/</span> <span class="n">n</span><span class="p">)</span> <span class="o">*</span> <span class="n">dz_norm_2</span> <span class="o">&lt;=</span> <span class="mi">1</span> <span class="o">/</span> <span class="p">(</span><span class="n">v</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span> <span class="o">+</span> <span class="n">ฮต</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="n">ฮณ</span> <span class="o">*</span> <span class="n">dy</span><span class="p">)</span><span class="o">.</span><span class="n">var</span><span class="p">(</span><span class="mi">1</span><span class="p">))</span><span class="o">.</span><span class="n">float</span><span class="p">()</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>1.0
</pre></div>
</div>
</div>
</div>
<p>Observe that the bound is super tight:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">gap</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">/</span> <span class="p">(</span><span class="n">v</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span> <span class="o">+</span> <span class="n">ฮต</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="n">ฮณ</span> <span class="o">*</span> <span class="n">dy</span><span class="p">)</span><span class="o">.</span><span class="n">var</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span> <span class="o">-</span> <span class="p">(</span><span class="mi">1</span> <span class="o">/</span> <span class="n">n</span><span class="p">)</span> <span class="o">*</span> <span class="n">dz_norm_2</span>
<span class="n">gap</span><span class="o">.</span><span class="n">min</span><span class="p">()</span><span class="o">.</span><span class="n">item</span><span class="p">(),</span> <span class="n">gap</span><span class="o">.</span><span class="n">max</span><span class="p">()</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>(2.436977316721666e-17, 5.478787996199275e-14)
</pre></div>
</div>
</div>
</div>
<aside class="margin sidebar">
<p class="sidebar-title"></p>
<p><a class="reference external" href="https://jakevdp.github.io/PythonDataScienceHandbook/02.05-computation-on-arrays-broadcasting.html#Rules-of-Broadcasting">Broadcasting rules</a></p>
</aside>
<br>
<p><strong>Remark.</strong> The above code uses a lot of broadcasting. The rule is that the leftmost dimension of the lower rank tensor is padded with 1 whenever the ranks of the tensors are unequal. If the shape does not match between two equal ranked tensors, the dimension with 1 is stretched to match the other shape. If any two non-unit dimensions are unequal, then an error is raised.</p>
<p>For example, <code class="docutils literal notranslate"><span class="pre">z</span> <span class="pre">-</span> <span class="pre">z.mean(1)</span></code> fails. The shapes are <code class="docutils literal notranslate"><span class="pre">[B,</span> <span class="pre">n]</span></code> and <code class="docutils literal notranslate"><span class="pre">[B,]</span></code>, respectively. The latter becomes <code class="docutils literal notranslate"><span class="pre">[1,</span> <span class="pre">B]</span></code> after the first rule, then <code class="docutils literal notranslate"><span class="pre">[B,</span> <span class="pre">B]</span></code>. Another one that gives a bug (i.e. a mistake) is <code class="docutils literal notranslate"><span class="pre">(1</span> <span class="pre">/</span> <span class="pre">v)</span> <span class="pre">*</span> <span class="pre">(ฮณ</span> <span class="pre">*</span> <span class="pre">dy).var(1)</span></code> since we have <code class="docutils literal notranslate"><span class="pre">[B,</span> <span class="pre">1]</span></code> and <code class="docutils literal notranslate"><span class="pre">[B,]</span></code> respectively, so that the latter becomes <code class="docutils literal notranslate"><span class="pre">[1,</span> <span class="pre">B]</span></code>. Hence, the final shape of the two tensors is <code class="docutils literal notranslate"><span class="pre">[B,</span> <span class="pre">B]</span></code>!
On the other hand, <code class="docutils literal notranslate"><span class="pre">ฮณ</span> <span class="pre">*</span> <span class="pre">dy</span></code> works since <code class="docutils literal notranslate"><span class="pre">[n,]</span></code> and <code class="docutils literal notranslate"><span class="pre">[B,</span> <span class="pre">n]</span></code>. The former becomes <code class="docutils literal notranslate"><span class="pre">[1,</span> <span class="pre">n]</span></code> which becomes <code class="docutils literal notranslate"><span class="pre">[B,</span> <span class="pre">n]</span></code>.</p>
</section>
<section id="weight-update-ratio">
<h3>Weight update ratio<a class="headerlink" href="#weight-update-ratio" title="Link to this heading">#</a></h3>
<p>Another thing we can look at is the magnitude of weight updates relative to the weights.
That is, the ratio <span class="math notranslate nohighlight">\(\zeta_k = \frac{\|\lambda {\nabla}{\boldsymbol{\mathsf{w}}_k}\|}{\|\boldsymbol{\mathsf{w}}_k\|}\)</span> where <span class="math notranslate nohighlight">\({\nabla}{\boldsymbol{\mathsf{w}}_k} = \frac{\partial \mathcal{L}}{\partial \boldsymbol{\mathsf{w}}_k}.\)</span> A low overall <span class="math notranslate nohighlight">\(\zeta_k\)</span> means that convergence is slow, whereas a high <span class="math notranslate nohighlight">\(\zeta_k\)</span> allows the network to quickly unlearn good weight configurations. Having layers with varying <span class="math notranslate nohighlight">\(\zeta_k\)</span> is not good for training as weights learned by slower layers can become outdated as faster layers learn new weights, unless the layers are doing different things (e.g. embeddings and logits). This can be seen in the plot below.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">WeightUpdateRatio</span><span class="p">(</span><span class="n">HookHandler</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">lr</span><span class="p">:</span> <span class="nb">float</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">lr</span> <span class="o">=</span> <span class="n">lr</span>

    <span class="k">def</span> <span class="nf">hook_fn</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">module</span><span class="p">,</span> <span class="n">grad_in</span><span class="p">,</span> <span class="n">grad_out</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">records</span><span class="p">[</span><span class="n">module</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">records</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="p">[])</span>
        <span class="k">for</span> <span class="n">g</span> <span class="ow">in</span> <span class="n">grad_in</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">g</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">g</span><span class="o">.</span><span class="n">shape</span> <span class="o">==</span> <span class="n">module</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">shape</span><span class="p">:</span>
                <span class="n">w</span> <span class="o">=</span> <span class="n">module</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">data</span>
                <span class="n">g_norm</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">g</span><span class="p">)</span>
                <span class="n">w_norm</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">w</span><span class="p">)</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">records</span><span class="p">[</span><span class="n">module</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">lr</span> <span class="o">*</span> <span class="p">(</span><span class="n">g_norm</span> <span class="o">/</span> <span class="n">w_norm</span><span class="p">)</span><span class="o">.</span><span class="n">item</span><span class="p">())</span>
</pre></div>
</div>
</div>
</div>
<p>Making the network deeper:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">num_linear</span> <span class="o">=</span> <span class="mi">4</span>
<span class="n">lr</span> <span class="o">=</span> <span class="mf">0.6</span>
<span class="n">steps</span> <span class="o">=</span> <span class="mi">2000</span>
<span class="n">init_fn</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">w</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">xavier_uniform_</span><span class="p">(</span><span class="n">w</span><span class="p">,</span> <span class="n">gain</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="mi">2</span><span class="p">),</span> <span class="n">generator</span><span class="o">=</span><span class="n">g</span><span class="p">())</span>
<span class="n">weight_update_ratios</span> <span class="o">=</span> <span class="p">{}</span>

<span class="k">for</span> <span class="n">ln</span> <span class="ow">in</span> <span class="p">[</span><span class="s2">&quot;un&quot;</span><span class="p">,</span> <span class="s2">&quot;ln&quot;</span><span class="p">]:</span>
    <span class="n">use_ln</span> <span class="o">=</span> <span class="n">ln</span> <span class="o">==</span> <span class="s2">&quot;ln&quot;</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">init_model</span><span class="p">(</span><span class="n">get_model</span><span class="p">(</span><span class="n">layernorm</span><span class="o">=</span><span class="n">use_ln</span><span class="p">,</span> <span class="o">**</span><span class="n">_update</span><span class="p">(</span><span class="n">model_params</span><span class="p">,</span> <span class="n">num_linear</span><span class="o">=</span><span class="n">num_linear</span><span class="p">)),</span> <span class="n">init_fn</span><span class="p">)</span>
    <span class="n">weight_update_ratio</span> <span class="o">=</span> <span class="n">WeightUpdateRatio</span><span class="p">(</span><span class="n">lr</span><span class="p">)</span>
    <span class="n">hooks</span> <span class="o">=</span> <span class="p">[{</span><span class="s2">&quot;fwd_hooks&quot;</span><span class="p">:</span> <span class="p">[],</span> <span class="s2">&quot;bwd_hooks&quot;</span><span class="p">:</span> <span class="p">[</span><span class="n">weight_update_ratio</span><span class="p">],</span> <span class="s2">&quot;layer_types&quot;</span><span class="p">:</span> <span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">Embedding</span><span class="p">)}]</span>
    <span class="n">trainer</span> <span class="o">=</span> <span class="n">run</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">train_dataset</span><span class="p">,</span> <span class="o">**</span><span class="n">_update</span><span class="p">(</span><span class="n">train_params</span><span class="p">,</span> <span class="n">lr</span><span class="o">=</span><span class="n">lr</span><span class="p">,</span> <span class="n">steps</span><span class="o">=</span><span class="n">steps</span><span class="p">),</span> <span class="n">hooks</span><span class="o">=</span><span class="n">hooks</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;[</span><span class="si">{</span><span class="n">ln</span><span class="si">}</span><span class="s2">] loss (last 100 steps):&quot;</span><span class="p">,</span> <span class="nb">sum</span><span class="p">(</span><span class="n">trainer</span><span class="o">.</span><span class="n">logs</span><span class="p">[</span><span class="s2">&quot;train&quot;</span><span class="p">][</span><span class="s2">&quot;loss&quot;</span><span class="p">][</span><span class="o">-</span><span class="mi">100</span><span class="p">:])</span> <span class="o">/</span> <span class="mi">100</span><span class="p">)</span>
    <span class="n">weight_update_ratios</span><span class="p">[</span><span class="n">ln</span><span class="p">]</span> <span class="o">=</span> <span class="n">weight_update_ratio</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<script type="application/vnd.jupyter.widget-view+json">{"model_id": "939c7458dea942f3a6099292268f84b8", "version_major": 2, "version_minor": 0}</script><div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[un] loss (last 100 steps): 2.5491889452934267
</pre></div>
</div>
<script type="application/vnd.jupyter.widget-view+json">{"model_id": "6fc4c554619b46caa55e23d6bd4fc4a5", "version_major": 2, "version_minor": 0}</script><div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[ln] loss (last 100 steps): 2.502973110675812
</pre></div>
</div>
</div>
</div>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>

<span class="n">records</span> <span class="o">=</span> <span class="n">weight_update_ratios</span><span class="p">[</span><span class="s2">&quot;un&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">records</span>
<span class="k">for</span> <span class="n">j</span><span class="p">,</span> <span class="n">m</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">records</span><span class="o">.</span><span class="n">keys</span><span class="p">()):</span>
    <span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">moving_avg</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">log10</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">records</span><span class="p">[</span><span class="n">m</span><span class="p">])),</span> <span class="n">window</span><span class="o">=</span><span class="mf">0.05</span><span class="p">),</span> <span class="n">label</span><span class="o">=</span><span class="n">m</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span> <span class="o">+</span> <span class="s2">&quot;.&quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">records</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">j</span><span class="p">))</span>
    <span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="n">linestyle</span><span class="o">=</span><span class="s2">&quot;dotted&quot;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.6</span><span class="p">)</span>
    <span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;Unnormalized&quot;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">12</span><span class="p">)</span>
    <span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="sa">r</span><span class="s2">&quot;$\log(\lambda \| \nabla \mathsf{\mathbf</span><span class="si">{w}</span><span class="s2">} \|\; / \;\| \mathsf{\mathbf</span><span class="si">{w}</span><span class="s2">} \|)$&quot;</span><span class="p">)</span>
    <span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s2">&quot;steps&quot;</span><span class="p">)</span>
    <span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylim</span><span class="p">(</span><span class="o">-</span><span class="mi">4</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.0</span><span class="p">);</span>

<span class="n">records</span> <span class="o">=</span> <span class="n">weight_update_ratios</span><span class="p">[</span><span class="s2">&quot;ln&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">records</span>
<span class="k">for</span> <span class="n">j</span><span class="p">,</span> <span class="n">m</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">records</span><span class="o">.</span><span class="n">keys</span><span class="p">()):</span>
    <span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">moving_avg</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">log10</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">records</span><span class="p">[</span><span class="n">m</span><span class="p">])),</span> <span class="n">window</span><span class="o">=</span><span class="mf">0.05</span><span class="p">),</span> <span class="n">label</span><span class="o">=</span><span class="n">m</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span> <span class="o">+</span> <span class="s2">&quot;.&quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">records</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">j</span><span class="p">))</span>
    <span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="n">linestyle</span><span class="o">=</span><span class="s2">&quot;dotted&quot;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.6</span><span class="p">)</span>
    <span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;Normalized (linear + LN)&quot;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">12</span><span class="p">)</span>
    <span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s2">&quot;steps&quot;</span><span class="p">)</span>
    <span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="sa">r</span><span class="s2">&quot;$\log(\lambda \| \nabla \mathsf{\mathbf</span><span class="si">{w}</span><span class="s2">} \|\; / \;\| \mathsf{\mathbf</span><span class="si">{w}</span><span class="s2">} \|)$&quot;</span><span class="p">)</span>
    <span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
    <span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylim</span><span class="p">(</span><span class="o">-</span><span class="mi">4</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.0</span><span class="p">);</span>
</pre></div>
</div>
</div>
</details>
<div class="cell_output docutils container">
<img alt="../../_images/ae0cda4441e696afca4c5f59cd4e54e39b68ce8eceba5678341cca497ded07f2.svg" src="../../_images/ae0cda4441e696afca4c5f59cd4e54e39b68ce8eceba5678341cca497ded07f2.svg" /></div>
</div>
<p><strong>Figure.</strong> Model training with +4 hidden layers, Kaiming init, and learning rate of 1. Since we took the logarithm, <code class="docutils literal notranslate"><span class="pre">y=-2</span></code> correspond to an update ratio of <code class="docutils literal notranslate"><span class="pre">0.01</span></code> for one mini-batch. This roughly means that it would take 100 steps to completely erase the current weights. The update ratios for the logits and embedding layers are nicely disambiguated: the logits weights train faster while the embeddings train slower. Moreover, notice the the update rates for the hidden layers are well ordered.</p>
</section>
</section>
<section id="appendix-activations">
<h2>Appendix: Activations<a class="headerlink" href="#appendix-activations" title="Link to this heading">#</a></h2>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">gain</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s2">&quot;tanh&quot;</span><span class="p">:</span> <span class="mi">5</span><span class="o">/</span><span class="mi">3</span><span class="p">,</span>
    <span class="s2">&quot;relu&quot;</span><span class="p">:</span> <span class="n">math</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="mi">2</span><span class="p">),</span>
    <span class="s2">&quot;selu&quot;</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span>                  <span class="c1"># https://github.com/pytorch/pytorch/pull/53694#issuecomment-795683782</span>
    <span class="s2">&quot;gelu&quot;</span><span class="p">:</span> <span class="n">math</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="mi">2</span><span class="p">),</span>       <span class="c1"># GELU plot looks like ReLU</span>
<span class="p">}</span>

<span class="c1"># Feel free to add to this list</span>
<span class="n">activation_fns</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s2">&quot;tanh&quot;</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">Tanh</span><span class="p">,</span>
    <span class="s2">&quot;selu&quot;</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">SELU</span><span class="p">,</span>
    <span class="s2">&quot;relu&quot;</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">,</span>
    <span class="s2">&quot;gelu&quot;</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">GELU</span>
<span class="p">}</span>

<span class="k">def</span> <span class="nf">d</span><span class="p">(</span><span class="n">act</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Compute derivative of activation with respect to x.&quot;&quot;&quot;</span>
    <span class="n">f</span> <span class="o">=</span> <span class="n">act</span><span class="p">()</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">f</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">y</span><span class="o">.</span><span class="n">backward</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">ones_like</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">x</span><span class="o">.</span><span class="n">grad</span>

<span class="c1"># Region x s.t. |f&#39;(x)| &lt;= 0.1:</span>
<span class="n">saturation_fns</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s2">&quot;tanh&quot;</span><span class="p">:</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span><span class="o">.</span><span class="n">abs</span><span class="p">()</span> <span class="o">&gt;</span> <span class="mf">1.8183</span><span class="p">,</span>
    <span class="s2">&quot;selu&quot;</span><span class="p">:</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span> <span class="o">&lt;</span> <span class="o">-</span><span class="mf">2.8668</span><span class="p">,</span>
    <span class="s2">&quot;relu&quot;</span><span class="p">:</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span> <span class="o">&lt;</span> <span class="mf">0.0</span><span class="p">,</span>
    <span class="s2">&quot;gelu&quot;</span><span class="p">:</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="p">(</span><span class="n">x</span> <span class="o">&lt;</span> <span class="o">-</span><span class="mf">1.8611</span><span class="p">)</span> <span class="o">|</span> <span class="p">((</span><span class="o">-</span><span class="mf">1.0775</span> <span class="o">&lt;</span> <span class="n">x</span><span class="p">)</span> <span class="o">&amp;</span> <span class="p">(</span><span class="n">x</span> <span class="o">&lt;</span> <span class="o">-</span><span class="mf">0.5534</span><span class="p">)),</span>
<span class="p">}</span>
</pre></div>
</div>
</div>
</div>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">plot_activation</span><span class="p">(</span><span class="n">act</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">ax</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
    <span class="n">x</span><span class="o">.</span><span class="n">grad</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="n">f</span> <span class="o">=</span> <span class="n">activation_fns</span><span class="p">[</span><span class="n">act</span><span class="p">]</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">f</span><span class="p">()(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">df</span> <span class="o">=</span> <span class="n">d</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>
    <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">(),</span> <span class="n">y</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
    
    <span class="c1"># Plotting</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span>  <span class="n">linewidth</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;red&quot;</span><span class="p">,</span>   <span class="n">label</span><span class="o">=</span><span class="s2">&quot;f(x)&quot;</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">df</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;black&quot;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;f&#39;(x)&quot;</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="n">act</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">grid</span><span class="p">()</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_ylim</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">act</span> <span class="o">==</span> <span class="s2">&quot;tanh&quot;</span><span class="p">:</span>
        <span class="n">ax</span><span class="o">.</span><span class="n">axvspan</span><span class="p">(</span><span class="o">-</span><span class="mi">5</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.8183</span><span class="p">,</span> <span class="o">-</span><span class="mi">10</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;lightgray&quot;</span><span class="p">)</span>
        <span class="n">ax</span><span class="o">.</span><span class="n">axvspan</span><span class="p">(</span><span class="mf">1.8183</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span>   <span class="o">-</span><span class="mi">10</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;lightgray&quot;</span><span class="p">)</span>
    <span class="k">elif</span> <span class="n">act</span> <span class="o">==</span> <span class="s2">&quot;relu&quot;</span><span class="p">:</span>
        <span class="n">ax</span><span class="o">.</span><span class="n">axvspan</span><span class="p">(</span><span class="o">-</span><span class="mi">5</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="o">-</span><span class="mi">10</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;lightgray&quot;</span><span class="p">)</span>
    <span class="k">elif</span> <span class="n">act</span> <span class="o">==</span> <span class="s2">&quot;leaky_relu&quot;</span><span class="p">:</span>
        <span class="n">ax</span><span class="o">.</span><span class="n">axvspan</span><span class="p">(</span><span class="o">-</span><span class="mi">5</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="o">-</span><span class="mi">10</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;lightgray&quot;</span><span class="p">)</span>
    <span class="k">elif</span> <span class="n">act</span> <span class="o">==</span> <span class="s2">&quot;elu&quot;</span><span class="p">:</span>
        <span class="n">ax</span><span class="o">.</span><span class="n">axvspan</span><span class="p">(</span><span class="o">-</span><span class="mi">5</span><span class="p">,</span> <span class="o">-</span><span class="mf">2.3025</span><span class="p">,</span> <span class="o">-</span><span class="mi">10</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;lightgray&quot;</span><span class="p">)</span>
    <span class="k">elif</span> <span class="n">act</span> <span class="o">==</span> <span class="s2">&quot;selu&quot;</span><span class="p">:</span>
        <span class="n">ax</span><span class="o">.</span><span class="n">axvspan</span><span class="p">(</span><span class="o">-</span><span class="mi">5</span><span class="p">,</span>      <span class="o">-</span><span class="mf">2.8668</span><span class="p">,</span> <span class="o">-</span><span class="mi">10</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;lightgray&quot;</span><span class="p">)</span>
    <span class="k">elif</span> <span class="n">act</span> <span class="o">==</span> <span class="s2">&quot;gelu&quot;</span><span class="p">:</span>
        <span class="n">ax</span><span class="o">.</span><span class="n">axvspan</span><span class="p">(</span><span class="o">-</span><span class="mi">5</span><span class="p">,</span>      <span class="o">-</span><span class="mf">1.8611</span><span class="p">,</span> <span class="o">-</span><span class="mi">10</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;lightgray&quot;</span><span class="p">)</span>
        <span class="n">ax</span><span class="o">.</span><span class="n">axvspan</span><span class="p">(</span><span class="o">-</span><span class="mf">1.0775</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.5534</span><span class="p">,</span> <span class="o">-</span><span class="mi">10</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;lightgray&quot;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;|f&#39;(x)| โค 0.1&quot;</span><span class="p">)</span>
        <span class="n">ax</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="s2">&quot;lower right&quot;</span><span class="p">)</span>


<span class="c1"># Plotting</span>
<span class="n">rows</span> <span class="o">=</span> <span class="n">math</span><span class="o">.</span><span class="n">ceil</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">activation_fns</span><span class="p">)</span> <span class="o">/</span> <span class="mf">2.0</span><span class="p">)</span>
<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">rows</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="n">rows</span><span class="o">*</span><span class="mf">3.5</span><span class="p">))</span>

<span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">1000</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">act_name</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">activation_fns</span><span class="o">.</span><span class="n">keys</span><span class="p">()):</span>
    <span class="n">plot_activation</span><span class="p">(</span><span class="n">act_name</span><span class="p">,</span> <span class="n">ax</span><span class="p">[</span><span class="nb">divmod</span><span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="mi">2</span><span class="p">)],</span> <span class="n">x</span><span class="p">)</span>  <span class="c1"># divmod(m, n) = m // n, m % n</span>

<span class="n">fig</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
</pre></div>
</div>
</div>
</details>
<div class="cell_output docutils container">
<img alt="../../_images/f581ccbb754360dd08646cd29806b9586f541e9205f3fbaeaba0236e458163b7.svg" src="../../_images/f581ccbb754360dd08646cd29806b9586f541e9205f3fbaeaba0236e458163b7.svg" /></div>
</div>
<p><strong>Figure.</strong> Activations functions and its derivatives. Saturation regions are highlighted grey. Note that GELU has 4 regions where the others only has 3 (Tanh) or 2 (ReLU and SELU). SELU has a spike in its derivative at zero, which can cause issues with large LR.</p>
<p>Having outputs that range to <span class="math notranslate nohighlight">\(+\infty\)</span> for ReLU (adopted by SELU and GELU) increases network expressivity. On the other hand, Tanh is bounded in <span class="math notranslate nohighlight">\([-1, 1]\)</span> with gradients that saturate in the asymptotes.
Moreover, a derivative of 1 for a large region helps to accelerate training and avoid vanishing gradients. Recall that if <span class="math notranslate nohighlight">\(\boldsymbol{\mathsf{y}} = \varphi(\boldsymbol{\mathsf{z}})\)</span>, where <span class="math notranslate nohighlight">\(\boldsymbol{\mathsf{z}} = \boldsymbol{\mathsf{x}}^\top\boldsymbol{\mathsf{w}}\)</span> and <span class="math notranslate nohighlight">\(\varphi\)</span> is an activation, then</p>
<div class="math notranslate nohighlight">
\[
\frac{\partial \mathcal L}{\partial \boldsymbol{\mathsf{x}}} = \left( \frac{\partial \mathcal L}{\partial \boldsymbol{\mathsf{y}}} \odot \varphi^{\prime}(\boldsymbol{\mathsf{z}}) \right)  \boldsymbol{\mathsf{w}}^{\top}.
\]</div>
<p>Here <span class="math notranslate nohighlight">\(\varphi^{\prime}(\boldsymbol{\mathsf{z}})\)</span> is a matrix with entries <span class="math notranslate nohighlight">\(\varphi^{\prime}(\boldsymbol{\mathsf{z}}_{bj})\)</span> and <span class="math notranslate nohighlight">\(\odot\)</span> denotes the <a class="reference external" href="https://en.wikipedia.org/wiki/Hadamard_product_(matrices)">Hadamard product</a>.</p>
<p>Stacking layers effectively forms a chain of weights matrix multiplication with entries of the gradients scaled in between by derivatives of the activation function. For <span class="math notranslate nohighlight">\(\varphi = \tanh\)</span> the gradients are scaled down since <span class="math notranslate nohighlight">\(\tanh^\prime(z) \leq 1\)</span> for any <span class="math notranslate nohighlight">\(z \in \mathbb R.\)</span> Hence, creating a deep stack of fully-connected hidden layers can result in vanishing gradients at initialization.  ReLU is interesting as it allows sparse gradient updates since <span class="math notranslate nohighlight">\(\varphi^\prime(z) \in [0, 1].\)</span></p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">get_model</span><span class="p">(</span><span class="n">vocab_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
              <span class="n">block_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
              <span class="n">emb_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
              <span class="n">width</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
              <span class="n">num_linear</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
              <span class="n">activation</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;relu&quot;</span><span class="p">,</span>
              <span class="n">layernorm</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">):</span>

    <span class="n">act_fn</span> <span class="o">=</span> <span class="n">activation_fns</span><span class="p">[</span><span class="n">activation</span><span class="p">]</span>
    <span class="n">layers</span> <span class="o">=</span> <span class="p">[</span>
        <span class="n">nn</span><span class="o">.</span><span class="n">Embedding</span><span class="p">(</span><span class="n">vocab_size</span><span class="p">,</span> <span class="n">emb_size</span><span class="p">),</span> 
        <span class="n">nn</span><span class="o">.</span><span class="n">Flatten</span><span class="p">(),</span>
        <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">block_size</span> <span class="o">*</span> <span class="n">emb_size</span><span class="p">,</span> <span class="n">width</span><span class="p">)</span>
    <span class="p">]</span>
    <span class="n">layers</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">LayerNorm</span><span class="p">(</span><span class="n">width</span><span class="p">))</span> <span class="k">if</span> <span class="n">layernorm</span> <span class="k">else</span> <span class="s2">&quot;&quot;</span>
    <span class="n">layers</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">act_fn</span><span class="p">())</span>
    
    <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_linear</span><span class="p">):</span>
        <span class="n">layers</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">width</span><span class="p">,</span> <span class="n">width</span><span class="p">))</span>
        <span class="n">layers</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">LayerNorm</span><span class="p">(</span><span class="n">width</span><span class="p">))</span> <span class="k">if</span> <span class="n">layernorm</span> <span class="k">else</span> <span class="s2">&quot;&quot;</span>
        <span class="n">layers</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">act_fn</span><span class="p">())</span>

    <span class="n">layers</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">width</span><span class="p">,</span> <span class="n">vocab_size</span><span class="p">))</span>
    <span class="n">layers</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">LayerNorm</span><span class="p">(</span><span class="n">vocab_size</span><span class="p">))</span> <span class="k">if</span> <span class="n">layernorm</span> <span class="k">else</span> <span class="s2">&quot;&quot;</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span><span class="o">*</span><span class="n">layers</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">model</span>
</pre></div>
</div>
</div>
</div>
<p>Running an experiment:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">model_params</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s2">&quot;vocab_size&quot;</span><span class="p">:</span> <span class="mi">29</span><span class="p">,</span>
    <span class="s2">&quot;block_size&quot;</span><span class="p">:</span> <span class="mi">3</span><span class="p">,</span>
    <span class="s2">&quot;emb_size&quot;</span><span class="p">:</span> <span class="mi">10</span><span class="p">,</span>
    <span class="s2">&quot;width&quot;</span><span class="p">:</span> <span class="mi">30</span><span class="p">,</span>
    <span class="s2">&quot;num_linear&quot;</span><span class="p">:</span> <span class="mi">6</span>
<span class="p">}</span>

<span class="n">train_params</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s2">&quot;steps&quot;</span><span class="p">:</span> <span class="mi">2000</span><span class="p">,</span>
    <span class="s2">&quot;batch_size&quot;</span><span class="p">:</span> <span class="mi">32</span><span class="p">,</span>
    <span class="s2">&quot;lr&quot;</span><span class="p">:</span> <span class="mf">0.2</span>
<span class="p">}</span>


<span class="n">train_hooks</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;ln&quot;</span><span class="p">:</span> <span class="p">{},</span> <span class="s2">&quot;un&quot;</span><span class="p">:</span> <span class="p">{}}</span>
<span class="n">trainers</span>    <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;ln&quot;</span><span class="p">:</span> <span class="p">{},</span> <span class="s2">&quot;un&quot;</span><span class="p">:</span> <span class="p">{}}</span>

<span class="k">for</span> <span class="n">act</span> <span class="ow">in</span> <span class="n">activation_fns</span><span class="o">.</span><span class="n">keys</span><span class="p">():</span>
    <span class="k">for</span> <span class="n">use_ln</span> <span class="ow">in</span> <span class="p">[</span><span class="kc">True</span><span class="p">,</span> <span class="kc">False</span><span class="p">]:</span>
        <span class="n">init_fn</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">w</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">xavier_uniform_</span><span class="p">(</span><span class="n">w</span><span class="p">,</span> <span class="n">gain</span><span class="o">=</span><span class="n">gain</span><span class="p">[</span><span class="n">act</span><span class="p">],</span> <span class="n">generator</span><span class="o">=</span><span class="n">g</span><span class="p">())</span>
        <span class="n">activation_stats</span> <span class="o">=</span> <span class="n">OutputStats</span><span class="p">()</span>
        <span class="n">weight_grad_stats</span> <span class="o">=</span> <span class="n">WeightGradientStats</span><span class="p">()</span>
        <span class="n">weight_update_ratio</span> <span class="o">=</span> <span class="n">WeightUpdateRatio</span><span class="p">(</span><span class="n">train_params</span><span class="p">[</span><span class="s2">&quot;lr&quot;</span><span class="p">])</span>
        <span class="n">hooks</span> <span class="o">=</span> <span class="p">[</span>
            <span class="p">{</span>
                <span class="s2">&quot;fwd_hooks&quot;</span><span class="p">:</span> <span class="p">[],</span> 
                <span class="s2">&quot;bwd_hooks&quot;</span><span class="p">:</span> <span class="p">[</span><span class="n">weight_grad_stats</span><span class="p">,</span> <span class="n">weight_update_ratio</span><span class="p">],</span> 
                <span class="s2">&quot;layer_types&quot;</span><span class="p">:</span> <span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">,</span> <span class="n">nn</span><span class="o">.</span><span class="n">Embedding</span><span class="p">)</span>
            <span class="p">},</span>
            <span class="p">{</span>
                <span class="s2">&quot;fwd_hooks&quot;</span><span class="p">:</span> <span class="p">[</span><span class="n">activation_stats</span><span class="p">],</span> 
                <span class="s2">&quot;bwd_hooks&quot;</span><span class="p">:</span> <span class="p">[],</span> 
                <span class="s2">&quot;layer_types&quot;</span><span class="p">:</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">activation_fns</span><span class="o">.</span><span class="n">values</span><span class="p">())</span>
            <span class="p">}</span>
        <span class="p">]</span>
        <span class="n">model</span> <span class="o">=</span> <span class="n">init_model</span><span class="p">(</span><span class="n">get_model</span><span class="p">(</span><span class="n">activation</span><span class="o">=</span><span class="n">act</span><span class="p">,</span> <span class="n">layernorm</span><span class="o">=</span><span class="n">use_ln</span><span class="p">,</span> <span class="o">**</span><span class="n">model_params</span><span class="p">),</span> <span class="n">init_fn</span><span class="p">)</span>
        <span class="n">trainer</span> <span class="o">=</span> <span class="n">run</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">train_dataset</span><span class="p">,</span> <span class="o">**</span><span class="n">train_params</span><span class="p">,</span> <span class="n">hooks</span><span class="o">=</span><span class="n">hooks</span><span class="p">)</span>

        <span class="n">k</span> <span class="o">=</span> <span class="s2">&quot;ln&quot;</span> <span class="k">if</span> <span class="n">use_ln</span> <span class="k">else</span> <span class="s2">&quot;un&quot;</span>
        <span class="n">trainers</span><span class="p">[</span><span class="n">k</span><span class="p">][</span><span class="n">act</span><span class="p">]</span> <span class="o">=</span> <span class="n">trainer</span>
        <span class="n">train_hooks</span><span class="p">[</span><span class="n">k</span><span class="p">][</span><span class="n">act</span><span class="p">]</span> <span class="o">=</span> <span class="p">{}</span> <span class="k">if</span> <span class="n">act</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">train_hooks</span><span class="p">[</span><span class="n">k</span><span class="p">]</span> <span class="k">else</span> <span class="s2">&quot;&quot;</span>
        <span class="n">train_hooks</span><span class="p">[</span><span class="n">k</span><span class="p">][</span><span class="n">act</span><span class="p">][</span><span class="s2">&quot;outs&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">activation_stats</span>
        <span class="n">train_hooks</span><span class="p">[</span><span class="n">k</span><span class="p">][</span><span class="n">act</span><span class="p">][</span><span class="s2">&quot;grad&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">weight_grad_stats</span>
        <span class="n">train_hooks</span><span class="p">[</span><span class="n">k</span><span class="p">][</span><span class="n">act</span><span class="p">][</span><span class="s2">&quot;rate&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">weight_update_ratio</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<script type="application/vnd.jupyter.widget-view+json">{"model_id": "b2d26788608d48c1a0fede2ccc8694c6", "version_major": 2, "version_minor": 0}</script><script type="application/vnd.jupyter.widget-view+json">{"model_id": "34bf3eab6c6d40fa9b66a2aa4cb75ab9", "version_major": 2, "version_minor": 0}</script><script type="application/vnd.jupyter.widget-view+json">{"model_id": "749035114ee74abdbc4eaa0492b1412d", "version_major": 2, "version_minor": 0}</script><script type="application/vnd.jupyter.widget-view+json">{"model_id": "56fabe2f9ebc4d6cbad9ce478bca26d6", "version_major": 2, "version_minor": 0}</script><script type="application/vnd.jupyter.widget-view+json">{"model_id": "a4ac15e5aa2a48ad8abcc548bd409a2b", "version_major": 2, "version_minor": 0}</script><script type="application/vnd.jupyter.widget-view+json">{"model_id": "0a71e6c497bb40e588c9da66b6d504ab", "version_major": 2, "version_minor": 0}</script><script type="application/vnd.jupyter.widget-view+json">{"model_id": "93138fb2eef649689679f991929c6d10", "version_major": 2, "version_minor": 0}</script><script type="application/vnd.jupyter.widget-view+json">{"model_id": "1ababc86b95d44f6b24a02b9df6abdf3", "version_major": 2, "version_minor": 0}</script></div>
</div>
<p>SELU does not train with large LR (can diverge even at <code class="docutils literal notranslate"><span class="pre">lr=0.3</span></code>) compared to other activations. Probably due to the spike in its derivative at zero. Tanh initial loss improves with LN consistent with the above discussion.</p>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">for</span> <span class="n">l</span><span class="p">,</span> <span class="n">ln</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">([</span><span class="s2">&quot;un&quot;</span><span class="p">,</span> <span class="s2">&quot;ln&quot;</span><span class="p">]):</span>
    <span class="n">message</span> <span class="o">=</span> <span class="s2">&quot;train loss (normalized)&quot;</span> <span class="k">if</span> <span class="n">ln</span> <span class="o">==</span> <span class="s2">&quot;ln&quot;</span> <span class="k">else</span> <span class="s2">&quot;train loss (unnormalized)&quot;</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">message</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">act</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">activation_fns</span><span class="p">):</span>
        <span class="n">trainer</span> <span class="o">=</span> <span class="n">trainers</span><span class="p">[</span><span class="n">ln</span><span class="p">][</span><span class="n">act</span><span class="p">]</span>
        <span class="n">train_loss</span> <span class="o">=</span> <span class="n">trainer</span><span class="o">.</span><span class="n">logs</span><span class="p">[</span><span class="s2">&quot;train&quot;</span><span class="p">][</span><span class="s2">&quot;loss&quot;</span><span class="p">]</span>
        <span class="n">start</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">train_loss</span><span class="p">[:</span><span class="mi">100</span><span class="p">])</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
        <span class="n">end</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">train_loss</span><span class="p">[</span><span class="o">-</span><span class="mi">100</span><span class="p">:])</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
        <span class="n">diff</span> <span class="o">=</span> <span class="n">end</span> <span class="o">-</span> <span class="n">start</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;    </span><span class="si">{</span><span class="n">act</span><span class="si">}</span><span class="s2">    start: </span><span class="si">{</span><span class="n">start</span><span class="si">:</span><span class="s2">.3f</span><span class="si">}</span><span class="s2">    end: </span><span class="si">{</span><span class="n">end</span><span class="si">:</span><span class="s2">.3f</span><span class="si">}</span><span class="s2">    diff: </span><span class="si">{</span><span class="n">diff</span><span class="si">:</span><span class="s2">.3f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">()</span>
</pre></div>
</div>
</div>
</details>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>train loss (unnormalized)
    tanh    start: 2.996    end: 2.501    diff: -0.495
    selu    start: 2.978    end: 2.500    diff: -0.478
    relu    start: 3.005    end: 2.521    diff: -0.484
    gelu    start: 3.015    end: 2.487    diff: -0.528

train loss (normalized)
    tanh    start: 2.968    end: 2.479    diff: -0.490
    selu    start: 2.919    end: 2.484    diff: -0.435
    relu    start: 3.002    end: 2.560    diff: -0.442
    gelu    start: 2.985    end: 2.493    diff: -0.491
</pre></div>
</div>
</div>
</div>
<br>
<p><strong>Histograms.</strong> Activation and gradient distributions at the last step of the hidden linear layers:</p>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">step</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span>
<span class="k">for</span> <span class="n">key</span> <span class="ow">in</span> <span class="p">[</span><span class="s2">&quot;outs&quot;</span><span class="p">,</span> <span class="s2">&quot;grad&quot;</span><span class="p">]:</span>
    <span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">activation_fns</span><span class="p">),</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>
    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">act</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">activation_fns</span><span class="o">.</span><span class="n">keys</span><span class="p">()):</span>
        <span class="n">h</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">ln</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">([</span><span class="s2">&quot;un&quot;</span><span class="p">,</span> <span class="s2">&quot;ln&quot;</span><span class="p">]):</span>
            <span class="n">records</span> <span class="o">=</span> <span class="n">train_hooks</span><span class="p">[</span><span class="n">ln</span><span class="p">][</span><span class="n">act</span><span class="p">][</span><span class="n">key</span><span class="p">]</span><span class="o">.</span><span class="n">records</span>
            <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
                <span class="k">for</span> <span class="n">j</span><span class="p">,</span> <span class="n">m</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">records</span><span class="o">.</span><span class="n">keys</span><span class="p">()):</span>
                    <span class="n">layer_types</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">activation_fns</span><span class="o">.</span><span class="n">values</span><span class="p">())</span> <span class="k">if</span> <span class="n">key</span> <span class="o">==</span> <span class="s2">&quot;outs&quot;</span> <span class="k">else</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span>
                    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">m</span><span class="p">,</span> <span class="n">layer_types</span><span class="p">)</span> <span class="ow">and</span> <span class="mi">0</span> <span class="o">&lt;</span> <span class="n">j</span> <span class="o">&lt;</span> <span class="nb">len</span><span class="p">(</span><span class="n">records</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span><span class="p">:</span>
                        <span class="n">data</span> <span class="o">=</span> <span class="n">records</span><span class="p">[</span><span class="n">m</span><span class="p">][</span><span class="n">step</span><span class="p">]</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
                        <span class="n">h</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="n">h</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">percentile</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="mi">99</span><span class="p">)))</span>
                        <span class="n">sns</span><span class="o">.</span><span class="n">distplot</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">bins</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">[</span><span class="n">k</span><span class="p">,</span> <span class="n">i</span><span class="p">],</span> <span class="n">label</span><span class="o">=</span><span class="sa">f</span><span class="s2">&quot;Linear.</span><span class="si">{</span><span class="n">j</span><span class="o">+</span><span class="mi">1</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">hist</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

            <span class="n">ax</span><span class="p">[</span><span class="n">k</span><span class="p">,</span> <span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlim</span><span class="p">(</span><span class="o">-</span><span class="n">h</span><span class="p">,</span> <span class="n">h</span><span class="p">)</span>
            <span class="n">ax</span><span class="p">[</span><span class="n">k</span><span class="p">,</span> <span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">act</span><span class="si">}</span><span class="s2"> (</span><span class="si">{</span><span class="s1">&#39;unnormalized&#39;</span><span class="w"> </span><span class="k">if</span><span class="w"> </span><span class="n">ln</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="s1">&#39;un&#39;</span><span class="w"> </span><span class="k">else</span><span class="w"> </span><span class="s1">&#39;layernorm&#39;</span><span class="si">}</span><span class="s2">)&quot;</span><span class="p">)</span>
            <span class="n">ax</span><span class="p">[</span><span class="n">k</span><span class="p">,</span> <span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">ticklabel_format</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="s2">&quot;x&quot;</span><span class="p">,</span> <span class="n">style</span><span class="o">=</span><span class="s2">&quot;sci&quot;</span><span class="p">,</span> <span class="n">scilimits</span><span class="o">=</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span>

    <span class="n">ax</span><span class="p">[</span><span class="n">k</span><span class="p">,</span> <span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">fontsize</span><span class="o">=</span><span class="mi">8</span><span class="p">)</span>
    <span class="n">fig</span><span class="o">.</span><span class="n">suptitle</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="s1">&#39;Activation&#39;</span><span class="w"> </span><span class="k">if</span><span class="w"> </span><span class="n">key</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="s1">&#39;outs&#39;</span><span class="w"> </span><span class="k">else</span><span class="w"> </span><span class="s1">&#39;Gradient&#39;</span><span class="si">}</span><span class="s2"> distribution at final step&quot;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">15</span><span class="p">)</span>
    <span class="n">fig</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
</pre></div>
</div>
</div>
</details>
<div class="cell_output docutils container">
<img alt="../../_images/eeacefb685281aa7b37c11f5817fb343c42f9a385d5ebb879b6b54757ef72421.svg" src="../../_images/eeacefb685281aa7b37c11f5817fb343c42f9a385d5ebb879b6b54757ef72421.svg" /><img alt="../../_images/2f203b423dc95192ff2cffe8c2a03bb4f3df95c3ad787bf595ea8896aafc31f4.svg" src="../../_images/2f203b423dc95192ff2cffe8c2a03bb4f3df95c3ad787bf595ea8896aafc31f4.svg" /></div>
</div>
<p>Observe that Tanh outputs have a small range compared to the other activations. ReLU and GELU have tails on the positive values, while SELU is more symmetric with a heavy tail in the negative axis. LN makes the distributions more similar across layers. The plot of percentiles per layer below allow us to quantify this better:</p>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">step</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span>
<span class="k">for</span> <span class="n">key</span> <span class="ow">in</span> <span class="p">[</span><span class="s2">&quot;outs&quot;</span><span class="p">,</span> <span class="s2">&quot;grad&quot;</span><span class="p">]:</span>
    <span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">activation_fns</span><span class="p">),</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>
    <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">ln</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">([</span><span class="s2">&quot;un&quot;</span><span class="p">,</span> <span class="s2">&quot;ln&quot;</span><span class="p">]):</span>
        <span class="n">alpha</span> <span class="o">=</span> <span class="mf">1.0</span> <span class="k">if</span> <span class="n">ln</span> <span class="o">==</span> <span class="s2">&quot;ln&quot;</span> <span class="k">else</span> <span class="mf">0.8</span>
        <span class="n">linestyle</span> <span class="o">=</span> <span class="s2">&quot;dotted&quot;</span> <span class="k">if</span> <span class="n">ln</span> <span class="o">==</span> <span class="s2">&quot;un&quot;</span> <span class="k">else</span> <span class="s2">&quot;solid&quot;</span>
        <span class="n">linewidth</span> <span class="o">=</span> <span class="mf">2.0</span> <span class="k">if</span> <span class="n">ln</span> <span class="o">==</span> <span class="s2">&quot;ln&quot;</span> <span class="k">else</span> <span class="mf">1.5</span>
        <span class="n">label</span> <span class="o">=</span> <span class="s2">&quot;LN&quot;</span> <span class="k">if</span> <span class="n">ln</span> <span class="o">==</span> <span class="s2">&quot;ln&quot;</span> <span class="k">else</span> <span class="s2">&quot;un&quot;</span>
        <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">act</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">activation_fns</span><span class="o">.</span><span class="n">keys</span><span class="p">()):</span>
            <span class="n">s</span> <span class="o">=</span> <span class="p">[]</span>
            <span class="n">u</span> <span class="o">=</span> <span class="p">[]</span>
            <span class="n">t</span> <span class="o">=</span> <span class="p">[]</span>
            <span class="n">records</span> <span class="o">=</span> <span class="n">train_hooks</span><span class="p">[</span><span class="n">ln</span><span class="p">][</span><span class="n">act</span><span class="p">][</span><span class="n">key</span><span class="p">]</span><span class="o">.</span><span class="n">records</span>
            <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
                <span class="k">for</span> <span class="n">j</span><span class="p">,</span> <span class="n">m</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">records</span><span class="o">.</span><span class="n">keys</span><span class="p">()):</span>
                    <span class="n">layer_types</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">activation_fns</span><span class="o">.</span><span class="n">values</span><span class="p">())</span> <span class="k">if</span> <span class="n">key</span> <span class="o">==</span> <span class="s2">&quot;outs&quot;</span> <span class="k">else</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span>
                    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">m</span><span class="p">,</span> <span class="n">layer_types</span><span class="p">)</span> <span class="ow">and</span> <span class="mi">0</span> <span class="o">&lt;</span> <span class="n">j</span> <span class="o">&lt;</span> <span class="nb">len</span><span class="p">(</span><span class="n">records</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span><span class="p">:</span>
                        <span class="n">data</span> <span class="o">=</span> <span class="n">records</span><span class="p">[</span><span class="n">m</span><span class="p">][</span><span class="n">step</span><span class="p">]</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
                        <span class="n">s</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">percentile</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="mi">16</span><span class="p">))</span>
                        <span class="n">u</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">percentile</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="mi">50</span><span class="p">))</span>
                        <span class="n">t</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">percentile</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="mi">84</span><span class="p">))</span>

            <span class="n">ax</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">s</span><span class="p">))</span> <span class="o">+</span> <span class="mi">2</span><span class="p">,</span> <span class="n">s</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="sa">f</span><span class="s2">&quot;C</span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="n">alpha</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="n">linewidth</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="n">linestyle</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="n">label</span><span class="p">)</span>
            <span class="n">ax</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">s</span><span class="p">))</span> <span class="o">+</span> <span class="mi">2</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="sa">f</span><span class="s2">&quot;C</span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="n">alpha</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="n">linewidth</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="n">linestyle</span><span class="p">)</span>
            <span class="n">ax</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">s</span><span class="p">))</span> <span class="o">+</span> <span class="mi">2</span><span class="p">,</span> <span class="n">u</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;black&quot;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="n">alpha</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="n">linewidth</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="n">linestyle</span><span class="p">)</span>
            <span class="n">ax</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">fill_between</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">s</span><span class="p">))</span> <span class="o">+</span> <span class="mi">2</span><span class="p">,</span> <span class="n">s</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="sa">f</span><span class="s2">&quot;C</span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.2</span><span class="p">)</span>

            <span class="n">ax</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="n">act</span><span class="p">)</span>
            <span class="n">ax</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s2">&quot;layer&quot;</span><span class="p">)</span>
            <span class="n">ax</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="sa">r</span><span class="s2">&quot;$\sigma$&quot;</span><span class="p">)</span>
            <span class="n">ylim</span> <span class="o">=</span> <span class="p">(</span><span class="o">-</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span> <span class="k">if</span> <span class="n">key</span> <span class="o">==</span> <span class="s2">&quot;outs&quot;</span> <span class="k">else</span> <span class="p">(</span><span class="o">-</span><span class="mf">0.05</span><span class="p">,</span> <span class="mf">0.05</span><span class="p">)</span>
            <span class="n">ax</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylim</span><span class="p">(</span><span class="o">*</span><span class="n">ylim</span><span class="p">)</span>
            <span class="n">ax</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="n">alpha</span><span class="o">=</span><span class="mf">0.4</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s2">&quot;dashed&quot;</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mf">0.6</span><span class="p">)</span>
            <span class="n">ax</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">fontsize</span><span class="o">=</span><span class="mi">8</span><span class="p">)</span>
            <span class="n">ax</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">set_xticks</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">s</span><span class="p">))</span> <span class="o">+</span> <span class="mi">2</span><span class="p">)</span>

    <span class="n">fig</span><span class="o">.</span><span class="n">suptitle</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="s1">&#39;Activation&#39;</span><span class="w"> </span><span class="k">if</span><span class="w"> </span><span class="n">key</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="s1">&#39;outs&#39;</span><span class="w"> </span><span class="k">else</span><span class="w"> </span><span class="s1">&#39;Weight gradient&#39;</span><span class="si">}</span><span class="s2"> distribution at final step&quot;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">15</span><span class="p">)</span>
    <span class="n">fig</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
</pre></div>
</div>
</div>
</details>
<div class="cell_output docutils container">
<img alt="../../_images/68e73d51ce0b7805dbb35f40ca66f29ab6f668caea2208b4a67385464fabd6ea.svg" src="../../_images/68e73d51ce0b7805dbb35f40ca66f29ab6f668caea2208b4a67385464fabd6ea.svg" /><img alt="../../_images/2503c78ab45e50a6f41da6791ab2b0a4531f6c3fe43dab1478eba4023b41a108.svg" src="../../_images/2503c78ab45e50a6f41da6791ab2b0a4531f6c3fe43dab1478eba4023b41a108.svg" /></div>
</div>
<p>ReLU and GELU have decreasing gradients while Tanh and SELU have increasing gradients.
In general, the propagation of weight gradients seem more constant across layers with LN.
Next, observe that SELU (and GELU to a lower degree) which are not symmetric have shifting median activations.</p>
<p>This induces a <strong>bias shift</strong> such that inputs <span class="math notranslate nohighlight">\(\boldsymbol{\mathsf{x}}\)</span> of a layer tend to have the same sign (over units). Recall <span class="math notranslate nohighlight">\(\frac{\partial\mathcal{L}}{\partial {\mathsf{w}}_{ij}} = {\mathsf{x}}_i \frac{\partial\mathcal{L}}{\partial {\mathsf{z}}_j},\)</span> so that <span class="math notranslate nohighlight">\(\frac{\partial\mathcal{L}}{\partial {\mathsf{w}}_{ij}}\)</span> will have the same sign for all <span class="math notranslate nohighlight">\(i,\)</span> which can slow down learning. Also, if you have two independent random vectors as input, the inner product of its outputs tends to become large and positive:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">n</span> <span class="o">=</span> <span class="mi">1000</span>
<span class="n">w</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">20</span><span class="p">)</span>
<span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="mi">20</span><span class="p">)</span> 
<span class="n">b</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="mi">20</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(((</span><span class="n">a</span> <span class="o">@</span> <span class="n">w</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="n">b</span> <span class="o">@</span> <span class="n">w</span><span class="p">))</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">abs</span><span class="p">()</span><span class="o">.</span><span class="n">mean</span><span class="p">())</span>
<span class="nb">print</span><span class="p">((((</span><span class="n">a</span> <span class="o">+</span> <span class="mf">0.2</span><span class="p">)</span> <span class="o">@</span> <span class="n">w</span><span class="p">)</span> <span class="o">*</span> <span class="p">((</span><span class="n">b</span> <span class="o">+</span> <span class="mf">0.2</span><span class="p">)</span> <span class="o">@</span> <span class="n">w</span><span class="p">))</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">abs</span><span class="p">()</span><span class="o">.</span><span class="n">mean</span><span class="p">())</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor(107.3766)
tensor(110.3988)
</pre></div>
</div>
</div>
</div>
<p>This effect compounds with depth. Hence, deep networks affected by bias shift tend to predict the same label for all training examples (e.g. at initialization). Bias shift is counteracted by LN, ensuring that the mean activation on each channel is zero across the layer.</p>
<br>
<p><strong>Weight update ratio.</strong> LN improves separation and tightness of update ratio curves. SELU looks ideal (e.g. less fluctuations, tightness across hidden layers of same shape) these properties seem to be correlated with better loss. The significant improvement for the Tanh and SELU curves may explain why their performance improved with LN.</p>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="n">Literal</span>

<span class="k">def</span> <span class="nf">plot_weight_update_ratio</span><span class="p">(</span><span class="n">key</span><span class="p">:</span> <span class="n">Literal</span><span class="p">[</span><span class="s2">&quot;un&quot;</span><span class="p">,</span> <span class="s2">&quot;ln&quot;</span><span class="p">],</span> <span class="n">ylim</span><span class="o">=</span><span class="p">(</span><span class="o">-</span><span class="mf">3.0</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.5</span><span class="p">),</span> <span class="n">legend</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
    <span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">activation_fns</span><span class="p">),</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>
    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">act</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">activation_fns</span><span class="p">):</span>
        <span class="n">records</span> <span class="o">=</span> <span class="n">train_hooks</span><span class="p">[</span><span class="n">key</span><span class="p">][</span><span class="n">act</span><span class="p">][</span><span class="s2">&quot;rate&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">records</span>
        <span class="n">title</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">act</span><span class="si">}</span><span class="s2"> (normalized)&quot;</span> <span class="k">if</span> <span class="n">key</span> <span class="o">==</span> <span class="s2">&quot;ln&quot;</span> <span class="k">else</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">act</span><span class="si">}</span><span class="s2"> (unnormalized)&quot;</span>
        <span class="k">for</span> <span class="n">j</span><span class="p">,</span> <span class="n">m</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">records</span><span class="o">.</span><span class="n">keys</span><span class="p">()):</span>
            <span class="n">ax</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">moving_avg</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">log10</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">records</span><span class="p">[</span><span class="n">m</span><span class="p">])),</span> <span class="n">window</span><span class="o">=</span><span class="mf">0.05</span><span class="p">),</span> <span class="n">label</span><span class="o">=</span><span class="n">m</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span> <span class="o">+</span> <span class="s2">&quot;.&quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">records</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">j</span><span class="p">))</span>
            <span class="n">ax</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="n">linestyle</span><span class="o">=</span><span class="s2">&quot;dotted&quot;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.6</span><span class="p">)</span>
            <span class="n">ax</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="n">title</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">12</span><span class="p">)</span>
            <span class="n">ax</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="sa">r</span><span class="s2">&quot;$\log(\lambda \| \nabla \mathbf</span><span class="si">{w}</span><span class="s2"> \|\; / \;\| \mathsf{\mathbf</span><span class="si">{w}</span><span class="s2">} \|)$&quot;</span><span class="p">)</span>
            <span class="n">ax</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s2">&quot;steps&quot;</span><span class="p">)</span>
            <span class="n">ax</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylim</span><span class="p">(</span><span class="o">*</span><span class="n">ylim</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">legend</span><span class="p">:</span>
        <span class="n">ax</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">fontsize</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span> <span class="n">loc</span><span class="o">=</span><span class="s2">&quot;lower right&quot;</span><span class="p">)</span>
        
    <span class="n">fig</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>


<span class="n">plot_weight_update_ratio</span><span class="p">(</span><span class="s2">&quot;un&quot;</span><span class="p">,</span> <span class="n">ylim</span><span class="o">=</span><span class="p">(</span><span class="o">-</span><span class="mf">4.0</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">))</span>
<span class="n">plot_weight_update_ratio</span><span class="p">(</span><span class="s2">&quot;ln&quot;</span><span class="p">,</span> <span class="n">ylim</span><span class="o">=</span><span class="p">(</span><span class="o">-</span><span class="mf">4.0</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">),</span> <span class="n">legend</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
</div>
</details>
<div class="cell_output docutils container">
<img alt="../../_images/464fbdda3a80726b1c8ae7f3107381e069ab3b23e0065535dbd76c2ef3b5248d.svg" src="../../_images/464fbdda3a80726b1c8ae7f3107381e069ab3b23e0065535dbd76c2ef3b5248d.svg" /><img alt="../../_images/f30af00473f83eb28ec6a6fc8a7d1b4e44112a232118b1706f58557d26805c03.svg" src="../../_images/f30af00473f83eb28ec6a6fc8a7d1b4e44112a232118b1706f58557d26805c03.svg" /></div>
</div>
</section>
<section id="appendix-rank-collapse">
<h2>Appendix: Rank collapse<a class="headerlink" href="#appendix-rank-collapse" title="Link to this heading">#</a></h2>
<p>Rank collapse happens when outputs of a layer lie in a small subspace of its output space. Xavier initialization by itself cannot prevent rank collapse. In fact, the difference between first and singular values of a product of Gaussian matrices sampled with Xavier initialization grows exponentially with depth (ยง3.2 of <span id="id12">[<a class="reference internal" href="../../intro.html#id43" title="Hadi Daneshmand, Jonas Kohler, Francis Bach, Thomas Hofmann, and Aurelien Lucchi. Batch normalization provably avoids rank collapse for randomly initialised deep networks. 2020. arXiv:2003.01652.">DKB+20</a>]</span>). Activations help mitigate rank collapse by adding non-linearities in between weight matrices.</p>
<p>One way to measure rank numerically is by using <strong>singular values</strong>. Every matrix <span class="math notranslate nohighlight">\(\boldsymbol{\mathsf{A}}\)</span> has a <a class="reference external" href="https://en.wikipedia.org/wiki/Singular_value_decomposition">singular value decomposition</a> (SVD) <span class="math notranslate nohighlight">\(\boldsymbol{\mathsf{A}} = \boldsymbol{\mathsf{U}} \boldsymbol{\Sigma} \boldsymbol{\mathsf{V}}^\top\)</span> where <span class="math notranslate nohighlight">\(\boldsymbol{\mathsf{U}}\)</span> and <span class="math notranslate nohighlight">\(\boldsymbol{\mathsf{V}}\)</span> are orthogonal matrices whose columns form an orthonormal basis on the output and input spaces, respectively. And <span class="math notranslate nohighlight">\(\boldsymbol{\Sigma}\)</span> is a diagonal matrix containing singular values of <span class="math notranslate nohighlight">\(\boldsymbol{\mathsf{A}}.\)</span> It turns out that the number of nonzero singular values of a matrix is equal to its rank. In particular, the first singular value <span class="math notranslate nohighlight">\(\sigma_1\)</span> is equal to the <a class="reference external" href="https://en.wikipedia.org/wiki/Operator_norm">operator norm</a> which is roughly:</p>
<div class="math notranslate nohighlight">
\[\sigma_1 = \max_{\lVert \boldsymbol{\mathsf{x}} \rVert = 1}\; \lVert \boldsymbol{\mathsf{A}} \boldsymbol{\mathsf{x}} \rVert.\]</div>
<p>Geometrically, this corresponds to the width of the output ellipsoid of a unit sphere in the input space transformed by <span class="math notranslate nohighlight">\(\boldsymbol{\mathsf{A}}.\)</span> The other singular values correspond to lengths of the other axes of the ellipsoid. For example, if only two singular values are nonzero, then the output ellipsoid, and hence the entire output space, is embedded in two dimensions.</p>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">rng</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">RandomState</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="n">A</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="n">u</span><span class="p">,</span> <span class="n">s</span><span class="p">,</span> <span class="n">vT</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">svd</span><span class="p">(</span><span class="n">A</span><span class="p">)</span>

<span class="n">N</span> <span class="o">=</span> <span class="mi">100</span>
<span class="n">t</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">N</span><span class="p">)</span>
<span class="n">unit_circle</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">stack</span><span class="p">([</span><span class="n">np</span><span class="o">.</span><span class="n">cos</span><span class="p">(</span><span class="mi">2</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">pi</span><span class="o">*</span><span class="n">t</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="mi">2</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">pi</span><span class="o">*</span><span class="n">t</span><span class="p">)],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">outputs</span> <span class="o">=</span> <span class="n">A</span> <span class="o">@</span> <span class="n">unit_circle</span>

<span class="c1"># Plot</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">unit_circle</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="p">:],</span> <span class="n">unit_circle</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="p">:],</span> <span class="n">s</span><span class="o">=</span><span class="mf">0.8</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;x&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">outputs</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="p">:],</span> <span class="n">outputs</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="p">:],</span> <span class="n">s</span><span class="o">=</span><span class="mf">0.8</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;Ax&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span><span class="s1">&#39;scaled&#39;</span><span class="p">)</span>

<span class="c1"># Checking: max norm == s[0]</span>
<span class="n">max_output_norm</span>  <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">max</span><span class="p">()</span>
<span class="n">min_output_norm</span>  <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">min</span><span class="p">()</span>
<span class="n">max_output_norm_</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">argmax</span><span class="p">()</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;A=&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">A</span><span class="p">)</span>
<span class="nb">print</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;max โAxโ = </span><span class="si">{</span><span class="n">max_output_norm</span><span class="si">:</span><span class="s1">.2f</span><span class="si">}</span><span class="s1">  (โxโ = 1)&#39;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;min โAxโ = </span><span class="si">{</span><span class="n">min_output_norm</span><span class="si">:</span><span class="s1">.2f</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;ฯโ = </span><span class="si">{</span><span class="n">s</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="si">:</span><span class="s1">.2f</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;ฯโ = </span><span class="si">{</span><span class="n">s</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="si">:</span><span class="s1">.2f</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>


<span class="c1"># Plotting singular vectors as axis of ellipse</span>
<span class="n">plt</span><span class="o">.</span><span class="n">arrow</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">s</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">*</span><span class="n">u</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">s</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">*</span><span class="n">u</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">width</span><span class="o">=</span><span class="mf">0.01</span><span class="p">,</span> <span class="n">length_includes_head</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">arrow</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">s</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">*</span><span class="n">u</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">s</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">*</span><span class="n">u</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">width</span><span class="o">=</span><span class="mf">0.01</span><span class="p">,</span> <span class="n">length_includes_head</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;x&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;y&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">(</span><span class="o">-</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">(</span><span class="o">-</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">savefig</span><span class="p">(</span><span class="s1">&#39;../../img/nn/05-singular-ellipsoid.svg&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">close</span><span class="p">()</span>
</pre></div>
</div>
</div>
</details>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>A=
[[1.76405235 0.40015721]
 [0.97873798 2.2408932 ]]

max โAxโ = 2.75  (โxโ = 1)
min โAxโ = 1.29
ฯโ = 2.75
ฯโ = 1.29
</pre></div>
</div>
</div>
</div>
<figure class="align-default" id="id22">
<img alt="../../_images/singular-ellipsoid.svg" src="../../_images/singular-ellipsoid.svg" /><figcaption>
<p><span class="caption-number">Fig. 51 </span><span class="caption-text">Singular values of a matrix <span class="math notranslate nohighlight">\(\boldsymbol{\mathsf{A}}\)</span> is equal the length of each axis of the output ellipsoid for an input unit sphere. The arrows in the ellipsoid are obtained using left singular vectors: <span class="math notranslate nohighlight">\(\sigma_1\, \boldsymbol{\mathsf{u}}_1\)</span> and <span class="math notranslate nohighlight">\(\sigma_2\, \boldsymbol{\mathsf{u}}_2.\)</span></span><a class="headerlink" href="#id22" title="Link to this image">#</a></p>
</figcaption>
</figure>
<p><strong>Batch normalization.</strong> Below we test rank collapse with LN and <strong>batch normalization</strong> (BN) <span id="id13">[<a class="reference internal" href="../../intro.html#id27" title="Sergey Ioffe and Christian Szegedy. Batch normalization: accelerating deep network training by reducing internal covariate shift. CoRR, 2015. URL: http://arxiv.org/abs/1502.03167, arXiv:1502.03167.">IS15</a>]</span>. BN is similar to LN except that the normalization occurs in the batch dimension. As such, BN has different prediction algorithms for training and inference since we have to <a class="reference external" href="https://en.wikipedia.org/wiki/Batch_normalization#Inference">estimate batch statistics</a> when making predictions with single instances at test time. This complicates using BN in practice (also see <span id="id14">[<a class="reference internal" href="../../intro.html#id65" title="Yuxin Wu and Justin Johnson. Rethinking &quot;batch&quot; in batchnorm. 2021. URL: https://arxiv.org/abs/2105.07576, doi:10.48550/ARXIV.2105.07576.">WJ21</a>]</span>). However, in addition to having the same benefits as LN, BN avoids rank collapse <span id="id15">[<a class="reference internal" href="../../intro.html#id43" title="Hadi Daneshmand, Jonas Kohler, Francis Bach, Thomas Hofmann, and Aurelien Lucchi. Batch normalization provably avoids rank collapse for randomly initialised deep networks. 2020. arXiv:2003.01652.">DKB+20</a>]</span> which will be shown in the following experiment.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">get_model</span><span class="p">(</span><span class="n">vocab_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
              <span class="n">block_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
              <span class="n">emb_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
              <span class="n">width</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
              <span class="n">num_linear</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
              <span class="n">activation</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;relu&quot;</span><span class="p">,</span>
              <span class="n">norm</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="kc">None</span><span class="p">):</span>

    <span class="n">norm_layer</span> <span class="o">=</span> <span class="p">{</span>
        <span class="s2">&quot;layernorm&quot;</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">LayerNorm</span><span class="p">,</span>
        <span class="s2">&quot;batchnorm&quot;</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm1d</span>
    <span class="p">}[</span><span class="n">norm</span><span class="p">]</span> <span class="k">if</span> <span class="n">norm</span> <span class="k">else</span> <span class="kc">None</span>

    <span class="n">act_fn</span> <span class="o">=</span> <span class="n">activation_fns</span><span class="p">[</span><span class="n">activation</span><span class="p">]</span>
    <span class="n">layers</span> <span class="o">=</span> <span class="p">[</span>
        <span class="n">nn</span><span class="o">.</span><span class="n">Embedding</span><span class="p">(</span><span class="n">vocab_size</span><span class="p">,</span> <span class="n">emb_size</span><span class="p">),</span> 
        <span class="n">nn</span><span class="o">.</span><span class="n">Flatten</span><span class="p">(),</span>
        <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">block_size</span> <span class="o">*</span> <span class="n">emb_size</span><span class="p">,</span> <span class="n">width</span><span class="p">)</span>
    <span class="p">]</span>
    <span class="n">layers</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">norm_layer</span><span class="p">(</span><span class="n">width</span><span class="p">))</span> <span class="k">if</span> <span class="n">norm</span> <span class="k">else</span> <span class="s2">&quot;&quot;</span>
    <span class="n">layers</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">act_fn</span><span class="p">())</span>
    
    <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_linear</span><span class="p">):</span>
        <span class="n">layers</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">width</span><span class="p">,</span> <span class="n">width</span><span class="p">))</span>
        <span class="n">layers</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">norm_layer</span><span class="p">(</span><span class="n">width</span><span class="p">))</span> <span class="k">if</span> <span class="n">norm</span> <span class="k">else</span> <span class="s2">&quot;&quot;</span>
        <span class="n">layers</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">act_fn</span><span class="p">())</span>

    <span class="n">layers</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">width</span><span class="p">,</span> <span class="n">vocab_size</span><span class="p">))</span>
    <span class="n">layers</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">norm_layer</span><span class="p">(</span><span class="n">vocab_size</span><span class="p">))</span> <span class="k">if</span> <span class="n">norm</span> <span class="k">else</span> <span class="s2">&quot;&quot;</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span><span class="o">*</span><span class="n">layers</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">model</span>
</pre></div>
</div>
</div>
</div>
<p>Hook for calculating singular values:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">SingularValuesRatio</span><span class="p">(</span><span class="n">HookHandler</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">hook_fn</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">module</span><span class="p">,</span> <span class="nb">input</span><span class="p">,</span> <span class="n">output</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">records</span><span class="p">[</span><span class="n">module</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">records</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="p">[])</span>
        <span class="n">s</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">svd</span><span class="p">(</span><span class="n">output</span><span class="p">)[</span><span class="mi">1</span><span class="p">]</span> <span class="c1"># singular values</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">records</span><span class="p">[</span><span class="n">module</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">((</span><span class="n">s</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">/</span> <span class="n">s</span><span class="o">.</span><span class="n">sum</span><span class="p">())</span><span class="o">.</span><span class="n">item</span><span class="p">())</span>
</pre></div>
</div>
</div>
</div>
<p>Here we are careful not to use large learning rate. Dying units causes rank collapse which may lead us to incorrectly conclude that LN mitigates rank collapse (since LN prevents dying units). Observe that final loss correlates with degree of rank collapse:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Note: act = relu. But same results for tanh and gelu.</span>
<span class="n">sv_hooks</span> <span class="o">=</span> <span class="p">{}</span>
<span class="n">message</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">loss (last 100 steps)&quot;</span><span class="p">]</span>
<span class="n">model_params</span><span class="p">[</span><span class="s2">&quot;num_linear&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="mi">6</span>
<span class="n">train_params</span><span class="p">[</span><span class="s2">&quot;lr&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="mf">0.3</span>
<span class="n">train_params</span><span class="p">[</span><span class="s2">&quot;steps&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="mi">3000</span>

<span class="k">for</span> <span class="n">norm</span> <span class="ow">in</span> <span class="p">[</span><span class="s2">&quot;batchnorm&quot;</span><span class="p">,</span> <span class="s2">&quot;layernorm&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">]:</span>
    
    <span class="n">sv_ratio</span> <span class="o">=</span> <span class="n">SingularValuesRatio</span><span class="p">()</span>
    <span class="n">init_fn</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">w</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">xavier_uniform_</span><span class="p">(</span><span class="n">w</span><span class="p">,</span> <span class="n">gain</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="mi">2</span><span class="p">),</span> <span class="n">generator</span><span class="o">=</span><span class="n">g</span><span class="p">())</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">init_model</span><span class="p">(</span><span class="n">get_model</span><span class="p">(</span><span class="n">norm</span><span class="o">=</span><span class="n">norm</span><span class="p">,</span> <span class="o">**</span><span class="n">model_params</span><span class="p">),</span> <span class="n">init_fn</span><span class="p">)</span>
    <span class="n">hooks</span> <span class="o">=</span> <span class="p">[{</span><span class="s2">&quot;bwd_hooks&quot;</span><span class="p">:</span> <span class="p">[],</span> <span class="s2">&quot;fwd_hooks&quot;</span><span class="p">:</span> <span class="p">[</span><span class="n">sv_ratio</span><span class="p">],</span> <span class="s2">&quot;layer_types&quot;</span><span class="p">:</span> <span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">,)}]</span>
    <span class="n">trainer</span> <span class="o">=</span> <span class="n">run</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">train_dataset</span><span class="p">,</span> <span class="o">**</span><span class="n">train_params</span><span class="p">,</span> <span class="n">hooks</span><span class="o">=</span><span class="n">hooks</span><span class="p">)</span>

    <span class="n">sv_hooks</span><span class="p">[</span><span class="n">norm</span><span class="p">]</span> <span class="o">=</span> <span class="n">sv_ratio</span>
    <span class="n">message</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;    </span><span class="si">{</span><span class="n">norm</span><span class="si">}</span><span class="s2">: </span><span class="se">\t</span><span class="si">{</span><span class="nb">sum</span><span class="p">(</span><span class="n">trainer</span><span class="o">.</span><span class="n">logs</span><span class="p">[</span><span class="s1">&#39;train&#39;</span><span class="p">][</span><span class="s1">&#39;loss&#39;</span><span class="p">][</span><span class="o">-</span><span class="mi">100</span><span class="p">:])</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="mi">100</span><span class="si">:</span><span class="s2">.3f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">message</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<script type="application/vnd.jupyter.widget-view+json">{"model_id": "e1baa4dce0c643e8a17fbe79b47b8677", "version_major": 2, "version_minor": 0}</script><script type="application/vnd.jupyter.widget-view+json">{"model_id": "3611d77d46344b40a79aa9d58aaaf36d", "version_major": 2, "version_minor": 0}</script><script type="application/vnd.jupyter.widget-view+json">{"model_id": "2d3688515ac644e5a4960ed7829f242e", "version_major": 2, "version_minor": 0}</script><div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>loss (last 100 steps)
    batchnorm: 	2.471
    layernorm: 	2.493
    None: 	2.500
</pre></div>
</div>
</div>
</div>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">sv_hooks</span><span class="p">),</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span>
<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">norm</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">sv_hooks</span><span class="p">):</span>
    <span class="n">records</span> <span class="o">=</span> <span class="n">sv_hooks</span><span class="p">[</span><span class="n">norm</span><span class="p">]</span><span class="o">.</span><span class="n">records</span>
    <span class="k">for</span> <span class="n">j</span><span class="p">,</span> <span class="n">m</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">records</span><span class="o">.</span><span class="n">keys</span><span class="p">()):</span>
        <span class="n">ax</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">moving_avg</span><span class="p">(</span><span class="n">records</span><span class="p">[</span><span class="n">m</span><span class="p">],</span> <span class="n">window</span><span class="o">=</span><span class="mf">0.0005</span><span class="p">),</span> <span class="n">label</span><span class="o">=</span><span class="n">m</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span> <span class="o">+</span> <span class="s2">&quot;.&quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">j</span><span class="p">),</span> <span class="n">color</span><span class="o">=</span><span class="sa">f</span><span class="s2">&quot;C</span><span class="si">{</span><span class="n">j</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="nb">min</span><span class="p">(</span><span class="mf">0.5</span> <span class="o">+</span> <span class="p">(</span><span class="mf">0.5</span> <span class="o">/</span> <span class="nb">len</span><span class="p">(</span><span class="n">records</span><span class="o">.</span><span class="n">keys</span><span class="p">()))</span> <span class="o">*</span> <span class="n">j</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">))</span>
        <span class="n">ax</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylim</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
        <span class="n">ax</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="n">linestyle</span><span class="o">=</span><span class="s2">&quot;dashed&quot;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>
        <span class="n">ax</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s2">&quot;steps&quot;</span><span class="p">)</span>
        <span class="n">ax</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="n">norm</span><span class="p">)</span>

<span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="sa">r</span><span class="s2">&quot;${\sigma}_1\; /\; {\sigma}$.sum()&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;Unnormalized&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">fontsize</span><span class="o">=</span><span class="mi">7</span><span class="p">)</span>
<span class="n">fig</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
</pre></div>
</div>
</div>
</details>
<div class="cell_output docutils container">
<img alt="../../_images/9dd6fe3b0dfc80179db3b36afaa69e014d296c39c81b9fefafc26ba76c216f90.svg" src="../../_images/9dd6fe3b0dfc80179db3b36afaa69e014d296c39c81b9fefafc26ba76c216f90.svg" /></div>
</div>
<p><strong>Remark.</strong> Degree of rank collapse increases with depth. This makes sense for linear networks since <span class="math notranslate nohighlight">\(\text{rank}\,\boldsymbol{\mathsf{A}}\boldsymbol{\mathsf{X}} \leq \text{rank}\,\boldsymbol{\mathsf{X}}\)</span> (see eqn. (8) in <span id="id16">[<a class="reference internal" href="../../intro.html#id106" title="Ruili Feng, Kecheng Zheng, Yukun Huang, Deli Zhao, Michael Jordan, and Zheng-Jun Zha. Rank diminishing in deep neural networks. 2022. arXiv:2206.06072.">FZH+22</a>]</span> for general networks).
LN does not fully diminish rank collapse. In the next section, we will look at <strong>residual connections</strong> which help to mitigate rank collapse without having to use BN.</p>
</section>
<section id="appendix-residual-connections">
<h2>Appendix: Residual connections<a class="headerlink" href="#appendix-residual-connections" title="Link to this heading">#</a></h2>
<p>Recall that as networks become larger, it becomes harder to train.
In part, this is because more parameters require more data to train, hence slower training loops.
But independent of the dataset, we know that depth can be an obstacle to training.
Since each layer is initialized with random weights,
the input effectively becomes random noise by the time it gets to last layer. Similarly, during backward pass, the initial gradient becomes random noise once it reaches the early layers. This slows down training.</p>
<p>We would like for more meaningful data to arrive at deeper layers, similarly for gradients at
early layers. Both of these goals are achieved by using <strong>residual connections</strong>. Instead of modeling
<span class="math notranslate nohighlight">\(\boldsymbol{\mathsf{x}}^{\ell+1} = H^\ell(\boldsymbol{\mathsf{x}}^{\ell})\)</span>, we model <span class="math notranslate nohighlight">\(\boldsymbol{\mathsf{x}}^{\ell+1} = \boldsymbol{\mathsf{x}}^{\ell} + H^\ell(\boldsymbol{\mathsf{x}}^{\ell})\)</span>
where <span class="math notranslate nohighlight">\(H_\ell\)</span> is a non-linear mapping usually a sequence of NN modules like convolutions, activation functions, and normalizations (<a class="reference internal" href="#residual-block"><span class="std std-numref">Fig. 52</span></a>).
The layer essentially figures out how to augment the input instead of learning the signal from scratch. At initialization, when the weights are close to zero, the layer is close to the identity function instead of a random transformation.</p>
<figure class="align-default" id="residual-block">
<a class="reference internal image-reference" href="../../_images/05-residual-block.png"><img alt="../../_images/05-residual-block.png" src="../../_images/05-residual-block.png" style="width: 80%;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 52 </span><span class="caption-text">Residual blocks may require โprojectionsโ such as 1 ร 1 convs or linear layers to match shapes when combining.</span><a class="headerlink" href="#residual-block" title="Link to this image">#</a></p>
</figcaption>
</figure>
<p>Modifying our architecture to allow for skip connections (but still include the default model):</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">ResidualConnection</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">n</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">skip</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">linear</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">n</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">ln</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">LayerNorm</span><span class="p">(</span><span class="n">n</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">act_fn</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">relu</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">skip</span> <span class="o">=</span> <span class="n">skip</span>
    
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">hx</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">linear</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">act_fn</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">ln</span><span class="p">(</span><span class="n">x</span><span class="p">)))</span>
        <span class="k">return</span> <span class="n">x</span> <span class="o">+</span> <span class="n">hx</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">skip</span> <span class="k">else</span> <span class="n">hx</span>


<span class="k">def</span> <span class="nf">get_model</span><span class="p">(</span><span class="n">vocab_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
              <span class="n">block_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
              <span class="n">emb_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
              <span class="n">width</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
              <span class="n">num_linear</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
              <span class="n">skip</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">):</span>

    <span class="n">layers</span> <span class="o">=</span> <span class="p">[</span>
        <span class="n">nn</span><span class="o">.</span><span class="n">Embedding</span><span class="p">(</span><span class="n">vocab_size</span><span class="p">,</span> <span class="n">emb_size</span><span class="p">),</span> 
        <span class="n">nn</span><span class="o">.</span><span class="n">Flatten</span><span class="p">(),</span>
        <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">block_size</span> <span class="o">*</span> <span class="n">emb_size</span><span class="p">,</span> <span class="n">width</span><span class="p">),</span>
        <span class="n">ResidualConnection</span><span class="p">(</span><span class="n">width</span><span class="p">,</span> <span class="n">skip</span><span class="p">)</span>
    <span class="p">]</span>
    
    <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_linear</span><span class="p">):</span>
        <span class="n">layers</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">ResidualConnection</span><span class="p">(</span><span class="n">width</span><span class="p">,</span> <span class="n">skip</span><span class="p">))</span>

    <span class="n">layers</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">LayerNorm</span><span class="p">(</span><span class="n">width</span><span class="p">))</span>
    <span class="n">layers</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">())</span>
    <span class="n">layers</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">width</span><span class="p">,</span> <span class="n">vocab_size</span><span class="p">))</span>
    <span class="n">layers</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">LayerNorm</span><span class="p">(</span><span class="n">vocab_size</span><span class="p">))</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span><span class="o">*</span><span class="n">layers</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">model</span>
</pre></div>
</div>
</div>
</div>
<p><strong>Remark.</strong> This iteratively adds <code class="docutils literal notranslate"><span class="pre">[LN,</span> <span class="pre">ReLU,</span> <span class="pre">Linear]</span></code> blocks instead of <code class="docutils literal notranslate"><span class="pre">[Linear,</span> <span class="pre">LN,</span> <span class="pre">ReLU]</span></code> used above. These are clearly equivalent.
More importantly, notice that the network is constant width, i.e. <code class="docutils literal notranslate"><span class="pre">ResidualConnection</span></code> only requires the width parameter. Having different input and output width requires an extra linear layer on the input <code class="docutils literal notranslate"><span class="pre">x</span></code> to match the weights.
Our implementation follows the <strong>PreAct ResNet</strong> <span id="id17">[<a class="reference internal" href="../../intro.html#id110" title="Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Identity mappings in deep residual networks. CoRR, 2016. URL: http://arxiv.org/abs/1603.05027, arXiv:1603.05027.">HZRS16</a>]</span> setup shown in <a class="reference internal" href="#resnet-block"><span class="std std-numref">Fig. 53</span></a> (right). Placing the linear layer after the activation allows the residual to have negative values.</p>
<figure class="align-default" id="resnet-block">
<a class="reference internal image-reference" href="../../_images/05-resnet_block.svg"><img alt="../../_images/05-resnet_block.svg" src="../../_images/05-resnet_block.svg" width="300px" /></a>
<figcaption>
<p><span class="caption-number">Fig. 53 </span><span class="caption-text">The original ResNet block (<strong>left</strong>) applies a non-linear activation function, usually ReLU, after the skip connection. In contrast, the pre-activation ResNet block (<strong>right</strong>) applies the non-linearity at the beginning of <span class="math notranslate nohighlight">\(H\)</span>. For very deep network, the pre-activation ResNet has shown to perform better as the gradient flow is guaranteed to have the identity matrix as calculated below, and is not harmed by any non-linear activation applied to it. Source: <span id="id18">[<a class="reference internal" href="../../intro.html#id110" title="Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Identity mappings in deep residual networks. CoRR, 2016. URL: http://arxiv.org/abs/1603.05027, arXiv:1603.05027.">HZRS16</a>]</span></span><a class="headerlink" href="#resnet-block" title="Link to this image">#</a></p>
</figcaption>
</figure>
<p>Despite its simplicity, the idea of a skip connection is highly effective as it supports stable gradient propagation through the network. Notice that there are now two paths across a layer: one through the non-linearity and one around it. In particular, the gradients across the skip connection passes undisturbed since:</p>
<div class="math notranslate nohighlight">
\[
\frac{\partial \boldsymbol{\mathsf{x}}^{\ell+1}}{\partial \boldsymbol{\mathsf{x}}^\ell}=\mathbf{I}+\frac{\partial H(\boldsymbol{\mathsf{x}}^\ell)}{\partial \boldsymbol{\mathsf{x}}^\ell}.
\]</div>
<p>Applying the residual formula recursively unravels a deep residual network into combinatorially many paths where the gradient can pass through the shortest (i.e. across all skip connections) or longest (i.e. across all nonlinearities) path through the network:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
\boldsymbol{\mathsf{x}}^{\ell+2} 
&amp;= \boldsymbol{\mathsf{x}}^{\ell + 1}
+ H^{\ell+1}(\boldsymbol{\mathsf{x}}^{\ell + 1}) \\
&amp;= \boldsymbol{\mathsf{x}}^{\ell} + H^\ell(\boldsymbol{\mathsf{x}}^{\ell})
+ H^{\ell+1}(\boldsymbol{\mathsf{x}}^{\ell} + H^\ell(\boldsymbol{\mathsf{x}}^{\ell})).
\end{aligned}
\end{split}\]</div>
<p>This is shown in <a class="reference internal" href="#residual-unroll"><span class="std std-numref">Fig. 54</span></a>. Allowing short paths for the gradient to flow to earlier layers of the network solves the problem of getting uninformative gradients with depth. In fact, <a class="reference internal" href="#grad-path-distribution"><span class="std std-numref">Fig. 55</span></a> shows that a large percentage of gradient updates come from shorter paths. The paper <span id="id19">[<a class="reference internal" href="../../intro.html#id29" title="Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. CoRR, 2015. URL: http://arxiv.org/abs/1512.03385, arXiv:1512.03385.">HZRS15a</a>]</span> demonstrated the benefits of this by succesfully training <a class="reference external" href="https://pytorch.org/vision/main/models/generated/torchvision.models.resnet152.html">ResNet-152</a>, a 152-layer deep residual CNN, getting SOTA performance on ImageNet in 2015.</p>
<figure class="align-default" id="residual-unroll">
<a class="reference internal image-reference" href="../../_images/05-residual-unroll.png"><img alt="../../_images/05-residual-unroll.png" src="../../_images/05-residual-unroll.png" style="width: 700px;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 54 </span><span class="caption-text">Unraveled view of a residual network. Source: <span id="id20">[<a class="reference internal" href="../../intro.html#id108" title="Andreas Veit, Michael J. Wilber, and Serge J. Belongie. Residual networks are exponential ensembles of relatively shallow networks. CoRR, 2016. URL: http://arxiv.org/abs/1605.06431, arXiv:1605.06431.">VWB16</a>]</span></span><a class="headerlink" href="#residual-unroll" title="Link to this image">#</a></p>
</figcaption>
</figure>
<figure class="align-default" id="grad-path-distribution">
<a class="reference internal image-reference" href="../../_images/05-grad-path-distribution.png"><img alt="../../_images/05-grad-path-distribution.png" src="../../_images/05-grad-path-distribution.png" style="width: 700px;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 55 </span><span class="caption-text">Most of the gradient in a residual network with 54 layers comes from paths that are only 5-17 layers deep. Residual networks avoid the vanishing gradient problem by introducing short paths during training.
Source: <span id="id21">[<a class="reference internal" href="../../intro.html#id108" title="Andreas Veit, Michael J. Wilber, and Serge J. Belongie. Residual networks are exponential ensembles of relatively shallow networks. CoRR, 2016. URL: http://arxiv.org/abs/1605.06431, arXiv:1605.06431.">VWB16</a>]</span></span><a class="headerlink" href="#grad-path-distribution" title="Link to this image">#</a></p>
</figcaption>
</figure>
<p>Trying out whether residual connections diminish rank collapse and accelerate convergence:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">time</span>

<span class="n">sv_hooks</span> <span class="o">=</span> <span class="p">{}</span>
<span class="n">trainers</span> <span class="o">=</span> <span class="p">{}</span>
<span class="n">message</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">loss (last 100 steps)&quot;</span><span class="p">]</span>
<span class="n">model_params</span><span class="p">[</span><span class="s2">&quot;num_linear&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="mi">5</span>
<span class="n">train_params</span><span class="p">[</span><span class="s2">&quot;lr&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="mf">0.3</span>
<span class="n">train_params</span><span class="p">[</span><span class="s2">&quot;steps&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="mi">3000</span>

<span class="k">for</span> <span class="n">skip</span> <span class="ow">in</span> <span class="p">[</span><span class="kc">True</span><span class="p">,</span> <span class="kc">False</span><span class="p">]:</span>
    
    <span class="n">start_time</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
    <span class="n">sv_ratio</span> <span class="o">=</span> <span class="n">SingularValuesRatio</span><span class="p">()</span>
    <span class="n">init_fn</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">w</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">xavier_uniform_</span><span class="p">(</span><span class="n">w</span><span class="p">,</span> <span class="n">gain</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="mi">2</span><span class="p">),</span> <span class="n">generator</span><span class="o">=</span><span class="n">g</span><span class="p">())</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">init_model</span><span class="p">(</span><span class="n">get_model</span><span class="p">(</span><span class="n">skip</span><span class="o">=</span><span class="n">skip</span><span class="p">,</span> <span class="o">**</span><span class="n">model_params</span><span class="p">),</span> <span class="n">init_fn</span><span class="p">)</span>
    <span class="n">hooks</span> <span class="o">=</span> <span class="p">[{</span><span class="s2">&quot;bwd_hooks&quot;</span><span class="p">:</span> <span class="p">[],</span> <span class="s2">&quot;fwd_hooks&quot;</span><span class="p">:</span> <span class="p">[</span><span class="n">sv_ratio</span><span class="p">],</span> <span class="s2">&quot;layer_types&quot;</span><span class="p">:</span> <span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">,)}]</span>
    <span class="n">trainer</span> <span class="o">=</span> <span class="n">run</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">train_dataset</span><span class="p">,</span> <span class="o">**</span><span class="n">train_params</span><span class="p">,</span> <span class="n">hooks</span><span class="o">=</span><span class="n">hooks</span><span class="p">)</span>
    
    <span class="n">trainers</span><span class="p">[</span><span class="n">skip</span><span class="p">]</span> <span class="o">=</span> <span class="n">trainer</span>
    <span class="n">sv_hooks</span><span class="p">[</span><span class="n">skip</span><span class="p">]</span> <span class="o">=</span> <span class="n">sv_ratio</span>
    <span class="n">train_time</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span> <span class="o">-</span> <span class="n">start_time</span>
    <span class="n">message</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;    skip=</span><span class="si">{</span><span class="s1">&#39;True: &#39;</span><span class="w"> </span><span class="k">if</span><span class="w"> </span><span class="n">skip</span><span class="w"> </span><span class="k">else</span><span class="w"> </span><span class="s1">&#39;False:&#39;</span><span class="si">}</span><span class="s2">   </span><span class="si">{</span><span class="nb">sum</span><span class="p">(</span><span class="n">trainer</span><span class="o">.</span><span class="n">logs</span><span class="p">[</span><span class="s1">&#39;train&#39;</span><span class="p">][</span><span class="s1">&#39;loss&#39;</span><span class="p">][</span><span class="o">-</span><span class="mi">100</span><span class="p">:])</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="mi">100</span><span class="si">:</span><span class="s2">.3f</span><span class="si">}</span><span class="s2">      (t=</span><span class="si">{</span><span class="n">train_time</span><span class="si">:</span><span class="s2">.3f</span><span class="si">}</span><span class="s2">s)&quot;</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">message</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<script type="application/vnd.jupyter.widget-view+json">{"model_id": "b33d9ae2f286462faf8d1c4e0173c47c", "version_major": 2, "version_minor": 0}</script><script type="application/vnd.jupyter.widget-view+json">{"model_id": "ebe2588c909444f680718b5fc6271065", "version_major": 2, "version_minor": 0}</script><div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>loss (last 100 steps)
    skip=True:    2.423      (t=90.175s)
    skip=False:   2.486      (t=86.408s)
</pre></div>
</div>
</div>
</div>
<br>
<p><strong>Faster convergence.</strong> A shallow network initially has rapid decrease in loss followed by convergence to a shallow minimum.
On the other hand, deep networks train slowly, but reaches a better minimum after many epochs.
Residual connections speed up training by solving the slowness issue of deep networks at the beginning of training:</p>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>
<span class="k">for</span> <span class="n">skip</span> <span class="ow">in</span> <span class="p">[</span><span class="kc">False</span><span class="p">,</span> <span class="kc">True</span><span class="p">]:</span>
    <span class="n">trainer</span> <span class="o">=</span> <span class="n">trainers</span><span class="p">[</span><span class="n">skip</span><span class="p">]</span>
    <span class="n">train_loss</span> <span class="o">=</span> <span class="n">trainer</span><span class="o">.</span><span class="n">logs</span><span class="p">[</span><span class="s2">&quot;train&quot;</span><span class="p">][</span><span class="s2">&quot;loss&quot;</span><span class="p">]</span>
    <span class="n">train_loss_ma</span> <span class="o">=</span> <span class="n">moving_avg</span><span class="p">(</span><span class="n">train_loss</span><span class="p">,</span> <span class="n">window</span><span class="o">=</span><span class="mf">0.05</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">train_loss_ma</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sa">f</span><span class="s2">&quot;MLP</span><span class="si">{</span><span class="s1">&#39; + skip&#39;</span><span class="w"> </span><span class="k">if</span><span class="w"> </span><span class="n">skip</span><span class="w"> </span><span class="k">else</span><span class="w"> </span><span class="s1">&#39;&#39;</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;loss (MA)&quot;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;steps&quot;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="n">linestyle</span><span class="o">=</span><span class="s2">&quot;dotted&quot;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.6</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">();</span>
</pre></div>
</div>
</div>
</details>
<div class="cell_output docutils container">
<img alt="../../_images/4e22d1ded8f4940be3b66f483391f9f3f851f0b47cad57c1854f7c278d6e0899.svg" src="../../_images/4e22d1ded8f4940be3b66f483391f9f3f851f0b47cad57c1854f7c278d6e0899.svg" /></div>
</div>
<br>
<p><strong>Rank collapse at initialization.</strong> From the functional form of the residual connection, assuming that the initial inputs are linearly independent, the layer should avoid rank collapse. As discussed above, this results in faster training since the gradients and inputs are more informative. The following plot confirms this:</p>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">sv_hooks</span><span class="p">),</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span>
<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">skip</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">([</span><span class="kc">False</span><span class="p">,</span> <span class="kc">True</span><span class="p">]):</span>
    <span class="n">records</span> <span class="o">=</span> <span class="n">sv_hooks</span><span class="p">[</span><span class="n">skip</span><span class="p">]</span><span class="o">.</span><span class="n">records</span>
    <span class="k">for</span> <span class="n">j</span><span class="p">,</span> <span class="n">m</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">records</span><span class="o">.</span><span class="n">keys</span><span class="p">()):</span>
        <span class="n">ax</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">moving_avg</span><span class="p">(</span><span class="n">records</span><span class="p">[</span><span class="n">m</span><span class="p">],</span> <span class="n">window</span><span class="o">=</span><span class="mf">0.0005</span><span class="p">),</span> <span class="n">label</span><span class="o">=</span><span class="n">m</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span> <span class="o">+</span> <span class="s2">&quot;.&quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">j</span><span class="p">),</span> <span class="n">color</span><span class="o">=</span><span class="sa">f</span><span class="s2">&quot;C</span><span class="si">{</span><span class="n">j</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="nb">min</span><span class="p">(</span><span class="mf">0.5</span> <span class="o">+</span> <span class="p">(</span><span class="mf">0.5</span> <span class="o">/</span> <span class="nb">len</span><span class="p">(</span><span class="n">records</span><span class="o">.</span><span class="n">keys</span><span class="p">()))</span> <span class="o">*</span> <span class="n">j</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">))</span>
        <span class="n">ax</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylim</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
        <span class="n">ax</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="n">linestyle</span><span class="o">=</span><span class="s2">&quot;dashed&quot;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>
        <span class="n">ax</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s2">&quot;steps&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">skip</span><span class="p">:</span>
            <span class="n">ax</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;LN + Residual MLP Block&quot;</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">ax</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;LN + MLP&quot;</span><span class="p">)</span>

<span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="sa">r</span><span class="s2">&quot;${\sigma}_1\; /\; {\sigma}$.sum()&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">fontsize</span><span class="o">=</span><span class="mi">7</span><span class="p">)</span>
<span class="n">fig</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
</pre></div>
</div>
</div>
</details>
<div class="cell_output docutils container">
<img alt="../../_images/90a6d194aaeb0a37b17ad8679a5dc97baf443af46c5a3465efc7afe0f824f803.svg" src="../../_images/90a6d194aaeb0a37b17ad8679a5dc97baf443af46c5a3465efc7afe0f824f803.svg" /></div>
</div>
<p><strong>Figure.</strong> Residual connections diminish rank collapse at initialization without using BN.</p>
<br><p><strong>Residual connections + low rank as memory.</strong> The combination of residuality and low rank turns out as simulating <a class="reference external" href="https://en.wikipedia.org/wiki/Content-addressable_memory">large associative memory</a>. The changes to the inputs are small relative to the large ambient dimension so that weight updates do not completely erase the current state of the network. Early layers can write information that can be used by later layers due to skip connections. Later layers can learn to edit this information, e.g. deleting it provided sufficient complexity, if doing so reduces the loss. But by default information is preserved. This also shows that a limitation or bottleneck such as low rank can be essential to learning provided the network architecture provides ways to exploit or sidestep it.</p>
<hr class="docutils" />
<p>โ</p>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./nb/dl"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="04-lm.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">Language Modeling</p>
      </div>
    </a>
    <a class="right-next"
       href="07-attention.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Attention and Transformers</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#introduction">Introduction</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#preliminaries">Preliminaries</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#datasets">Datasets</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#weight-initialization">Weight initialization</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#training-with-hooks">Training with hooks</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#histograms">Histograms</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#dying-units">Dying units</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#layer-normalization">Layer normalization</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#immediate-effects">Immediate effects</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#gradient-normalization">Gradient normalization</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#weight-update-ratio">Weight update ratio</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#appendix-activations">Appendix: Activations</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#appendix-rank-collapse">Appendix: Rank collapse</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#appendix-residual-connections">Appendix: Residual connections</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By ๐ฝ๐ฎ๐ฟ๐๐ถ๐ฐ๐น๐ฒ๐ญ๐ฏ๐ฏ๐ญ. Powered by <a href="https://jupyterbook.org">Jupyter Book</a>.
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      ยฉ Copyright 2023.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../../_static/scripts/bootstrap.js?digest=3ee479438cf8b5e0d341"></script>
<script src="../../_static/scripts/pydata-sphinx-theme.js?digest=3ee479438cf8b5e0d341"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>